<html>
<head>
	
	<title>Privacy Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models</title>
	<meta name="keywords" content="fzb.me,冯宗宝,冯宗宝的blog" />

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    
	   <link href="/blog/css/main.css?v=3" rel="stylesheet" type="text/css" />
    
        
<script src="/blog/js/util.js"></script>

        <script>
            if(isMobile()) {
                loadjscssfile('../css/mobile.css', 'css');
            } else {
                loadjscssfile('../css/desktop.css', 'css');
            }
        </script> 
    

    <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">

    
	<link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico?v=3"/>
    
    

<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body>


<h2 class="title">Privacy Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models</h2>
<!--
<div style="text-align:center;margin-top: -10px;">
<div class="article-category">
发表于2019年11月15日




 </div>
-->
</div>

<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Privacy-Preserving-Recommender-Systems-with-Synthetic-Query-Generation-using-Differentially-Private-Large-Language-Models"><span class="toc-text">Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-Work"><span class="toc-text">Related Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Background"><span class="toc-text">Background</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-Retrieval"><span class="toc-text">Deep Retrieval</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Differentially-Private-Training"><span class="toc-text">Differentially Private Training</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Approach"><span class="toc-text">Approach</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-using-Synthetic-Data-obtained-from-a-DP-LLM"><span class="toc-text">Training using Synthetic Data obtained from a DP LLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DP-Training-using-Original-Data"><span class="toc-text">DP-Training using Original Data</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluation"><span class="toc-text">Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Experimental-Setup"><span class="toc-text">Experimental Setup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation-on-Retrieval-Tasks"><span class="toc-text">Evaluation on Retrieval Tasks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Similarity-between-Synthetic-and-Original-Datasets"><span class="toc-text">Similarity between Synthetic and Original Datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Empirical-Privacy"><span class="toc-text">Empirical Privacy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-text">Conclusion</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol>
<h1 id="Privacy-Preserving-Recommender-Systems-with-Synthetic-Query-Generation-using-Differentially-Private-Large-Language-Models"><a href="#Privacy-Preserving-Recommender-Systems-with-Synthetic-Query-Generation-using-Differentially-Private-Large-Language-Models" class="headerlink" title="Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models"></a>Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>我们提出了一种新颖的方法，用于开发隐私保护的大规模推荐系统，该系统利用差分隐私（DP）大型语言模型（LLM），克服了在训练这些复杂系统时遇到的某些挑战和限制。我们的方法特别适用于基于LLM的推荐系统这一新兴领域，但也可以轻松应用于处理自然语言输入表示的任何推荐系统。我们的方法涉及使用DP训练方法，在查询生成任务上对公开预训练的LLM进行微调。由此产生的模型可以生成代表原始查询的私有合成查询，并可自由共享给下游非私密推荐训练过程中使用，而不会增加额外的隐私成本。我们评估了我们的方法在安全地训练有效深度检索模型方面所能取得的能力，并观察到与直接进行DP训练检索模型相比，在不损害查询级隐私保证情况下显著改善了其检索质量。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在许多推荐应用中，输入查询包含用户隐私信息，而候选推荐（如文章、产品、电影、广告）是公开信息。鉴于此，本文的目标是开发一种用于训练查询级隐私保护推荐系统的方法。</p>
<p>隐私保护推荐系统的标准方法是直接使用差分隐私（DP）训练方法来训练模型。然而，直接DP训练推荐系统存在各种问题。首先，DP训练方法通常确保示例级的隐私，这比我们的隐私目标更严格，因为推荐数据集中的示例对应于一个查询和若干候选推荐。因此，标准的DP训练隐私保证可能导致比必要更大的性能降低，以保护查询隐私。</p>
<p>与直接DP训练推荐系统的各种问题相比，我们采取了更简单的方法，在训练系统之前确保查询隐私。在这项工作中，我们在使用DP大型语言模型（LLM）生成合成数据的框架上进行了扩展，以开发一种用于具有查询级隐私保护的下游推荐系统的私密文本数据共享方法。我们的方法涉及使用DP训练方法对公开预训练的生成型LLM进行微调（或第二阶段预训练），针对包含匹配的查询-文档对的私有推荐数据集执行文档条件的查询生成任务。对于DP微调，我们采用差分隐私随机梯度下降（DP-SGD），其通过对每个示例的梯度进行剪辑，并在反向传播期间向聚合梯度注入校准噪声来工作。所得到的DP微调LLM用于生成私有合成查询，这些查询与训练数据中的文档有关。通过利用DP的后处理属性，可以安全地共享这些私有合成查询，用于任何下游的非私有训练程序，以用于推荐系统，而不会增加原始查询的任何隐私成本。我们使用这种方法在实证中证明，在与直接DP训练方法相比不损害隐私保证的情况下，深度检索任务的表现得到了显著改进。</p>
<p>我们的贡献如下：</p>
<ol>
<li>我们提出了一种新颖的方法，用于训练具有查询级隐私保护的推荐系统。我们的方法涉及使用DP微调的LLM生成合成查询数据，并使用这些私有合成数据来训练下游推荐系统，而无需对标准训练过程进行任何修改。 </li>
<li>通常情况下，我们的方法提供了一种新颖的方式，用于获得具有非逐示例可分解损失（non-per-example decomposable loss，推荐数据集中的示例对应于一个查询和若干候选推荐，不是per-example）的模型的DP保证，这种损失在大规模多阶段推荐系统中普遍存在，如对比损失。</li>
<li>通过广泛的实验，我们从经验上证实了我们的方法生成的合成数据确实具有查询级隐私保护，并且我们证明，使用这些私有合成数据训练的下游深度检索模型与标准DP训练方法相比在性能方面表现出色。</li>
</ol>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>在推荐系统中的隐私保护。以往关于推荐系统的差分隐私研究主要涉及矩阵分解类模型。这些方法具有挑战性，容易受到冷启动问题的影响，并且在更严格的差分隐私参数下性能降低更为明显。以联邦学习方式训练推荐系统是另一种保护隐私的方法。然而，这种学习方法涉及到数据所有权和处理方式，而非数学上的隐私保证。</p>
<p>使用差分隐私语言模型生成合成数据。对于生成具有个人隐私特征表格数据已经进行了广泛探索，但是生成带有个人隐私特征文本数据还相对较新颖。利用差分隐私微调语言模型（LLMs）来生成带有差分隐私特征文本是目前主流方法，并且最近的研究也探讨了使用从差分隐私微调LLMs得到的合成数据在下游任务中的效用。这些研究发现，在相同的随机性预算下，使用合成数据训练的下游模型在性能上明显优于直接使用差分隐私训练的模型。有趣的是，在与非差分隐私模型进行比较时，结果各异。例如，Yue等人和Putta等人报告了使用合成数据训练的文本分类模型效用显著下降的情况，即使是非差分隐私微调LLM数据，这表明合成文本具有低保真度。然而，Mattern等人和Mireshghallah等人则报告了在文本分类和语义解析任务中使用非差分隐私合成数据生成可以提高性能的实例。这些发现表明，在没有应用差分隐私技术时，通过生成合成数据可以改善下游任务的性能，并且我们的工作探索了另一种潜在优势所在：针对部分数据（如我们所研究对象中的查询）确保隐私。</p>
<p>据我们所知，并没有先前研究探索仅针对部分数据（如我们案例中的查询）生成合成数据以确保隐私。</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Deep-Retrieval"><a href="#Deep-Retrieval" class="headerlink" title="Deep Retrieval"></a>Deep Retrieval</h3><p>深度检索系统，也被称为密集检索系统，在现代推荐系统中已经成为有效的组件。这些系统通常由两个encoder组成，能够生成丰富、密集的查询和文档表示，从而实现近似最近邻搜索，以高效地检索与查询语义意义相符的相关文档。深度检索系统通常通过对比损失进行训练，使用两种类型的示例：正样本和负样本。正样本将相关的查询-文档对embedding在embedding space中拉近，而负样本则将不相关对的embedding推远。获取硬负样本可能具有挑战性，因为它们需要从大量候选文档中进行额外挖掘。因此，在深度检索中流行选择 contrastive loss是in-batch softmax loss，该函数利用batch内的其他文档作为软负例。</p>
<blockquote>
<p>假设我们有一个深度检索模型，用于给用户推荐电影。给定一个查询（用户的喜好或兴趣），模型的目标是从候选电影中找到与查询相关的电影，同时将不相关的电影视为负样本。在这种情况下，获取困难负样本可能是一个挑战。想象一下，我们有数千部电影作为候选，但在现实世界中，获取用户明确表示不喜欢的电影可能会很困难，因为用户通常只会对喜欢的电影进行评分或点击。为了解决这个问题，我们可以采用in-batch softmax loss。假设我们有一个训练批次，其中包含一些查询和与之对应的正样本电影，我们可以将这些正样本电影作为硬正样本。然后，我们可以将同一批次中的其他电影视为软负样本，通过计算查询与这些电影之间的相似度来推动模型学习。</p>
</blockquote>
<p>特别地，在给定一个训练批次的查询-文档对<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.71ex;" xmlns="http://www.w3.org/2000/svg" width="11.551ex" height="2.406ex" role="img" focusable="false" viewBox="0 -750 5105.7 1063.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mrow"><g data-mml-node="mo"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="mrow" transform="translate(500,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(479,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1162,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1606.6,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2453.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mo" transform="translate(3342.6,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(3875.6,-285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1012,0)"><g data-mml-node="mi"><path data-c="42" d="M304 342Q292 342 292 353Q292 372 323 391Q331 396 417 428T533 487Q563 512 563 555V562Q563 575 557 589T530 618T475 636Q429 636 396 613T330 539Q263 446 210 238Q196 183 173 120Q135 31 121 16Q108 1 85 -10T47 -22T32 -10Q32 -5 44 18T77 93T112 206Q135 296 154 395T182 550T191 615Q191 616 190 616Q188 616 179 611T157 601T131 594Q113 594 113 605Q113 623 144 644Q154 650 205 676T267 703Q277 705 279 705Q295 705 295 693Q295 686 288 635T278 575Q278 572 287 582Q336 635 402 669T540 704Q603 704 633 673T664 599Q664 559 638 523T580 462Q553 440 504 413L491 407L504 402Q566 381 596 338T627 244Q627 172 575 110T444 13T284 -22Q208 -22 158 28Q144 42 146 50Q150 67 178 85T230 103Q236 103 246 95T267 75T302 56T357 47Q436 47 486 93Q526 136 526 198V210Q526 228 518 249T491 292T436 330T350 345Q335 345 321 344T304 342Z"></path></g></g></g></g></g></g></svg></mjx-container>中，每个 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.916ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 847 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 都是查询 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.749ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 773 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(479,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 的正向文档，并且批次中所有其他文件<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.09ex;" xmlns="http://www.w3.org/2000/svg" width="6.929ex" height="2.787ex" role="img" focusable="false" viewBox="0 -750 3062.7 1231.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mrow"><g data-mml-node="mo"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="msub" transform="translate(500,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(1394.3,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1927.3,-329.6) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(412,0)"><path data-c="2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"></path></g><g data-mml-node="mi" transform="translate(1190,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container>被视为负样本。in-batch softmax loss 对于批次中的每个样本都是</p>
<script type="math/tex; mode=display">
\mathcal{L}_i=-\log \frac{e^{\operatorname{sim}\left(q_i, d_i\right)}}{\sum_{j \in \mathcal{B}} e^{\operatorname{sim}\left(q_i, d_j\right)}}</script><p>其中对于任意的<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="6.971ex" height="2.057ex" role="img" focusable="false" viewBox="0 -705 3081.2 909"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(789.7,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(1479.4,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2424.2,0)"><g data-mml-node="mi"><path data-c="42" d="M304 342Q292 342 292 353Q292 372 323 391Q331 396 417 428T533 487Q563 512 563 555V562Q563 575 557 589T530 618T475 636Q429 636 396 613T330 539Q263 446 210 238Q196 183 173 120Q135 31 121 16Q108 1 85 -10T47 -22T32 -10Q32 -5 44 18T77 93T112 206Q135 296 154 395T182 550T191 615Q191 616 190 616Q188 616 179 611T157 601T131 594Q113 594 113 605Q113 623 144 644Q154 650 205 676T267 703Q277 705 279 705Q295 705 295 693Q295 686 288 635T278 575Q278 572 287 582Q336 635 402 669T540 704Q603 704 633 673T664 599Q664 559 638 523T580 462Q553 440 504 413L491 407L504 402Q566 381 596 338T627 244Q627 172 575 110T444 13T284 -22Q208 -22 158 28Q144 42 146 50Q150 67 178 85T230 103Q236 103 246 95T267 75T302 56T357 47Q436 47 486 93Q526 136 526 198V210Q526 228 518 249T491 292T436 330T350 345Q335 345 321 344T304 342Z"></path></g></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="10.366ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 4581.9 1044.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(814,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1692,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(2081,0)"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(479,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2854,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3298.6,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(4192.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.749ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 773 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(479,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex;" xmlns="http://www.w3.org/2000/svg" width="2.023ex" height="2.236ex" role="img" focusable="false" viewBox="0 -694 894.3 988.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container> embedding vectors之间的余弦相似度。批次越大且多样化，对于表示学习来说越好。</p>
<h3 id="Differentially-Private-Training"><a href="#Differentially-Private-Training" class="headerlink" title="Differentially Private Training"></a>Differentially Private Training</h3><p>尽管DP-SGD可以无缝地应用于针对每个训练数据示例定义损失（例如交叉熵损失），但它不太适用于使用多个训练实例计算损失值（例如in-batch softmax loss）的对比性损失。原因是每个示例梯度还取决于多个示例，因此灵敏性可能会随着计算中使用的示例数量而增加。因此，确保隐私需要通过将渐变剪切中灵敏性按照用于计算对比损耗所使用的示例数量进行缩放，这可能会大大增加噪声。找到更好的方法来对非逐个分解损失进行DP-Training模型仍然是一个活跃的研究课题，各种工作在特定条件下引入了专门的算法，如凸性、平滑性和Lipshitz连续性，以保持合理的灵敏度界限。我们强调，尽管直接将DP-training应用于对比损失是粗糙的，因为它引入了太多噪声，但获得更好的算法本身就是一项艰巨任务。</p>
<h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>我们描述了一种通用方法，可以在确保查询级隐私的同时获得DP合成数据，以训练下游推荐系统。我们特别关注以这种方式训练双encoder 检索系统。我们还将讨论直接在原始数据上进行DP训练双 encoder 的替代方法，以进行比较。</p>
<h3 id="Training-using-Synthetic-Data-obtained-from-a-DP-LLM"><a href="#Training-using-Synthetic-Data-obtained-from-a-DP-LLM" class="headerlink" title="Training using Synthetic Data obtained from a DP LLM"></a>Training using Synthetic Data obtained from a DP LLM</h3><p><strong>DP-Training LLM on Conditional Query Generation Task</strong> 我们使用的数据集包含查询-文档对。我们使用一个合适的公开预训练LLM，该模型没有用我们使用的数据集进行预训练。我们使用T5语言模型的encoder，它们能够根据输入文本生成回答，并针对特定的生成任务进行微调。我们使用DP-Adafactor2来微调LLM，并完成以下条件查询生成任务：给定训练数据中的一个查询-文档对<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.983ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2202.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(849,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1293.7,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(1813.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，通过输入“generate_query：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container> ”与target：”<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 460 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container> “，来用==teacher forcing==训练T5模型（从参数量来说T5还真能算大模型，毕竟最大的T5有110亿参数，而ChatGLM只有60亿参数）。</p>
<p><strong>Synthetic Query Generation using DP LLM</strong> 对于每个文档𝑑，我们通过将输入“generate_query: 𝑑”提供给模型来生成匹配的合成查询<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="1.041ex" height="2.136ex" role="img" focusable="false" viewBox="0 -750 460 944"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(316.8,332) translate(-250 0)"><path data-c="7E" d="M179 251Q164 251 151 245T131 234T111 215L97 227L83 238Q83 239 95 253T121 283T142 304Q165 318 187 318T253 300T320 282Q335 282 348 288T368 299T388 318L402 306L416 295Q375 236 344 222Q330 215 313 215Q292 215 248 233T179 251Z"></path></g></g></g></g></g></svg></mjx-container>。在采样时，我们采用nucleus sampling策略。然后构建一个由原始文档及其相应合成查询组成的合成训练数据集。</p>
<blockquote>
<p>Top-P Sampling (Nucleus sampling) 是预先设置一个概率界限 p 值，然后将所有可能取到的单词，根据概率大小从高到低排列，依次选取单词。当单词的累积概率大于或等于 p 值时停止，然后从已经选取的单词中进行采样，生成下一个单词。</p>
</blockquote>
<p><strong>Training Dual Encoder with DP Synthetic Data</strong> 合成数据可以安全地共享给任何下游训练程序，在原始查询上不会产生额外的DP损失，这是由DP的后处理属性所保证的。特别地，我们可以使用标准SGD方法，在合成训练数据上使用in-batch softmax loss来训练双encoder模型。</p>
<p>==<strong>这个方法不是被谷歌说不好吗？拿生成出来的文本再做训练。</strong>==</p>
<h3 id="DP-Training-using-Original-Data"><a href="#DP-Training-using-Original-Data" class="headerlink" title="DP-Training using Original Data"></a>DP-Training using Original Data</h3><p>为了进行比较基准，我们还将直接在原始数据上对深度检索系统进行DP微调。然而，正如第3.4节中讨论的那样，DP-SGD需要考虑与对比损失（如in-batch softmax loss）兼容的额外因素。DP-SGD要求每个 per-example gradient clipping 中的敏感性按 batch 大小缩放。这意味着通过增加 batch size 来减少每个示例噪声以提高DP训练模型效用的最常见方法将无法奏效。此外，DP-SGD将保证示例级隐私，在本案例中一个示例包含一个查询和批次中的每个文档。然而，我们希望实现查询级隐私，这应该比保护查询和文档都更容易。与DP训练实施相关的另一个问题是为了利用计算每个示例梯度向量化和并行化策略，在批处理中的每个查询-文档示例必须复制到批处理中的每个示例中，导致内存需求呈二次增加趋势。在固定内存资源下，这就需要显著降低批量大小，并且除了梯度剪辑和添加噪声之外还会对学习产生负面影响，因为in-batch softmax loss的质量高度依赖于批内示例的数量和多样性。</p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><h3 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h3><p><strong>数据集 </strong>我们在信息检索任务中使用公开可用的数据集。对于微调和评估，我们考虑了MSMARCO数据集[2]，该数据集包含从Bing搜索日志中采样得到的532,000个查询-文档对，涵盖了广泛的领域和概念。此外，我们还考虑了BEIR基准套件[52]中的数据集，该套件包含各种领域的信息检索数据集，用于零-shot评估。</p>
<p><strong>数据生成 </strong>我们使用不同大小（小型、基础、大型、超大）和隐私保证（𝜖 ∈ {3, 8, 16, ∞}）训练了各种T5模型，以生成给定输入文档的合成查询。我们将MSMARCO数据集视为查询-文档训练数据。然后，我们使用每个训练好的模型为原始训练数据中的文档生成合成查询。这些合成查询和原始文档对构成了一个新的合成数据集。请参阅附录C，了解如何确保LLMs没有在训练数据中预先进行过查询预训练。我们对每个模型进行了30轮次的训练，批量大小为1024，并将输入文档的最大令牌长度设置为384，目标查询设置为128. 我们使用DP-Adam优化器，学习率为0.001，并且剪辑范数设定为0.1. 根据[33]所述，我们将隐私参数𝛿 = 1/2𝑛设定其中 𝑛 是培训资料集规模. 对于采样方法, 我们采用核心抽样(nucleus sampling)并设定 𝑝 = 0.8 。鉴于时间和计算限制，在T5-Small模型上通过小规模超参数搜索获得了结果(详见附录B)。</p>
<p><strong>下游检索任务 </strong>对于每个数据源（原始MSMARCO数据和不同𝜖和模型大小的合成数据集），我们在 in-batch softmax loss上训练了一个深度检索双编码器模型（如第3.1节所述）。对于合成数据集和没有DP保证的原始数据集，我们使用标准SGD训练方法，而对于具有DP保证的原始数据集，则采用DP-SGD方法。我们利用预训练的T5-Base编码器作为查询和文档编码器，并共享它们之间的参数。我们将每个双编码器模型训练了5个epochs，并且使用与上述相同的令牌长度和超参数。对于直接DP训练，我们使用了与上述相同的隐私参数，并且考虑到第4.2节中讨论到的内存限制，用于DP训练双编码器模型的批量大小必须显著减小至32。重要说明一点，检索模型中的编码器与用于生成合成数据的T5模型是不同的。我们选择T5来评估基于LLM（语言理解与生成）技术推荐系统，在实际应用中越来越普遍[16]。由于我们旨在严格评估合成数据集性能，因此并未尝试不同的深度检索模型。</p>
<h3 id="Evaluation-on-Retrieval-Tasks"><a href="#Evaluation-on-Retrieval-Tasks" class="headerlink" title="Evaluation on Retrieval Tasks"></a>Evaluation on Retrieval Tasks</h3><p><strong>MSMARCO评估 </strong>表1显示了在原始数据和合成数据上训练的深度检索模型在MSMARCO测试集上的评估结果。在每个表格中，我们提供了一个基准评估，该评估是在没有任何差分隐私（DP）的情况下对原始数据进行训练得到的双编码器模型。我们观察到，在使用带有DP的合成数据进行训练时，检索模型明显优于使用原始数据进行DP训练时的检索效果。正如第4.2节所讨论的那样，使用对比损失来训练DP模型存在一些挑战。这很可能解释了为什么在原始数据上进行DP训练效果不佳。此外，我们发现，在非DP合成数据上进行训练得到的检索模型胜过了在原始数据上进行训练得到的检索模型。这表明合成数据生成确实增强了原始数据，并且在某种程度上改善了泛化能力，无论是通过融入额外公共知识还是通过清洗数据来实现。事实上，利用大规模语言模型生成合成数据以用于深度检索是近年来引起极大兴趣的研究领域[4, 13, 25]。我们还观察到，模型大小的增加会提高性能。这与之前类似的结果一致，表明在过参数化模型上进行差分隐私随机梯度下降（DP-SGD）可以比以前预期的效果要好得多[14, 33]。总体而言，我们展示了利用LLMs生成合成数据是一个可行的方法，在需要保护隐私的情况下训练检索模型。</p>
<p><strong>zero-shot 评估 </strong>我们还评估了在合成数据上训练的检索模型的 zero-shot 泛化能力。我们将其与在原始数据上训练且没有差分隐私（DP）和 𝜖 = 16 的检索模型进行比较。结果见表2。再次，我们的结果显示，在某些情况下，使用 DP 合成数据相对于 DP 训练原始数据具有显著优势，并且几乎可以与非 DP 结果相媲美甚至超越其性能。这表明，在合理程度的隐私保护水平下，合成数据生成所带来的好处可能会超过 DP 训练中的效用降低，至少在零-shot 泛化任务中是如此。需要进一步研究以理解这一观察结果。</p>
<h3 id="Similarity-between-Synthetic-and-Original-Datasets"><a href="#Similarity-between-Synthetic-and-Original-Datasets" class="headerlink" title="Similarity between Synthetic and Original Datasets"></a>Similarity between Synthetic and Original Datasets</h3><p><strong>相似度评分</strong> 由于合成数据是从原始数据一对一生成的，我们可以计算BLEU分数来评估相似性[41]。我们还计算了MAUVE分数，该分数被证明更能够比较文本分布的相似性[39]。请参见表3中的得分。我们观察到非DP微调模型生成的合成数据在这些指标下与预期相似，并且随着有限𝜖值的增加，与更高𝜖和模型大小增加而增加的相似性也会显著下降。通过将相似度评分与检索评价结果进行比较，我们观察到尽管更大的模型在合成数据相似性方面取得了显著改进，但随着模型大小增加，下游检索性能提升幅度相对较小。</p>
<h3 id="Empirical-Privacy"><a href="#Empirical-Privacy" class="headerlink" title="Empirical Privacy"></a>Empirical Privacy</h3><p>可证明的差分隐私在𝜖增大时会显著衰减，但之前的研究表明即使在这些较大的值下也能提供强大的保护，防止最先进的隐私攻击[7, 9, 40]。为了验证我们的训练技术仍然遵循这一趋势，我们在此评估DP训练语言模型时所产生的实际隐私泄露情况，使用了[9]中引入的金丝雀暴露度指标。该技术经常用于评估实际隐私[23, 46, 62]。为了进行这个测试，我们构建包含个人信息（称为金丝雀）的示例，并将其中一部分引入原始训练数据中，然后测量模型输出注入式金丝雀查询结果的可能性。通常来说，生成金丝雀是一个依赖于领域特定决策，在我们检索应用程序中设计了三种类型的查询-文档对作为金丝雀：（随机查询、随机10位数字字符串）、（随机查询、相应文档+ 随机10位数字字符串）、（随机查询、随机文档+ 随机10位数字字符串）。每个金丝雀的秘密部分是随机的10位数字字符串。我们重复这些类型的金丝雀多次，并将它们包含在训练数据中。然后，我们使用不同DP保证对毒化训练数据集进行语言模型训练。一旦模型训练完成，我们生成合成数据集，并检查每个金丝雀查询的秘密部分是否出现在其输出中，并将注入式金丝雀与100个具有不同随机10位数字秘密字符串的候选序列进行比较，测量其在这些秘密之间的排名。我们重复整个实验过程，生成金丝雀、训练模型和测量泄露情况多次，并对指标取平均值，在表4中报告结果。如预期所见，在没有差分隐私的情况下进行训练会导致显著泄露。重复10次的金丝雀始终是最可能被提取出来的秘密，并经常能够被提取出来，而重复100次以上则总能够被提取出来。然而，即使𝜖为16时采用了我们的方法也可以防止模型泄露秘密信息，并显著增加排名[8]。最近关于将攻击成功率转换为𝜖参数下限[51]的技术使我们能够将这些排名解释为𝜖的下限，大约为0.015。这种巨大的差距与之前关于DP-SGD在大型语言模型上实际隐私性发现[9, 40]是一致的。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>我们提出了一种新颖的方法，通过使用差分隐私语言模型（DP LLMs）生成合成查询来以保护隐私的方式训练下游检索系统。更一般地说，我们的方法为具有非逐例可分解损失函数的模型获得了差分隐私保证的新途径。从实证上看，我们已经展示了在标准检索指标上可以实现具备差分隐私保证的高性能，并验证了我们方法的隐私保护效果。同时，我们还发现较大规模的模型可以在相同隐私水平下改善性能。因此，我们得出结论：基于合成文本生成技术是开发大规模、保护用户隐私推荐系统所迈出重要一步。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Learned a lot. 这篇文章思路很简单，感觉有点水，但这个思路很直接，可以在这篇文章基础上继续拓展到别的NLP领域。</p>


<!--<a href="https://asparticguan.github.io/blog/Privacy-Preserving%20Recommender%20Systems%20with%20Synthetic%20Query%20Generation%20using%20Differentially%20Private%20Large%20Language%20Models.html#disqus_thread" class="article-comment-link">Comments</a>
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = ''; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
-->
<div style="display:none">
<script src="http://s4.cnzz.com/stat.php?id=&web_id=" language="JavaScript"></script>script>
</div>






<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>