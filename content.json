{"meta":{"title":"躲进小楼成一统","subtitle":"躲进小楼成一统","description":"","author":"Aspartic","url":"https://AsparticGuan.github.io/blog","root":"/blog/"},"pages":[],"posts":[{"title":"抽象解释札记","slug":"抽象解释札记","date":"2024-03-06T06:38:17.978Z","updated":"2024-05-27T07:45:11.101Z","comments":true,"path":"抽象解释札记.html","link":"","permalink":"https://asparticguan.github.io/blog/%E6%8A%BD%E8%B1%A1%E8%A7%A3%E9%87%8A%E6%9C%AD%E8%AE%B0.html","excerpt":"","text":"抽象解释札记——我们抽象解释是这样的，在网上搜半天资料还是一点都看不懂，更别提自己写点东西了，写也写不出。 从小学数学开始在网上找到的很多关于抽象解释的资料都从这么一个小学数学的例子开始讲起：负+负=负，负*负=正。所以，有了这样的总结，我们在面对问题“是正数还是负数“时就不用真的把这个大数算出来，而是通过对数的总结就能直接说出答案是”正数“。 事实上在这个过程中用到的就是抽象的思想。我们看的不是具体的​这些数，而是将它们抽象出来，以便于我们进行分析。在程序静态分析中也是一样，我们把程序中的某些具体量抽象出来考虑，也是因为一个程序完全可以有无数种具体的运行时状态（可以理解为变量的值的组合），静态不可能穷举完，所以用抽象解释牺牲精度换取时间。它将程序某些具体值（concrete value，不一定是变量的值）概括成一个抽象值（abstract value），这样有些语句只用遍历有限次就可以概括它所有运行时的具体属性。 抽象解释所做的事情，也就是解释抽象空间和具体空间的关系。 伽罗瓦连接抽象解释背后的数学原理就是伽罗瓦连接（Galois Connection）。我们首先定义具体化函数来将抽象值映射为具体值的集合，定义抽象化函数将具体值的集合映射为抽象值。我们称和构成抽象域和具体域集合的全集​之间的一个伽罗瓦连接，记为 (\\mathcal{C},\\preceq)\\underset{\\gamma}{\\stackrel{\\alpha}{\\rightleftharpoons}}(\\mathcal{A},\\preceq^\\prime)当且仅当： \\forall X \\in \\mathcal{C}, Y \\in \\mathcal{A}, \\alpha(X) \\preceq^\\prime Y \\Leftrightarrow \\gamma(Y) \\preceq X接下来的证明就不细说了，点到为止。 抽象解释举隅符号域最简单的抽象域应该是符号域。例如说把所有的数字都抽象为中的一个。其中表示任意值，表示NaN，如的结果。剩下三个分别就是正数、零和负数。这个可以用来判断数组越界和除零错误。例如，如果分析到某值为​而处于数组取数位置，就可以判断其为数组越界。 区间域—宽窄算子—w.t.o.区间域另一个比较常见的抽象域为区间域。区间域用于区间分析。我们举一例来说明。 int a[10]; // length of a is 10 int i = 0; while(i &lt; 10) { a[i] = 0; i = i + 1; } 我们使用区间分析说明这个程序执行时不会数组越界。 首先，将程序中的循环拆开，写成如下形式： L1: i = 0; ,-&gt; L2: goto L3, L7; ------. L1 ∪ L5 | | | L3: assume(i &lt; 10); &lt;--| L2 ∩ [-∞,9] | L4: a[i] = 0; | | L5: i = i + 1; | `---L6: goto L2; | | L7: assume(i &gt;= 10); &lt;-' L2 ∩ [10,∞] 接下来进行分析。第一次循环，走完L2后，i区间为，走完L5后，i区间为。第二次循环，走完L2后，和并得，到L5时，增为。故第三次循环，走完L2后，得……与此同时，L2在每次循环中也可以往L7走，但每次和assume的条件取交后仍为空集。直到第十一次循环，走完L2后，得，接着进入L3，与取交后得，而此时与L7取交为，达到不动点，即继续循环各处区间值也不会再有变化了。从而知道在循环中不会数组越界。 宽窄算子这个方法固然好，但有个问题，比较慢。这里i还只是小于10，就已经循环了11轮才到不动点。要是是小于10000，循环轮数想都不敢想了。所以要想个法子降循环轮数。方法就是用”宽窄算子“——宽算子和窄算子（刚去成都玩完回来，宽窄巷子也是有宽巷子和窄巷子w）。 在此例中，只需用到加宽算子。我们定义宽算子如下： [l_1,h_1] \\bigtriangledown[l_2,h_2]=[l_3,h_3]其中， l_3 = \\begin{cases} l_1, & l_2 \\geq l_1 \\\\ -\\infty, & l_2 < l_1 \\end{cases} h_3 = \\begin{cases} h_1, & h_2 \\leq h_1 \\\\ +\\infty, & h_2 > h_1 \\end{cases}假定在循环头处（L2位置）对其进行加宽。 对应的后继计算公式从变为带有加宽算子的计算公式。 即第四轮循环时，对L2有。 将增长的上界变为，即对应区间变为​。 这样，接下来走完L3后，结果就是了。就像下面这样： 1st loop 2nd loop 3rd loop L1 [0,0] [0,0] [0,0] L2 [0,0] [0,T] [0,T] L3 [0,0] [0,9] [0,9] L4 [0,0] [0,9] [0,9] L5 [1,1] [1,10] [1,10] L6 [1,1] [1,10] [1,10] L7 [] [10,T] [10,T] 实际上，从格理论的角度看，加宽的思想就是在从最下端往上逼近不动点的过程中直接跳过不动点，改为从上往下逼近。对于无限的抽象域（如区间抽象域），抽象解释借助于宽算子保证迭代的终止性。 很快啊，三轮就不变了。但是很明显这样是不精确的。原本应该是的，现在是。为了解决这个问题，有两种方法。 第一种是不要每次都用加宽。在这里，令区间边界连续单调变化三次后，对其进行加宽。再在此基础上，我们继续从被加宽的位置开始，进行新一轮的迭代。再经过两轮迭代，各个位置的区间不再发生变化，迭代就可以停止。 1st 2nd 3rd 4th 5th 6th L1 [0,0] [0,0] [0,0] [0,0] [0,0] [0,0] L2 [0,0] [0,1] [0,2] [0,T] [0,10] [0,10] L3 [0,0] [0,1] [0,2] [0,9] [0,9] [0,9] L4 [0,0] [0,1] [0,2] [0,9] [0,9] [0,9] L5 [1,1] [1,2] [1,3] [1,10] [1,10] [1,10] L6 [1,1] [1,2] [1,3] [1,10] [1,10] [1,10] L7 [] [] [] [10,T] [10,10] [10,10] 另一种通用的方法是在用了宽算子后再补用窄算子。我们定义窄算子如下： [l_1,h_1] \\bigtriangleup[l_2,h_2]=[l_3,h_3]其中， l_3 = \\begin{cases} l_2, & l_1 \\equiv -\\infty \\\\ l_1, & l_1 \\neq -\\infty \\end{cases} h_3 = \\begin{cases} h_2, & h_1 \\equiv \\infty \\\\ h_1, & h_1 \\neq \\infty \\end{cases}在用宽算子达到不动点后，改用窄算子。L2处对应计算公式改为。然后继续迭代直到不变： 1st loop 2nd loop 3rd loop 4th loop 5th loop L1 [0,0] [0,0] [0,0] [0,0] [0,0] L2 [0,0] [0,T] [0,T] [0,10] [0,10] L3 [0,0] [0,9] [0,9] [0,9] [0,9] L4 [0,0] [0,9] [0,9] [0,9] [0,9] L5 [1,1] [1,10] [1,10] [1,10] [1,10] L6 [1,1] [1,10] [1,10] [1,10] [1,10] L7 [] [10,T] [10,T] [10,10] [10,10] 结果重新达到精确。 w.t.o.上面的例子中，我们直接规定了在L2处进行加宽。但至于为什么是在这里而不是别的程序点加宽没讲。Efficient chaotic iteration strategies with widenings这篇文章讲了如何选取进行widening的程序点集合。即选取admissible set of widening points。 这篇文章一个微信公众号讲得很清楚：https://mp.weixin.qq.com/s/lh87a0rs4DzL-LIp4Bj0HA。我这只是在其基础上再做一点解释。 根据文章中的定义，一个层次化排序（hierarchical ordering）是一个集合的良好括号化排列，不含有连续的两个”(“。在这种排序中，每对匹配的括号之间的元素称为一个组件（component），每个组件的第一个元素称为头（head）。对于集合中的每个元素，我们用表示包含c的嵌套组件的头的集合. 例如，假设我们有一个集合，一个可能的层次化排序是： 1 2 (3 4 (5 6) 7) 8 在这个例子中： 数字1和2位于最外层，没有被任何括号包围，因此它们是深度0的元素。 3是一个组件的头，这个组件包含了4, 5, 6, 和7，其中5是另一个组件的头，这个内部组件包含了6。所以，3和5是头元素，而5和6是一个嵌套的组件。 ，因为1和2不被任何组件包含。 ，因为6被头元素3和5包含的组件所包含。 具有以下性质： 对于控制流图中的一条从顶点 u 指向顶点 v 的边，那么在排序中如果 u 在 v 之前，则 ，如果 v 在 u 之前，则 。 本质上来说就是找循环。对于下图，其层次化排序就是1 2 (3 4 (5 6) 7) 8，观察后可以发现就是把循环包了起来。 论文中证明了，w.t.o.的组件的头的集合就是一个admissible set of widening points。则对于上图，admissible set of widening points为​，即程序中的循环头。 论文定义了w.t.o.的同时定义了两种迭代策略：iterative strategy和recursive strategy。我们用$[…]表示迭代直到稳定。那么给定一个：。对于，它的迭代是这样的：12[34567]8。而对于，它的迭代是这样的：12[34[56]7]8$。 论文后面关于构建w.t.o.的算法就没看了，没看太懂。 关系型抽象域 上面讲的区间域有缺陷之处，在于无法反映多变量之间的关系。例如对下面的例子： 若用区间域对x y 分别分析，结果如下： 可以看到，x确实卡住了，但和x很有关系的y却并没有分析出什么名堂。 此时就需要运用关系型抽象域了。比如在这里，我们可以用八边形抽象域来使结果更精确。 用一点高中的线性规划知识可以知道，加上对和的单独约束可以表示一个八边形。于是在区间分析的基础上，我们再加入对，的记录。如下所示。 八边形抽象的转换函数设计的基本思路和之前类似：给定抽象域限定的所有取值，考虑语句执行之后的所有取值，然后重新计算八边形的范围。 因为八边形抽象的整体计算比较复杂，详细的计算规则可以参考原始论文。如果采用八边形抽象域，可以确保在上面的程序中推导出和的相反数关系，即x+y的区间始终是​。 当然原始论文太难看了，等没事时再仔细看吧，符号太复杂了。 同时还可以参考的是Antoine Miné氏的博士毕业论文，Weakly Relational Numerical Abstract Domains ，其中P122页的举例简单提及了此处不变可用来限制的区间的原因。 抽象解释涵盖的范围很大。其重点主要就在于抽象域的设计。在我们使用抽象域来进行程序分析时，一般会采用 over-approximation（上近似），也就是说要把程序状态都包在里面，以防有所遗漏。我们来直观解释下，比如对于下面这个图，每一个蓝色的小十字叉代表了这个程序的一个具体状态，那用我们刚才列举的这些抽象域去包住程序状态时，在几何上就需要把这些蓝色的十字叉给包进来。如果我们用多面体抽象域，那么对应的二维形状就是一个多边形。我们用图中这个深绿色的多边形就可以把所有十字叉都包进来。假设图中外侧的红色的地方表示这个程序会出错，比如说会出现数组越界、除零、整数上溢等之类的错误。我们会发现这个程序的所有的状态都被这个多边形包住了，而且这个多边形跟这个红色的区域并没有相交，那也就是说，我们用多面体抽象域证明了在这个程序点处这个程序是不会出错的。但如果用浅绿色的区间抽象域，就会发现还存在误报的情况，那么就不如多面体抽象域好。 以上就是抽象解释的理论部分内容。SVF中实现了抽象解释，可能会再写另一篇博客来讲讲吧。 参考文献网址数值程序分析 https://zhuanlan.zhihu.com/p/602442089 区间分析 https://zhuanlan.zhihu.com/p/325018769 抽象解释 https://zhuanlan.zhihu.com/p/662630157 https://blog-blockchain.xyz/pl/data-flow-analysis/ https://blog-blockchain.xyz/pl/abstract-interpretation/ 熊英飞 软件分析技术 https://xiongyingfei.github.io/SA_new/2023/ 李樾 谭添 软件分析 https://static-analysis.cuijiacai.com/ Abstract Interpretation https://en.wikipedia.org/wiki/Abstract_interpretation Effective Chaotic Iteration Strategies With Widenings https://mp.weixin.qq.com/s/lh87a0rs4DzL-LIp4Bj0HA 论文Effective Chaotic Iteration Strategies With Widenings 抽象解释及其应用研究进展 Weakly Relational Numerical Abstract Domains The Octagon Abstract Domain","categories":[],"tags":[]},{"title":"Can AI-Generated Text be Reliably Detected?","slug":"Can AI-Generated Text be Reliably Detected","date":"2023-08-25T11:20:57.294Z","updated":"2024-03-02T14:07:19.580Z","comments":true,"path":"Can AI-Generated Text be Reliably Detected.html","link":"","permalink":"https://asparticguan.github.io/blog/Can%20AI-Generated%20Text%20be%20Reliably%20Detected.html","excerpt":"","text":"Can AI-Generated Text be Reliably Detected?作者在文中提出的观点是人类和AI生成文本序列分布之间的TV距离（total variation distance）在逐渐减小。 AUROC(D)（检测器D的AUROC面积）和TV(M,H)间的关系为： 也就是说，随着TV(M,H)逐渐减小至0，即在模型语言越来越接近自然语言时，AUROC(D)趋近于1/2，也就是接近随便猜的水平了。 比如，在大多数实际场景中，如果探测器能够达到高TPR（例如90%）的同时保持低FPR（例如1%），则被认为是好的。当两个分布重叠超过11％时（即TV距离小于0.89），这是不可能实现的。 作者强调说并没有对H的分布做任何假设，转换思路的话这个结果还可以用于模仿特定写作风格上。 对于用了伪随机数生成器的模型，虽然这种模型的TV接近1，但也可以证得： ==不对啊，是正数，这不是提高了AUROC上限吗。== 不可能性结果也可以用来表明，即使嵌入了水印，由AI生成的文本通过改写工具进行简单改写也很难被检测到。对于由语言模型生成的序列s，我们将M和H分别设置为由释义器和人类产生的与s有类似意思的句子分布。释义器的目标是使其输出分布与人类生成的句子的分布相似，并且在TV距离方面接近于该分布。上述结果对重新构造后AI文本检测器性能施加了限制。 这项分析的目的是警告不要过于依赖声称可以识别AI生成文本的检测器。在实际部署之前，应该独立而严格地评估可靠性，最好是针对旨在规避检测的语言模型。 在第二章，作者讲述了如何用一个改写工具让所有水印方法破功。 第三章中的主要问题： 不是很清楚这个公式是怎么来的，如果是TV距离的定义式的话，TPR和FPR为什么能够代表M和H的分布？首先的一个问题就是TV距离要求的式中出现的两个分布必须在同一定义域内，但TPR=TP/(TP+FN)，定义域为机器生成的句子；FPR=FP/(TN+FP)，定义域为人生成的句子，定义域就不同。","categories":[],"tags":[]},{"title":"Three Bricks to Consolidate Watermarks for Large Language Models","slug":"Three Bricks to Consolidate Watermarks for Large Language Models","date":"2023-08-21T02:23:01.833Z","updated":"2024-03-02T14:15:45.272Z","comments":true,"path":"Three Bricks to Consolidate Watermarks for Large Language Models.html","link":"","permalink":"https://asparticguan.github.io/blog/Three%20Bricks%20to%20Consolidate%20Watermarks%20for%20Large%20Language%20Models.html","excerpt":"","text":"Three Bricks to Consolidate Watermarks for Large Language ModelsAbstract在当前情境下，区分生成文本和自然文本的任务变得越来越具有挑战性。在这种情况下，数字水印技术成为了一种有前途的技术，可以将生成的文本归属于特定模型。它改变了采样生成过程，以便在生成输出中留下一个不可见的痕迹，从而方便后续检测。该研究基于三个理论和实证考虑因素巩固了大型语言模型的数字水印技术。首先，我们引入新的统计测试方法，在低误报率（小于10-6）时仍能提供强健的理论保证。其次，我们使用自然语言处理领域中经典基准测试比较数字水印技术的有效性，并获得关于它们现实世界应用价值方面洞察力。第三，在可以访问LLM（Large Language Model）场景下开发高级检测方案，并进行多位数码水印。 Introduction首先，误报可能会在需要结果的完整性和准确性至关重要的情况下产生严重后果，例如错误指控用户制造虚假新闻或学生在考试中作弊。然而，当前方法[18]、[19]将研究重点放在灵敏度（真阳性率：TPR）上，而不是特异度（与假阳性率FPR相关）。尚未以有趣的规模（超过1k个负面示例）对FPR进行经验检查。我们的大规模实验揭示了之前工作的假设不成立，并且它们的检测阈值在低FPR时大大低估了误报数量。本文提供了基于统计学原理保证假阳率和准确p值适用于现实环境中测试数据集合。我们通过实验证明这些方法可以接近完美地控制FPR。 其次，我们比较了水印方法，并分析了水印对传统自然语言处理（NLP）基准测试的实际影响。目前的水印评估主要考虑与原始LLM分布的偏差，例如使用困惑度。这与LLM文献中的情况相反，在LLM文献中，模型通常根据其有效性进行评估，例如在自由形式完成任务时如问答等方面。这些评估更能够提供有关模型在下游任务中实际能力的信息。 第三，我们将这些算法扩展到高级检测方案。当在检测时可以访问LLM时，我们提供最佳统计测试。我们还调查多位水印（将二进制消息隐藏为水印），而当前方法仅处理零位水印。这不仅允许确定文本是否由带有水印的LLM生成，还可以识别生成它的模型版本。 Technical BackgroundLarge Language Models (LLMs)Watermarking Text GenerationModification of the Distribution这种就是Kirchenbauer et al.在ICML上的方法。大模型水印技术：判断文本是不是LLM生成的 - 知乎 (zhihu.com) 下面这个引用还会多次出现，称作[18]。 Modification of the Sampling这个部分就是OpenAI的PPT内容，我觉得论文中这一部分讲得有大问题（怎么可能是鼓励和同时大？），下面讲一下PPT的思路。 给定: 1. Token 和生成第个 token 所需的概率分布 ；2. 一个伪随机函数 ，将最近的 c 个 tokens 映射到 。 目标：选出第个token ，该token不仅符合的分布，且同时由激发生成。 在检测阶段中，我们知道，亦即知道，但不知道。 在每个位置处，选择使最大的token ，这个方法的含义是，再大，如果很小也不行；小了，大点也可以。 在检测阶段，计算。如果该值超过某一阈值，则判定为是GPT生成的。这个算式的值很大，说明很大，即说明是机器生成的可能性很大。这也就是在不改变结果概率分布的情况下通过改变抽样方法来加入水印。 下面这个引用还会多次出现，称作[17]。 Quality-Robustness Trade-off对于这两种方法，我们可以通过调整水印强度来权衡生成质量和鲁棒性。在[18]中，增加δ参数会增加绿色令牌的生成，但也有可能包含不太可能的令牌。在[17]中，增加温度系数θ具有相同效果，因为它使概率向量变得平坦化，从而降低了相对于的重要性。 Key Management在[18]中，生成绿名单或采样的秘密密钥必须具有广泛的差异性。固定密钥会导致安全问题并使文本生成有偏差。一种可能性是将其与时间相关联。因此，每个令牌的秘密密钥都不同。然而，在检测阶段会存在同步问题（例如删除句子时）。确保自我同步的常见做法是使该密钥依赖于前h个token：，其中H是一个加密哈希函数，k是主密钥——如图1所示。该机密是在时间t初始化随机数发生器（random number generator，RNG）的种子。反过来，RNG用于生成绿名单或采样 。该窗口宽度定义了关键字多样性和水印鲁棒性之间的权衡取舍。特别情况下当h=0时, 所有令牌使用相同密钥(k(t)= k)，这使得水印对文本编辑攻击仍具有很强鲁棒性。 检验假设：“文本是自然的”（人工编写或没有水印编写），备择假设：“文本有水印”。目前的方法[17]，[18]使用Z-test来近似得出分数的潜在分布。这个统计假设检验当标准差已知时样本均值是否与其期望显著不同。 其中µ和是在检验假设下，即分析文本未被水印化时每个标记的期望值和标准差。Z检验通常用于大样本量，在检验假设下假定正态分布，这得益于中心极限定理。该假设对于计算p值至关重要，p值即在成立的条件下，出现该实验结果或更极端情况的概率值。 其中是正态分布的累积分布函数（）。在检测时，我们固定一个假阳性率（FPR），如果，则将文本标记为有水印。 其实就是什么呢，Z分布就是标准正态分布，算的这个p-value就是从x坐标轴上Z点开始向右的面积。如果这个面积太小了显然就是极端情况了，也就是说不自然，有水印。 至于说为什么说成小于FPR。首先FPR可称作误伤率，即预测为负的正样本数/预测为负的总数。在下，有那么千分之一万分之一的可能确实自然写出来的东西Z也太大，我们把这个范围的边界找到，在此边界范围外的一律判负样本，这也是误伤了，所以是小于FPR。 Reliability of the Detection这一段的意思总结起来就是在说不应当用正态分布去拟合的分布，并且尝试了贝塔分布和伽马分布，发现结果确实好些。 Empirical Validation of FPR with Z-Scores到目前为止，关于假阳率（FPR）的检验主要集中在小规模实验中，大约涵盖了约500个负样本。本节旨在进行大规模研究，以验证当前的方法。我们从多语种维基百科中选择了10万份文本，以涵盖自然文本的分布。我们使用LLaMA的分词器对它们进行分词，并从每个文本中截取T = 256个标记。我们在进行检测测试时，会对用于初始化随机数生成器（RNG）种子的上下文窗口长度h进行变化。我们将这个实验重复进行10次，每次使用不同的主密钥，这将为每种方法和h值产生100万次下的检测结果。对于绿色列表水印的检测，我们使用γ = 0.25。 图2a比较了实际和理论FPR。显而易见的观察结果是，在实际情况下，理论保证并不成立：实际的FPR要比理论值高得多。我们还在实际操作中观察到，P值的分布不是均匀的（在下应该是如此）。此外，水印上下文窗口h越大，我们就越接近理论保证。在实际操作中，人们需要h &gt;&gt; 8才能获得可靠的P值，但这会使水印方法无法抵挡生成文本的攻击，因为它会影响同步。 下面讲一下这个图2是怎么回事。我们首先设定一个FPR值作为理论FPR值，打个比方我们设个，然后就拿这个FPR值去套上面的检测方法，然后套出来就去看有多少加了水印的被误报成人写的了，就能算出真正的误报率了。结果一看好家伙真误报率能有，那显然是不行的。 New Non-Asymptotical Statistical Tests高斯假设关于Z检验在短文本中失效。这里提供了两种非渐近性测试方法，它们可以减小经验和理论FPR之间的差距，特别是对于低FPR值，如图2所示。 Non-asymptotic tests（非渐近性检验）是统计学中一类用于小样本情况的假设检验方法。与渐近性检验（asymptotic tests）不同，非渐近性检验不依赖于样本大小趋近于无穷大的假设，而是专注于有限样本情况下的检验方法。 对[18]，在假设下，我们假设事件（是处的密钥）以概率γ发生，并且这些事件是独立同分布的。因此，的分布是具有参数T和γ的二项式分布。考虑一个得分为s的待检测文本。p值被定义为在下获得比s更高得分的概率： 的累积分布函数是正则不完全贝塔函数 。 ==这个真没学过== 对[17]，在 下，我们假设待检测文本和是相互独立的，即 （独立同分布）。于是遵循分布。分数的p值是: \\text { p-value }(s)=\\mathbb{P}\\left(S_T>s \\mid \\mathcal{H}_0\\right)=\\frac{\\Gamma(T, s)}{\\Gamma(T)},是上不完全伽玛函数。在的情况下分数将会更高一些（附录A中给出证明），所以p值会比较小。 Rectifying the Detection Scores即使使用新的统计测试方法，实证FPR仍然高于理论值，这对检测的可靠性是有问题的。事实上，Kirchenbauer等人在[18]中提到随机变量只是伪随机，因为重复窗口会生成相同的密钥。这甚至可能发生在短文本和格式化数据中。例如，在项目符号列表中，“\\n\\n*_”标记序列经常重复，如图3所示。重复降低了计算p值所需的独立性假设。 我们在检测阶段尝试了两种简单的启发式方法来缓解这个问题。第一种方法只考虑水印上下文窗口在检测期间还没有被看到过的token。第二种方法是对由{水印上下文+当前标记}构成的h + 1元组尚未被看到的标记组进行评分。其中第二种方法更好，因为它计算了更多ngram，并且具有更好的TPR。它也可以处理h = 0的特殊情况。 当时，我们在图2c中报告了选择不对已经看到的h + 1元组进行评分时的实验和理论FPR（其他得分策略结果相同）。现在它们完全匹配，除了当h = 0时，FPR仍然稍微低估了一些。 总之，通过新的统计测试并仅对尚未看到{水印上下文+当前标记} 的标记进行评分，我们保证了FPR。 Watermark EvaluationRobustness Analysis我们现在通过分析检测水印文本时的TPR来比较水印方法。对于检测，我们采用先前的统计测试和评分策略。如果一个文本的p值低于，我们将其标记为带有水印的文本，确保FPR=。在这些实验中，我们接近聊天机器人场景。我们使用Alpaca数据集[28]中前1k个提示来提示Guanaco-7b [27]（LLaMA的指令微调版本）。对于生成，我们使用p = 0.95 的top-p抽样，在[18]的情况下使用温度θ = 0.8 和γ = 1/4 。 我们通过随机替换概率为0.3 的标记来模拟同义词攻击（其他攻击在相关工作[19]中研究）。 表II报告了不同强度水印（见第II-C节）的TPR以及生成文本与未加水印文本之间S-BERT [29]相似性得分，以衡量由水印引起的语义扭曲程度。表II中结果揭示了不同行为方式。例如，[18]可以更好地控制权衡水印强度和质量, 其TPR值范围从0.0到0.9。而[17]则更加一致，但即使S-BERT得分大幅降低时也无法实现高于0.8的TPR。 水印上下文宽度也有很大的影响。当 h 较低时，我们观察到重复出现的次数更多，因为生成器容易偏向于某些标记的重复。这导致平均 S-BERT 分数低于 0.5 并且无法完成。另一方面，较低的 h 使水印更加稳健，特别是对于 [18]。还要注意的是，h 对分析标记数量有影响，因为我们仅对尚未看到过 h + 1 元组的标记进行评分（参见第 III-C 节）。如果 h 很高，则几乎所有这些元组都是新元组；而如果 h 很低，则重复元组出现的机会增加。例如，在我们的情况下，得分标记的平均数量约为 100（h = 0），150（h = 1 和 h =4）。 是温度系数，是绿名单的偏置。 ==啥子哟，这不是完全防不住攻击吗？== Impact of Watermarks on Free-Form Generation Tasks以前的研究使用诸如困惑度或相似性得分等畸变指标来衡量质量的影响，如表II所示。然而，这些指标并不能说明LLMs对下游任务的实用性[25]，而正是这些任务才是LLMs真正感兴趣的领域。事实上，在需要非常精确答案的任务中，给LLMs加水印可能会有害。因此本节将定量评估典型NLP基准测试对其产生的影响，以评估加水印技术是否可行。 通常通过比较纯文本生成样本与一组目标参考文献（自由形式生成）或按多项选择题方式比较预定义选项集合的可能性来评估LLMs。后者在加水印情况下没有意义，因为只会影响抽样过程。因此我们限制我们的评估范围在自由形式生成任务上进行。我们采用与LLaMA相同的评估设置：1）封闭书问答（自然问题[30]、TriviaQA [31]），报告5-shot完全匹配性能；2）数学推理（MathQA [32]、GSM8k [33]），报告无多数投票情况下完全匹配性能；3）代码生成（HumanEval [34]、MBPP [35]），报告pass@1分数。对于[18]，我们在贪心解码之前应用分布偏移，δ=1.0。对于[17]，我们将概率向量的top-p设置为0.95，然后应用算法给出的抽样。 表I报告了LLaMA模型在上述基准测试中的性能，在不同窗口大小h下有无水印时进行比较。加水印并没有显著影响LLM的性能。分布偏移方法（II-B1）比基于采样的方法（II-B2）略微更有害，但与香草模型相比差异很小。有趣的是，随着模型规模增大这种差异会减小：具有更高生成能力的模型受到加水印影响较小。一个可能的解释是更大模型全局分布更好，并且因此对小扰动更稳健。总体而言，在下游任务上评估表明加水印可能会引入事实错误，并且这些错误不能通过困惑度或相似性得分很好地捕捉到。 从这里来看，Aaronson et al.这个模型属于加水印效果不如Kirchenbauer et al.，但对模型本身影响更小。其实这也显然，有得必有失了。 Advanced Detection Schemes这一段终于是作者自己的一点东西了，提了几种可能更好的检测改进方法。 Neyman-Pearson and Simplified Score Function在下， ；在下， 。故奈曼-皮尔逊得分函数为： S_T=\\sum_{t=1}^T \\ln \\frac{f_{\\mathcal{H}_1}\\left(\\mathbf{r}_{x^{(t)}}\\right)}{f_{\\mathcal{H}_0}\\left(\\mathbf{r}_{x^{(t)}}\\right)}=\\sum_{t=1}^T\\left(\\frac{1}{\\mathbf{p}_{x^{(t)}}}-1\\right) \\ln \\left(\\mathbf{r}_{x^{(t)}}\\right)+A不依赖于，可被丢弃。有两个缺点：（1）检测时需要LLM算 ；（2）没有p值的公式。 这个问题可以通过使用Chernoff bound来解决，但不能保证其紧密性：，其中c是方程的解，。实验表明，这种检测方法对于带水印的文本产生极低的p值，但它们很脆弱：任何攻击都会将p值增加到原始检测方案（[17]的）甚至更高的水平。另一种选择是去除前面一个括号的加权： S_T=\\sum_{t=1}^T \\ln \\left(\\mathbf{r}_{x^{(t)}}\\right)其p值为，但这个方法还不如原方法。 Multi-bit Watermarking在数字水印技术中，零比特水印（Zero-bit watermarking）和多比特水印（Multi-bit watermarking）是两种常见的水印类型。这两者主要的区别在于嵌入到被保护数据中的水印信息的数量。 零比特水印（Zero-bit watermarking）：零比特水印并不包含任何特定的信息或数据，而是通过对数据进行一定的修改，产生一种可以检测到的模式或结构。它的主要目的是通过简单的存在性检测来证明数据的所有权或真实性，而不是传递特定的信息。例如，一种常见的零比特水印方法是在图像的频域中添加一个特定的噪声模式。如果检测到这个模式，那么就可以确认该图像包含水印，从而证明该图像的所有权。 多比特水印（Multi-bit watermarking）：多比特水印包含一串特定的数据或信息，可以是一段文本、数字或者其他类型的数据。这种类型的水印不仅可以用于证明数据的所有权，还可以用于传递特定的信息。例如，一个多比特水印可以包含版权信息、创作日期、作者信息等。多比特水印的主要挑战在于如何在不影响数据质量的前提下，嵌入尽可能多的信息。 假设某意外事件在一次实验（活动）中发生的概率为，则在n次实验（活动）中至少有一次发生的概率为。 将零比特水印方案转换为多比特水印方案相当容易，只需为每条信息关联一个秘密密钥即可。解码使用每个密钥进行检测，并且解码的消息是与给定最低p值的密钥相关联的那条消息。全局p值变成了，其中是可能的信息数量。 Conclusion这项研究提供了理论和实证洞见，这些洞见在关于LLM水印的文献中被忽略。主要观点是现有方法采用偏差的统计测试，导致不准确的误报率。我们还引入了修复措施、额外评估设置和检测方案来巩固LLM水印。进一步的工作还应该调查如何适应更复杂的抽样方案（例如像[18]中所做的beam search），因为某些任务（如图像字幕）使用这些方法可以获得显着更好的结果。 总体而言，我们认为数字水印既可靠又实用，在生成模型领域相对较新时已经具备许多识别和追踪LLM输出技术上优势。 总结这篇文章切入点确实很强，抓住Z值的分布问题谈论文的假设不成立，从而提出自己的分布假设。同时还对现有的两个模型效果进行了评测。总的来讲我觉得评判数字水印好不好主要就这几个方面：在检测时能不能被区分出来，对原来的文本生成有多大影响、抗干扰能力怎么样。文中的模型主要关注了前两点，而对第三点着墨较少。另外关于多比特水印我觉得也是一个很好的方向。还有就是自然文本的水印方法能否直接用于一些格式特殊的文本（如代码等）上，也是一个关注点。下一篇论文是针对Kirchenbauer et al.的攻击，看一下对水印的攻击到底发展如何。","categories":[],"tags":[]},{"title":"Just Fine-tune Twice- Selective Differential Privacy for Large Language Models","slug":"Just Fine-tune Twice- Selective Differential Privacy for Large Language Models","date":"2023-08-08T12:17:03.696Z","updated":"2024-03-02T14:16:33.342Z","comments":true,"path":"Just Fine-tune Twice- Selective Differential Privacy for Large Language Models.html","link":"","permalink":"https://asparticguan.github.io/blog/Just%20Fine-tune%20Twice-%20Selective%20Differential%20Privacy%20for%20Large%20Language%20Models.html","excerpt":"","text":"Just Fine-tune Twice- Selective Differential Privacy for Large Language ModelsEMNLP 2022 Abstract保护大型语言模型免受隐私泄露的影响在其在现实世界产品中广泛应用时变得越来越重要。然而，将可证明具有隐私保证的差分隐私（DP）应用于这些模型仍然具有挑战性，因为模型效用和隐私损失之间存在权衡。利用语言数据中敏感信息往往是稀疏的这一事实，Shi等人（2021年）形式化了一种名为选择性差分隐私（SDP）的DP概念扩展，以保护仅由策略函数定义的敏感标记。然而，他们的算法只适用于基于RNN的模型。本文提出了一个新颖框架Just Fine-tune Twice (JFT)，可以实现最先进的基于Transformer的大型模型（指GPT2）对SDP进行处理。我们方法易于实施：首先使用经过删除敏感信息后域内数据对模型进行微调，然后再次使用原始域内数据通过private training 机制进行微调。此外，我们研究了策略函数不完善的情况，即错过敏感标记，并开发了系统化的方法来处理这种情况。实验证明与之前基准相比，我们方法能够取得较强效用。我们还通过金丝雀插入攻击对SDP隐私保证进行了实证分析。 Introduction随着自然语言处理（NLP）的快速发展，保护NLP模型免受泄露隐私信息的重要性日益增加。先前的研究尝试通过在这些模型上应用差分隐私（DP，Dwork等人，2014）来解决这一挑战（McMahan等人，2018；Li等人，2021）。然而，现有的DP学习算法存在用户控制有限和效用低下的问题，因为它们保护每个训练示例的全部内容（例如完整句子），而不考虑用户对隐私偏好，并且当只有训练示例中的部分信息是敏感时往往过于悲观。这个问题在NLP领域特别相关，因为NLP训练数据通常与稀疏领域相关的私密信息混合在一起，并且并非所有标记都需要得到保护。例如，在句子“我的社会安全号码是123-45-6789”中，只需保护实际社会安全号码的最后几个标记即可。 事实上，DP的定义并不妨碍我们仅保护数据中的敏感部分。具体而言，DP确保数据分析算法的输出在相邻数据集之间基本保持一致，同时提供了根据特定应用环境调整相邻关系定义的灵活性。Shi等人（2021年）最近提出了DP的一个实例化方法，称为Selective-DP（SDP），它将相邻数据集定义为仅在训练示例的敏感部分有所差异，并且因此SDP只选择隐藏敏感部分的差异。SDP特别适用于NLP和其他非结构化、高维度数据，在这些数据中，敏感信息只占很小一部分。但是他们用于实现SDP的隐私机制存在三个问题：1）需要对模型有大量知识才能区分私有变量和公共变量，并且不清楚他们针对循环神经网络设计的算法如何扩展到现代基于Transformer架构的NLP模型；2）它只评估了显式私有实体而没有考虑上下文敏感信息；3）它不能保护未检测到的敏感标记；这些限制限制了SDP在真实场景中的适用性。 大型语言模型（LLMs）（Vaswani等，2017年）在自然语言处理领域取得了巨大的成功。它们在大量公共文本数据上进行预训练，因此擅长捕捉通用的语言结构。在NLP中，一种常见的做法是对这些LLMs进行下游任务的微调。这种微调过程在私有训练环境中也表现良好。之前，Yu等人（2021a）证明了使用私有数据，在现成的LLMs基础上额外微调一小部分参数可以达到与非私有baseline相当的性能。受到他们研究结果的启发，本文提出了一个两阶段微调隐私机制——Just fine-tune twice (JFT)，以实现LLMs的SDP。我们不直接使用现成模型进行单次微调，而是采用两个微调步骤：首先我们将下游任务领域内数据进行删除，并使用这些经过删除处理后的领域内数据对模型进行微调（删除-微调），然后再对原始私有数据进行私有化微调（私有-微调）。这个额外的删除-微调步骤允许模型直接从领域内数据中学习信息，从而为第二个私有-微调步骤提供更好的模型初始化。此外，在删除-微调步骤中，我们证明即使只有有限的公共数据（可以进行手动筛选），JFT也比单次微调基准线具有更好的效用。另外，我们还可以应用轻度加噪优化器和隐私放大技术来保护未被检测到的敏感标记。 我们的贡献如下。首先，我们提出了一种有效且可推广的隐私机制，以实现大型语言模型在各种自然语言处理任务中的差分隐私保护。其次，我们设计了不同隐私级别（显式和上下文敏感数据）的秘密检测器，并研究了它们对模型的影响。第三，我们的方法可以利用少量公开数据来获得更好的效用，并通过轻度噪声优化器和隐私放大来缓解遗漏敏感token问题。最后，我们证明与常见观点相反，即隐私与效用相互冲突，在学习任务中进行差分隐私训练并不一定会损害效用，因为数据中包含的个人信息可能与学习任务无关。 ==一开始想的是把论文的成果用在代码上，但这篇论文重点在于强调选择性保护。对一段代码而言，只保护一部分是否有意义？暂时想到的是保护一部分可能就可以达到保护全部的效果。这样甚至不需要这篇文章里面的策略函数了……== PreliminaryDifferential Privacy邻近关系捕捉了被保护的内容。传统的差分隐私文献认为，邻近数据集是指在一个训练样本上有所不同；因此，相应的差分隐私保护整个训练样本。我们用和表示在这种传统邻近关系定义下实现的隐私参数。鉴于语言数据中敏感信息的稀疏性，这种邻近关系实例显然过于悲观（也就是说对NLP而言可以放松一些）。Shi等人（2021）提出了选择性差分隐私（SDP），它将邻近数据集定义为在训练样本的敏感属性上有所不同；因此，SDP只选择性地隐藏敏感部分的差异。在自然语言处理的背景下，一个训练样例可以是一个句子或一个段落，取决于任务，而属性则是单一的token。 本文将重点设计学习算法来实现SDP。形式上，SDP依赖于策略函数，在特定应用中指定要保护的训练示例中的敏感信息。 Policy FunctionA policy function decides which attributes of an example are public or private is the number of attributes in . 根据策略函数在大规模语料库中手动检测私人信息往往代价高昂。在这种情况下，可以建立自动的秘密检测器来识别敏感属性。一个简单的秘密检测器示例是使用正则表达式捕获电话号码。然而，秘密检测器可能会漏掉一些私人属性并产生假阴性，直观上会削弱隐私保证。现有研究（Doudalis等，2017；Shi等，2021；Zhao等，2022）选择性地保护数据要么假设存在完美的检测器，要么使用过于保守的低假阴性但高假阳性的检测器。本文提供了解决这个问题的替代方法，并实现更好的隐私-效用权衡（第3节）。 SDP通过F定义了-Neighbors。 -Neighbors那个抽象定义又长又没用，直接看底下的举例。 根据定义，包含“My ID is 123”的数据集和包含“My ID is 456”的数据集是-Neighbors（因为除了实际的ID号码外，其他标记都相同），而包含“Hi, there”和包含“Hello, there” 的数据集不是-Neighbors，因为它们唯一不同的标记并不敏感。SDP算法保证攻击者查看输出结果时无法区分-Neighbors之间的差异。 Selective Differential PrivacyJFT: Just Fine-tune Twice 现在我们描述JFT，这是一个两阶段的隐私机制，用于实现大型语言模型的SDP。在第一阶段中，我们使用秘密检测器对私有数据进行删除以获得已删除版本，并以保护隐私的方式从中学习一个“删除模型”。在第二阶段的私有微调中，我们进一步对删除模型（来自第一阶段）使用私有优化器在私有数据上进行微调，以实现SDP保证。 Phase 1:Redacted-fine-tuneDirect Usage 如果秘密检测器屏蔽了中的所有敏感信息（当足够小以支持彻底检查或者检测器非常保守并删除了大部分关键信息时），我们可以直接使用经过编辑的来用公开且无噪声优化器（如SGD）对模型进行微调。 Selective Manual Screening 如果秘密检测器不完善，我们可以从中选择一个 affordable 的子集，然后手动清理所有被错过的秘密。然后，我们使用公共优化器对这个小型清理过的子集进行微调模型。实验结果显示，即使只使用少量经过清理的领域内数据，所得模型的表现仍优于传统的差分隐私学习算法，这些算法对每个单独的 token 过于谨慎地进行保护。 Lightly-Noised Fine-tuning 当探测器存在缺陷时，除了手动筛选出被错过的敏感信息之外，我们还可以使用私有优化器对包含被错过敏感标记的 进行训练。因为被错过的标记只占据了 的一小部分，直观上来说，为了确保这些被错过的标记的隐私安全性，所需噪声要比确保整个 隐私安全性所需噪声更小。我们提出利用子采样进行隐私放大（PAS）（Balle 等人，2018）来计算与私有优化器相关联的隐私参数。PAS 的基本思想是，在数据的随机子样本上执行差分隐私机制，并且如果一个数据点未包含在子样本中，则无法泄露任何关于它的信息。通过这种方式，我们可以增强隐私保证。在我们的场景中，我们需要保护那些被漏检到的敏感标记。如果我们知道秘密探测器漏检率 （即 漏检到敏感标记数目总标记数目），则可以通过使用子采样比例 来计算隐私预算 的值。 请注意，PAS的应用要求任何批次中出现的错过标记数量相同，但这并不一定成立。因此，从隐私放大计算得出的隐私参数是对实际隐私损失的经验估计。在实践中，秘密检测器的缺失率是未知的，我们需要进行估计（表示为 ）。然后，在基于时刻账户法进行隐私参数计算（Abadi等人，2016）时将原始采样率p0更改为，并根据预定义的隐私预算计算注入到每个私有优化器迭代中的噪声。 Phase 2: Private-fine-tune在第二阶段中，我们使用来自第一阶段的已编辑模型进行初始化，并使用原始私有数据D和私有优化器（例如DPSGD（Abadi等人，2016）或任何其他实现差分隐私的更高级私有优化器）对其进行微调。与Shi等人（2021）中的隐私机制不同，我们的算法不需要了解模型或任务的知识，因此可以轻松应用于不同的模型，如GPT2（Radford等人，2019）和Roberta（Liu等人，2019），以及不同的任务，如语言生成和自然语言理解。更多实施细节请参见A.2节。 与传统差分隐私训练相比，我们的算法引入了一个额外阶段，在该阶段上进行了数据编辑和常规非加噪训练。事实上，额外阶段的计算成本远低于DP学习最初产生的成本，因为额外的第一阶段不需要昂贵的逐样本梯度剪辑和加噪操作。而且数据编辑是一种常见且熟悉的首步操作。此外还存在大量现成工具可供批量进行数据编辑。 Secret Detectors of Different Levels 用一张图来解释。使用现成的实体识别工具来检测文字中的敏感信息。 Experiments我们在两个自然语言处理（NLP）任务上进行实验：1）自然语言理解（NLU，在GLUE数据集上）和2）语言生成（在Wikitext-2和ABCD数据集上）。 Datasets 1）GLUE（Wang等，2018）是一个广泛使用的多任务基准数据集，用于NLU任务。它包含了一些敏感信息，如姓名和日期。2）Wikitext-2（Merity等，2017）包含了带有私人信息（如姓名和日期）的维基百科文章。3）ABCD（Chen等，2021）是一个人类-人类的客户服务对话数据集，在真实场景下收集，其中包含用户的私人信息，如姓名和订单编号。 Models 我们在NLU分类任务中使用Roberta（Liu等，2019），在语言生成任务中使用GPT2（Radford等，2019）。由于计算限制，我们在实验中使用Roberta-base和GPT2-small。我们在实现中使用了Li等人（2021）中的高效DPSGD（差分隐私随机梯度下降）方法。根据先前的研究（Li等，2021；Yu等，2021a），更大的DP模型通常可以取得更好的结果，因此我们预期更大的SDP模型将获得更好的性能。 Baselines 1）No-DP：模型使用常规的Adam优化器（Kingma和Ba，2014）进行微调，没有额外的噪声，因此没有任何隐私保证（即）。2）DPSGD：模型使用传统的DPSGD方法进行微调（Abadi等，2016），其中梯度在每次梯度下降迭代中进行剪裁和加噪（我们使用DP-Adam变种，其中优化器为Adam，但其梯度隐私化与DPSGD相同，我们保留术语DPSGD，因为这更容易理解）。虽然DPSGD最初是为了实现保护整个训练样本的DP保证，但它也可以实现相同隐私参数下的SDP保证（即和）。3）CRT：模型使用最近提出的机密消除训练（Zhao等，2022）进行训练，实现，。，是与SDP相关但与SDP不同的新定义，它确保秘密信息与&lt;MASK&gt;标记之间不可区分，因此其隐私参数不能直接与SDP进行比较。因此，我们对CRT和SDP添加相同数量的噪声，在图2和3中通过金丝雀插入攻击进行经验比较SDP和CRT，并在附录的表6中报告效用。4）Redacted：我们还展示了Redacted模型的效用，因为它们也是隐私保护的。请注意，当秘密检测器完美时，已消除模型具有完美的SDP隐私保证（即）。然而，它不允许模型从敏感标记中学习。相比之下，JFT允许模型从敏感数据中学习，具有隐私和效用之间的灵活可调平衡。此外，它提供了在存在不完美的秘密检测器的情况下提供量化隐私的方法。 Our Models 1）JFT：这是我们的JFT模型，直接在第一阶段使用已消除的数据。2）JFT +手动筛选：这是JFT，使用已消除数据的子集，在第一阶段手动过滤掉遗漏的秘密信息。3）JFT +轻噪声：这是JFT，在第一阶段根据估计的遗漏率添加轻微噪声。 Results我们展示了三个主要发现：1）秘密检测器对结果为JFT模型的影响取决于任务，但即使对于保守的上下文检测器（30%以上的标记被删除），JFT仍然比天真的DPSGD获得更好的结果（第6.1节）；2）尽管规模较小，使用手动筛选过的领域内数据仍然提高了JFT模型的效用（第6.2节）；3）轻微加噪优化器与隐私放大可以保护免受攻击的敏感标记（第6.3节）。隐私和效用之间总是存在权衡，因此较大的epsilon值会带来更好的效用但更差的隐私。在比较模型时，我们需要考虑在相似隐私预算下模型效用。各种隐私文献中通常使用1到3作为epsilon值（Yu等人，2021a; Li等人，2021; Zhao等人，2022）。在我们的实验中，我们预先计算了隐私参数，在训练结束时花费约为3个epsilon。 Related WorkConclusions在本文中，我们提出了JFT，它可以实现大型语言模型的选择性差分隐私。我们还设计了通用的秘密检测器，以在不同层面上提供保护，并研究其对结果SDP模型的影响，并通过选择性手动筛选和减少噪声进行私人训练来解决错过敏感标记的问题，这是由于隐私放大效应所证明的。结果表明，所提出的JFT生成具有强大性能且对金丝雀插入攻击具有鲁棒性的SDP模型。 总结其实读完全文还是感觉有点玄：这个微调两次居然就可以提高准确率？就是6.1节中的这段： 此外，如果JFT能够改进被编辑过的模型，则取决于任务。对于情感分析中的SST-2任务来说，私有微调步骤并不能改善被编辑过的模型。这是因为被编辑过的模型已经达到了很高的准确率（即使最差的准确率也有91.86，仅比94.8的公共模型下降了2.94），而在带有噪声梯度的私有数据上进行微调并不足以弥合这个小差距。但是对于存在较大差距（例如MNLI、QQP和QNLI） 的任务来说，JFT可以进一步改善被编辑过的模型。此外，随着隐私级别增加，被编辑模型与相应JFT模型之间的差距也变得更大：对于QNLI（低语境），差距为87.99-85.30=2.69；而对于QNLI（高语境），差距为87.06-82.81=4.25。这表明在私有微调步骤中，该模型确实从敏感标记中学习到了有效信息。 我觉得他消融实验没做完，感觉并没有排除是因为微调了两次才导致效果变好的，他应该重复做两次phase1或者重复做两次phase2，要不然感觉说服力还是不强。 别的方面暂时就是觉得挺有意思的，如果说想用到代码领域可以想个办法和AST结合，但提前预感这么麻烦的方法效果不会很好…… 还有一点就是全文都没讲到底是怎么fine-tune的模型，看了代码才看出来可能是lora。","categories":[],"tags":[]},{"title":"Controlling Large Language Models to Generate Secure and Vulnerable Code","slug":"Controlling Large Language Models to Generate Secure and Vulnerable Code","date":"2023-06-04T11:45:28.824Z","updated":"2024-03-02T14:06:44.267Z","comments":true,"path":"Controlling Large Language Models to Generate Secure and Vulnerable Code.html","link":"","permalink":"https://asparticguan.github.io/blog/Controlling%20Large%20Language%20Models%20to%20Generate%20Secure%20and%20Vulnerable%20Code.html","excerpt":"","text":"Controlling Large Language Models to Generate Secure and Vulnerable CodeAbstract正如标题所写的那样，作者提出了一种叫svGen的算法控制LM生成安全的程序。 Introduction针对现有的自动代码生成模型生成的代码常常不安全的问题，作者提出svGen。svGen的核心思想是学习两个特定属性的连续向量序列，称为前缀。通过这个“前缀”来生成包含特定属性的代码。 对svGen的训练很依赖有效的前缀，训练集是GitHub上修复过安全漏洞的代码。由于修复前和修复后的代码比起来可能只改动过很小一部分，所以作者还进一步把代码划分成了修改部分与未修改部分。在修改部分，使用conditional language modeling loss和contrastive loss来对控制代码生成进行优化。在未改变的区域中，使用 KL 散度来规范化前缀对代码分布所施加的扰动量。通过这些损失函数即可在不影响LM原有效能的情况下控制代码生成。 Background and Related WorkProblem Setup 对模型建模的结果。是前缀，有“sec”（安全）和“vul”（不安全）两个选项。在属性下生成的概率为在知道前文的情况下各单词出现概率之积。 svGen: Inference, Training, and Use CasesInference作者首先强调continuous prompts比离散的好，并且他的continuous prompts是PL专用的，跟NL无关。 然后还讲了prefix的长度，说经过实验最好长度为5。 另外作者还讲到了svGen特别方便，比如可以直接当作huggingface中Transformers的past_key_values参数。past_key_values是用于存储过去已经算出来过的key和value值。但在这里是怎么用上的还要往下看。 Training对svGen的训练存在一对矛盾：既要让前缀可以更好地控制模型，又不能降低模型原有的能力。作者通过巧妙的loss设计来解决矛盾。 Training Data and Token Masks如前所言，代码可能只被替换了很小一部分。对此作者使用了一个mask。对于每个训练程序，我们提取一个二进制掩码向量，其长度等于的长度。如果在代码更改的范围内，则将每个元素设置为1，否则设置为0。对于向量的粒度作者也做了考察。分为把改过的函数都换成1，只把改过的行换成1，只把有改过的字符换成1。 虽然字符级别的粒度最细，但如果用字符级别的话原始的不安全版本常常mask全为0（比如要把代码改安全的方法是加上什么东西）。所以最终采用的方法是安全版本采用字符级别粒度，不安全版本采用行级别粒度，这样子的效果最好。 总结起来，svGen训练数据集中的每个样本是一个三元组。由于我们的训练集是由代码对构建的，它还包含了具有相反属性的的另一个版本。 （我觉得这作者分段有问题，接下来都是损失函数了就不能另找一个大标题括起来吗） 各种Loss作者在前面就讲了为了调和一组矛盾，要设计出巧妙的loss，下面看看loss。 Conditional Language Modeling 第一个是条件语言模型，条件指的就是。其实这个损失函数就是正常的交叉熵损失，由于非零即一，故这个函数重点关注的是的部分写得好不好。 Contrasting Security and Vulnerability 这个损失函数的目的是让相反的属性不能生成。 Regularization with KL Divergence 由于只关注的部分写得好不好可能会影响的部分，用这个损失函数来控制部分在有条件和无条件时的生成情况，以保证两种情况下这部分内容是尽量接近的。 将上面三个loss合在一起就是总的loss： Low Training Data and Computation CostLoss是用来训练前缀的，由于把模型参数冻结了，所以训练成本其实很低，况且需要的数据量也很少。 Use CasesExperimental EvaluationDiscussion and Future Work首先，训练集太小，模型没法把握全部出错情况；其次，漏洞查找工具可能出错；最后，可解释性有待探索。 Conclusion总结这篇文章的亮点在loss设计，这个loss能确保模型的准确率提升。","categories":[],"tags":[]},{"title":"CodeBERT论文笔记","slug":"CodeBERT论文笔记","date":"2023-06-04T11:45:28.446Z","updated":"2024-03-02T14:20:19.620Z","comments":true,"path":"CodeBERT论文笔记.html","link":"","permalink":"https://asparticguan.github.io/blog/CodeBERT%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0.html","excerpt":"","text":"CodeBERT笔记摘要CodeBERT是一个针对PL与NL的双模态的预训练模型，CodeBERT基于Transformer，通过混合目标方法训练，预训练任务是替换词语检测（replaced token detection, RTD），即检测生成器中可行的替代样本。这个方法使模型既可以运用双模态数据（bimodal data，即NL-PL对）又可以运用单模态数据（unimodal data，即单纯的NL或PL）。双模态数据为模型训练提供输入的tokens，单模态数据便于生成器学习。CodeBERT在通过自然语言搜索代码任务和代码文档化任务中都取得了好结果。 光看摘要真的不知道是在干啥，接着往下看好了。 1 介绍ELMo/GPT/BERT/XLNet/RoBERTa等一众预训练模型大幅提升了NLP任务完成水平。这些预训练模型通过MLM等自监督学习手段学习。预训练模型在NLP领域的成功也催生了别的领域内的预训练模型。 本论文介绍了CodeBERT的工作。CodeBERT是一个针对PL与NL的双模态预训练模型，CodeBERT能够抓住NL和PL的语义学联系，并生成可以用来完成各种关于NL-PL理解任务的向量表示。CodeBERT由多层Transformer构成。为了充分利用双模态和单模态数据，使用一个混合目标函数来训练。混合目标包含MLM和RTD（即首先使用一个生成器预测句中被mask掉的token，接下来使用预测的token替代句中的[MASK]标记，然后使用一个判别器区分句中的每个token是原始的还是替换后的）。单模态数据可以为替换词语检测训练更好的生成器，训练得越好就可以生成更好的替代词汇。代码包含GitHub代码仓库中的六种语言，不显式标注语言类型。 这一段讲的就稍微能理解一些了。 2 背景首先是又介绍了一遍预训练模型（感觉已经介绍了三遍了）。 然后是讲了一下自己的模型与之前模型的不同。首先是既用了双模态数据又用了单模态数据，第二是与一个同时期的工作（Kanade et al., 2019)相对比有三点不同：①不止用了单纯的代码，还用了NL-PL结合的代码；②不止用了Python代码，一共用了六种不同语言的代码。③CodeBERT创新了训练方式，采用替换词语检测任务训练。 3 CodeBERT本段正式介绍模型。 3.1 模型架构采用与RoBERTa相同的模型架构，即多层双向Transformer。 3.2 输入输出表示在预训练阶段，将句子表示如下：，就是那个用来输出分类信息的token。按照Transformer中的常规操作，把词汇拆成wordpiece。直接把代码段当作token序列。 结合上次看的code2seq这个地方应该可以优化代码段的输入，用抽象语法树来表示代码段。 输出包括包含NL和PL在内每个token的向量表示。的向量表示作为整个序列的总向量表示。 3.3 预训练数据还就那个双模态数据（NL-PL对）和单模态数据（单独的NL和PL），在论文里提过无数次了。 论文中此处提到了数据的来源及选取标准。 3.4 预训练CodeBERT训练CodeBERT用到的两个任务：MLM和RTD。其中MLM用双模态数据，RTD用单模态数据。 MLM输入：，其中是NL序列，是PL序列。随机选定中的一些位置用遮盖，遮盖率定为15%。 目标：预测覆盖住的token是什么。 RTD输入：，其中是NL序列，是PL序列。这里有针对NL和PL的两个生成器，生成中可能的替代项。 目标：确定句子中的词语是不是原本的那一个。 备注：生成器的训练方式在这篇论文中并没有被特别强调。论文中就是用的简单的n-gram模型。用n-gram也能练双模态的生成器。当然也可以不用n-gram用Transformer，论文中并没有涉及这部分工作了。 上面这两个任务的损失函数都是交叉熵损失函数。 3.5 CodeBERT微调举例以自然语言代码搜索为例，那就是先把自然语言过CodeBERT，然后把生成的向量过encoder-decoder架构。详情见下面实验部分。 4 实验4.1 自然语言代码搜索输入为一段自然语言，输出为一堆代码段中最符合语义的那段代码段。这应该是一个选择任务而非生成任务。 4.2 探测任务本小节实验主要为了从一个小细节上去探究CodeBERT到底学了什么。因为预训练模型在fine-tuning上表现出色有可能是fine-tuning得好，并不一定就是这个预训练模型训得好，所以要设计任务看预训练模型本身是否学到了东西。 probing的目的是为了弄清楚在预训练模型的每一层中模型究竟学了什么。一般做法是设计一些简单的分类任务（一种任务包含一种特定的语言学信息），将 BERT 每一层的输出作为 embedding，在得到 embedding 后通过简单的分类器（例如一层的 FFN，为了不过多干扰模型本身的能力），来验证模型的不同层在不同 probing task 上的能力。 在本实验中，设定的NL-PL probing的目标是在一些干扰选项之间选择正确的masked token。一般而言，有两种干扰方式：其一是整个词汇表都用来生成干扰选项，第二种是根据专家的专业知识选取特定的一些干扰选项。此处选择第二种干扰方式。整个NL-PL probing其实就像是让电脑做完形填空。如果要看在NL上的表现，那就给完整的代码和遮盖过的文档（此处遮盖的词统一为最大（max/maximize）、最小（min/minimize）、小于（less）、大于（greater)），然后让电脑填空，看是否能填对。如果要看在PL上的表现也是一样，不过文中PL给了两种验证方式，一种是两边的文本都有，另一种是只有左边的文本（preceding PL codes）。 4.3 代码文档生成代码文档生成任务的挑战在于训练CodeBERT时并没有做生成方面的任务，但最后结果居然是SOTA。 4.4 生成不在预训练范围内的语言（讲真这个听起来就很nb） 这个的结果是比RoBERTa好但比code2seq差，也算是个好结果了。 5 结论本文提出了第一个同时针对NL和PL的大型双模态预训练模型CodeBert，在双模态和单模态数据上训练CodeBert，并在两个NL-PL下游任务中进行fine-tuning取得了SOTA的结果，接下来的工作目标有：①使用更好的神经网络架构；②在预训练时增加生成任务；③将AST等结构融入特征提取的过程中；④尝试更多的下游任务。","categories":[],"tags":[]},{"title":"从word2vec到GPT","slug":"从word2vec到GPT","date":"2023-06-04T11:45:22.085Z","updated":"2024-03-02T13:00:21.652Z","comments":true,"path":"从word2vec到GPT.html","link":"","permalink":"https://asparticguan.github.io/blog/%E4%BB%8Eword2vec%E5%88%B0GPT.html","excerpt":"","text":"从word2vec到GPT前言写这篇文章的目的是为了总结一下在大二上学期初接触NLP至今走的不少弯路，也因为一直学得有点云里雾里，所以尝试着能不能把这一条线整个串起来试一试。 NLP这个研究领域的历史在计算机领域算是比较古早的了，1954年就有尝试把俄文翻成英文。原始的基于规则的NLP一直持续到了80年代末，随后被基于统计的NLP所取代，例如早期的决策树和后来引入的隐马尔可夫模型。21世纪以降，随着深度学习的引入，NLP领域迎来了新的春天。 斯坦福大学的CS224n NLP课程选择将word2vec作为首先讲解的内容，这是很有道理的。因为没有词嵌入模型，就无法将字符串转化为向量，自然也就没有办法再进行后面的工作。所以本文也将从word2vec开始。 注：本文中的人名均优先用科研人员出身国通用语言标出以获得直观印象，非拉丁字母人名将备注英语译名，东亚人名还会备注汉语译名。 word2vecword2vec是2013年由捷克计算机科学家Tomáš Mikolov带领的团队提出的一种词嵌入技术。词嵌入技术历史也比较久了，2000年时加拿大计算机科学家Yoshua Bengio就提出用神经概率语言模型降维词语空间的方法。word2vec分成两种，一种是CBOW，一种是Skip-gram。CBOW是给出上下文来预测中心词，Skip-gram是给出中心词来预测上下文。这两个模型有点像镜像对称的关系，所以此处我们就挑CBOW来讲。 首先给一句话：The cat jumped over the puddle. 那么CBOW的目标就是，训练好embedding用的中心词矩阵和上下文词矩阵，让输入为”The cat _ over the puddle.”时输出“jumped”（也可以是遮别的词又输出别的词）。整个模型的架构如下： 输入为”The cat over the puddle.”中每个单词的独热向量，每个独热向量与上下文词矩阵相乘都能得到一个embedding向量，这些个embedding向量一平均得到隐藏层的向量，隐藏层向量再和一个中心词矩阵相乘就得到预测的结果。预测的结果和真实的独热向量可以算交叉熵损失，算了交叉熵损失就能BP，能BP就能更新参数，然后模型就这么给训好了。 有了word2vec我们就可以把词语做embedding啦，这真是太好啦，下面让我们来看看RNN吧。 RNNRNN其实也不是什么新东西了，RNN的前身是1982年由美国学者John Hopfield提出的Hopfield神经网络，后来1986年美国科学家Michael I. Jordan提出了Jordan网络。1990年美国科学家Jeffrey Elman在Jordan网络的基础上进行创新，提出了Elman网络，是为现在所说的循环神经网络（Recurrent Neural Network），即RNN。虽然RNN不是什么新东西，但把RNN用在NLP中确实已经比较晚了。2010年捷克计算机科学家Tomáš Mikolov提出基于RNN的语言模型，从此开始了RNN在NLP中的应用。 RNN的模型其实说来也简单，它主要的工作就是应付序列输入。如下图所示，与矩阵相乘，与矩阵相乘，二者相加后再过一个或函数得到。其后依次类推，可得： h^{(t)} = f(\\pmb{W_h}h^{(t-1)}+\\pmb{W_e}e^{(t)}) 从上面这个公式中即可看出循环的特点。 RNN中需要训练的就是和两个矩阵，同样还是用BP来训。一般就初始化为。 RNN出来后也产生了很多变体。例如多层RNN，如下图所示： 这个我觉得是一目了然。 又例如双向RNN，既要正着背完，又要倒背如流： 从正反两个生成的过程可以上各种奇技淫巧。 把这两个一结合就又有双向多层RNN！ 当然学过的都知道，RNN是个基础模型，在RNN的基础上还发展出了LSTM和GRU两大模型。 LSTMLSTM（Long Short-Term Memory）也不是什么新玩意了。1995年，德国科学家Sepp Hochreiter与Jürgen Schmidhuber首次提出LSTM，并在1997年发表主要论文。说实话，LSTM的网络架构真的挺复杂的，设计这么复杂的网络架构主要是为了解决梯度消失问题（“学了后面，忘了前面”）。下图是LSTM的结构图： 细节如下： LSTM的网络结构中使用了门激活函数。LSTM中有三个门，分别叫做遗忘门、输入门、输出门（按照谷歌研究的重要性顺序排序）。 遗忘门：控制从传送带上传过来的数据中哪些是要遗忘的哪些是要保留的。训练矩阵即可（忽略，下同，且下面两个门也是训练对应矩阵）。 输入门：控制要向传送带上写下什么。原本就直接把和传进去就行了，现在不行，要审查。 输出门：控制要输出些什么，这些输出会被用到下一个LSTM的计算中。 不得不说，这个结构还是巧思很多的，的确是个很伟大的创造。 GRUGRU（Gated Recurrent Unit）终于是个新东西了。2014年，韩国研究员조경현（Kyunghyun Cho，赵景贤（音））提出了GRU，是为另一种使用了门激活函数的网络结构。以下是GRU的结构图（Google好像没做上面那种图的GRU版，网上现有的图总感觉有问题，拿CS224n讲义里的图代替了）： GRU中也有两个门，分别为重置门和更新门。重置门可以判断对这个GRU块的初步输出是否有用。如果对新记忆的计算没用就完全削除。更新门可以判断在中的占比，趋于则基本相当于把赋给，趋于0则表示基本是把赋给。 LSTM和GRU各有所长，一般来说，默认选用LSTM，但由于GRU参数较少，所以如果想算得更快可以用GRU。 说实话，写到这里其实感觉讲的东西还是有点古典，和2022年的潮流有点不符。但NLP当下的潮流——预训练模型，正是基于这些神经网络发展起来的。下面开始看预训练模型的发展历程。 ELMo2018年3月，Allen Institute for Artificial Intelligence和University of Washington的研究人员提出了ELMo预训练模型。ELMo能做什么？如果将其视作一个黑盒的，对ELMo的第一印象就会是：输入了通过word2vec或GloVe得到的词向量，结果输出的还是词向量。这是在干什么？其实，ELMo主要解决的问题是对于同一个词可能不同语境下表达的含义不同的问题。比如bank，有可能说的是银行，但也有可能是在说河岸。相同的词在不同语境下会有不同含义，甚至即使字典上的含义相同，词在句子中的意思都会有区别。ELMo的工作，就是做到了“一词多向量“，真正做到了精准。 ELMo的网络结构如下图所示： 其中到是词序列（句子）中的词语初步embedding的结果，左边是一个双层的正向LSTM，右边是双层的逆向LSTM，把两个LSTM的输出拼接在一起就得到了ELMo的输出到。 既然用到了LSTM，那就一定要有训练LSTM的方法。训练这两个LSTM用的就是预测下一个单词的任务。预测下一个单词的任务就是把LSTM的输出和一个上下文矩阵相乘得到一个向量，对这个向量softmax得到针对每个单词的概率。整个任务做的事情就是最大化 其中是embedding时用到的网络参数，是LSTM里面的网络参数，是上下文矩阵的参数。 ELMo训好了，现在还要能使用之。考虑到ELMo中每层LSTM学到的东西都不一样且都可能有用，我们先把正逆LSTM中对应相同token的向量位置的两个纵列的每个LSTM单元的输出都拿出来，然后和上面一样相同层的拼在一起，再赋给每一层拼好的向量一个权重，最后加权平均求和就得到这个词在这个任务中应该表示成的向量。ELMo在运用到NLP具体任务中时仍要训练，训练的就是这些权重。 ELMo是第一个做到训好一个模型，就可以用到多种NLP任务上，并成功SOTA的预训练模型。从这个意义上来讲ELMo是革命性的。从ELMo开始，科研人员就把注意力集中到了预训练模型上来，越来越多的预训练模型被发明了出来，这些模型也就是接下来要讲的内容。 Seq2Seq(45条消息) 【神经网络】学习笔记十四——Seq2Seq模型_昔我往矣wood的博客-CSDN博客_seq2seq 后来突然想起来忘记写了，有时间再补了。 Transformer2017年，谷歌机器翻译团队发表论文《Attention Is All You Need》，完全抛弃CNN、RNN等神经网络架构，引入自注意力机制创建了Transformer模型，成功在机器翻译任务中取得SOTA成果。从此，Transformer逐步取代LSTM等RNN模型成为了NLP问题的首选模型。 想要知道什么是Transformer，就要知道什么是Self-attention；而想要知道什么是attention，就得知道attention为何。下面从attention讲起。 attentionattention的前身其实很早就有了，上世纪90年代就在视觉图像领域有一些应用。当然，正式将attention引入NLP领域还是在2014年，由白俄罗斯科学家Дзмітрый Багданаў（Dzmitry Bahdanau，国籍和原名均不完全确定）在论文《Neural Machine Translation by Jointly Learning to Align and Translate》正式提出。 attention机制的思想是，让decoder的输出再去回顾encoder的隐层输出，让decoder和每个encoder经过一个attention函数（有挺多不同的函数可以用）算出一个attention分数，把attention分数向量过了softmax后和decoder的输出向量拼接在一起再来进行decoder的输出。模型图如下： attention的提出使得在encoder-decoder架构中，decoder可以直接查看源语句而非通过encoder的输出雾里看花，并且也帮助解决了部分梯度消失问题。事实上，这一个简单的结构不仅在NLP中有很大用处，在几乎所有领域我们都能找到attention的应用。 self-attentionattention说到底，其实也只是在现有的CNN、RNN架构上锦上添花，离了这些模型，attention就无用武之地了。但是self-attention并非如此。《Attention Is All You Need》中提出的自注意力机制抛开了这些模型，做到了自成体系。 在attention被提出后，很多模型都是基于RNN及其变体加上注意力机制运作的。然而，RNN由于其顺序结构训练速度常常受到限制，并行计算能力不强。自注意力机制解决了这一问题，利用attention本身能够瞻前顾后的特性进行学习，取得了很好的效果。 下图是Self-attention的架构图： 如图所示，到是句子序列的输入，首先经过embedding得到到。再将分别乘三个矩阵得到三个向量： 以产生为例，此时我们拿和每个做attention再softmax得到到，这样得到的就是词和词间的关联信息。把与相乘再求和就得到了。这就是所谓”自注意力机制就是加权平均“的意思。 那么为什么又说self-attention解决了并行计算的问题？还请看下面的推导： 记句子序列为矩阵（里面的单词都是列向量），那么嵌入后的矩阵为： \\pmb{WX}=\\pmb{I}做self-attention用到的三个矩阵分别为： \\pmb{W^qI}=\\pmb{Q}\\\\ \\pmb{W^kI}=\\pmb{K}\\\\ \\pmb{W^vI}=\\pmb{V}\\\\做attention再softmax（假设attention函数就是点积）： softmax(\\pmb{K}^\\mathrm{T}\\pmb{Q})={\\widehat{\\pmb{A}}}最后一步好像不是标准的矩阵乘法，不过也没差： b^j =\\sum_{i=1}^{n} {\\widehat{\\alpha}_{j,i}v^i}self-attention还有个变体，叫做Multi-head self attention（多头自注意力机制）。如下图所示： 第一个头算，第二个头算,得到两个头后再过一个矩阵即可得。 self-attention还有点不足。因为随便在哪个位置都能无差别地学到所有信息，单词在句中的位置这一重要信息就可能会丢失从而影响效果，这时我们可以通过给附加一个特殊的位置向量就是来解决。位置向量直接指定，不经过学习。论文中给出的计算公式如下： 现在已经讲完Transformer中最重要的部件——self-attention了，下面就来看Transformer的本体。 Transformer那既然讲到Transformer了自然要把那张经典图贴上不是： 即使已经知道self-attention是什么了，看这个图的第一感觉大概率还是”什么玩意“，所以这东西得再拆开来讲。 首先是Transformer的训练任务。应该是因为是机器翻译小组发明的模型，所以任务是机器翻译，即输入A语言序列，输出B语言序列。 Transformer其实也是一种encoder-decoder模型。拆开来看长这样： 以机器翻译任务为例，Transformer输出结果的过程：根据输入生成一个初步的embedding，把这个初步的embedding输入进encoder中得到每个词正式的embedding，在decoder中，首先输入起始编码“\\”，由起始编码和embedding一起输出“I”，然后“\\ I”一起输入来输出“have”，依此类推，直至输出“\\”。 encoder中的Add&amp;Norm中Add是指把输入和输出加起来，如下所示： 而Norm指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。 重点来看下decoder。decoder的第一层叫做masked multi-head attention。为什么要有这个masked的操作。这是为了解决训练阶段和测试阶段不匹配的问题。在训练阶段，解码器会有输入，这个输入是目标语句，就是 I have a cat，通过已经生成的词，去让解码器更好的生成（每一次都会把所有信息告诉解码器）。而在测试阶段，解码器也会有输入，但是此时，测试的时候是不知道目标语句是什么的，这个时候，你每生成一个词，就会有多一个词放入目标语句中，每次生成的时候，都是已经生成的词（测试阶段只会把已经生成的词告诉解码器）。 为了匹配，masked multi-head attention就登场了，我在训练阶段，我就做一个 masked，当你生成第一个词，我啥也不告诉你，当你生成第二个词，我告诉第一个词，依此类推。 那么怎么进行这个masked multi-head attention？其实很简单，就是在算出后再按位乘一个矩阵后再softmax即可。就是把要mask起来的部分设为-1e-9，这样softmax出来的结果就会是0。如下图所示： decoder的第二层是一个正常的multi-head attention，但与来自encoder的输出，来自上一层的输入。仔细想想是有道理的（反正深度学习网络可解释性就这样，怎么说都有道理（雾））。 Transformer就讲这么多了。Transformer的出现也深远地改变了深度学习的格局，在Transformer后出现的很多模型身上都有它的影子。BERT就是其中之一。 BERT2019年，谷歌团队发表论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》，正式提出BERT（Bidirectional Encoder Representations from Transformers，源自Transformers的双向编码器表示）。 BERT其实就是Transformer中encoder的堆叠。把12个Transformer的encoder叠在一起就得到了论文中基础版的BERT。 有了BERT的结构后，现在就要训练BERT。由于带标注的数据比较难找，所以设计了两个预训练任务来训练BERT。分别是MLM（Masked Language Model）和NSP（Next Sentence Prediction）。这两个任务都不需要打标，自己就是标。 MLM为了训练深度双向表征，我们只需随机屏蔽一定比例的输入符号，然后预测这些被屏蔽的符号。我们把这个过程称为 MLM，在文献中也常被称为Cloze任务。在MLM中，输出的隐藏向量经过softmax后和真实值比对并通过交叉熵计算损失，就像在标准LM中一样。在我们所有的实验中，我们随机屏蔽了每个序列中所有Word Piece标记的15%。 尽管这使我们能够得到一个双向的预训练模型，但缺点是我们在预训练和fine tune之间产生了不匹配，因为[MASK]标记在fine tune期间没有出现。为了缓解这一问题，我们并不总是用实际的[MASK]标记来替换 “被屏蔽”的词。训练数据生成器随机选择15%的令牌位置进行预测。如果选择了第个标记，我们就①80%的概率用[MASK]标记；②10%的概率用另一个词替换；③10%的概率不作处理。 NSP在 BERT 训练过程中，模型接收成对的句子作为输入，并学习预测该对中的第二个句子是否是原始文档中的后续句子。在训练期间，50% 的输入是一对句子，其中第二个句子是原始文档中的后续句子，而在另外 50% 的输入中，从语料库中随机选择一个句子作为第二个句子，并假设该随机句子与第一句不相连。 为了帮助模型在训练中区分两个句子，输入在进入模型之前按如下方式处理： 在第一句的开头插入一个 [CLS] 标记，在每个句子的结尾插入一个 [SEP] 标记。 将指示句子 A 或句子 B 的sentence Embedding添加到每个标记中。sentence Embedding在概念上类似于当词汇表长度为 2 的token embedding。sentence Embedding只有两种，要么是指示A句子的，要么是指示B句子的。 positional embedding 被添加到每个标记以指示其在序列中的位置。Transformer 论文中介绍了positional embedding 的概念和实现。 为了预测第二个句子是否确实与第一个句子相关，执行以下步骤： 整个输入序列通过 Transformer 模型。 使用简单的分类层（权重和偏差的学习矩阵）将 [CLS] 标记的输出转换为 2×1 形状的向量。 用 softmax 计算 IsNextSequence 的概率。 在训练 BERT 模型时，Masked LM 和 Next Sentence Prediction 一起训练，目标是最小化这两种策略的组合损失函数。 fine tune一般情况下，我们训练一个模型是从头开始训练，花费时间较长，BERT微调就是在预训练模型BERT的基础上只需更新后面几层的参数，这相对于从头开始训练可以节省大量时间，甚至可以提高性能，通常情况下在模型的训练过程中，我们也会更新BERT的参数，这样模型的性能会更好。 将 BERT 用于特定任务相对简单： BERT 可用于多种语言任务，仅需在核心模型中添加一小层： 情感分析等分类任务与 Next Sentence 分类类似，方法是在 [CLS] 令牌的 Transformer 输出之上添加一个分类层。 在问答任务（例如 SQuAD v1.1）中，当需要接收与文本序列的问题，并且需要在序列中标记答案。使用 BERT，可以通过学习两个标记答案开始和结束的额外向量来训练问答模型。 在命名实体识别 (NER) 中，接收文本序列并需要标记文本中出现的各种类型的实体（人、组织、日期等）。使用 BERT，可以通过将每个标记的输出向量输入到预测 NER 标签的分类层来训练 NER 模型。 这部分基本机翻的原论文或摘抄的知乎帖子](https://zhuanlan.zhihu.com/p/509244050))，因为觉得讲得都挺好的。 BERT开启了NLP领域新的纪元，2020年的一项文献调查得出结论：“在一年多一点的时间里，BERT已经成为NLP实验中无处不在的baseline”，算上分析和改进模型的研究出版物超过150篇。 GPTGPT可以算是NLP中目前最出圈的模型，且看网友把ChatGPT玩出花来，而我却因为不想花钱买国外手机号至今都没有体验过（悲）。不止ChatGPT，Copilot也是基于GPT-3的模型，可见GPT的强大。 GPT到现在一共有三代。初代GPT由OpenAI团队在2018年6月提出，结果在提出后仅四个月就被BERT干下去了。OpenAI痛定思痛，又过了4个月，提出了GPT-2，在零次学习（zero-shot learning，即不fine tune）上有优异表现，但效果也不太出众。终于，在2020年5月，暴力出奇迹，在把数据和模型都增大100倍后，GPT-3诞生了，终于取得了惊艳的成果。 （说到零次学习，预训练模型确实有这个问题，因为确实无法确定究竟是预训练训得好还是微调调得好。况且，存在微调也就代表还是要有微调用的数据集，这就带来了模型的使用门槛。） 但是，在学术界，把GPT到GPT-3这几篇文章引用加起来都没BERT高。主要应该是因为GPT太大了，别的团队复现不出来……大是什么概念？这么看，ELMo有9400万个参数，BERT有3.4亿个参数，GPT-2有15.4亿参数，GPT-3呢？1750亿。😰 GPTGPT主要基于Transformer的decoder，并删除了encoder-decoder attention层。 GPT-2在GPT时我们在构建下游任务输入时引入了起始、截断和终止符，这些模型在开始的预训练阶段时没有看到的，但是有微调的情况时，模型可以再学习到这些符号的意思。但是GPT-2要做zero-shot时，在做下游任务时模型不能被调整了，再引入这些特殊字符时模型会感到很困惑，所以在构建下游任务输入时不能引入那些模型没见过的符号，而需要使下游任务的输入和之前预训练时模型看到的文本长得一样，输入形式应该更像一个自然语言。 例如：在做句子翻译任务时，训练的句子可以被写为： (translate to french, english text, french text)。其中translate to french在后文叫做prompt（提示），相当于做了一个特殊的提示词。 如果要做阅读理解任务时：可以写作(answer the question, document, question, answer)，answer the question相当于任务提示。 这些构建提示词的方式是前人提出的，假设为如果训练的模型足够强大就可以理解这些提示词的意思，而且这种提示词在文本中也比较常见，模型可以理解。 GPT-2训练数据是什么样的？ 没有选择Common Crawl这种具有很多冗余无用信息的项目，选用的是reddit里面已经被人工筛选出的有意义的，并且具有至少3karma值的网页进行数据处理，大概有800万个文本，40GB的文字。 在这种大数据集的基础上进行预训练可能真的可以做英语翻译成法语的任务。因为数据集中这种例子确实多。 GPT-3之前的GPT是讲的在子任务训练时提供少量的训练数据，GPT-2是子任务时不提供任何相关的训练样本，直接使用预训练模型在子任务上面做预测， 一篇论文的价值取决于它的新意度 x 有效性 x 问题的大小 GPT-2的新颖度很高，但是有效性很低，所以GPT-3又回到了GPT要解决的few-shot问题 只要样本数目在可控范围内，那么这个成本还是非常低的。 GPT-3是一个自回归模型，有1750亿个可学习参数（比现在非稀疏的模型大十倍——稀疏模型里面权重0的含量很多），因为模型很大了，所以再在子任务上面做训练那么成本是很难的，所以GPT3在作用在子任务上面不做任何的梯度更新或者是微调，即使是在few-shot情况下GPT3也不是做微调，因为大模型如果算梯度是很难的事情，所以不做梯度更新。在nlp任务上相对于GPT2取得了很好的成绩。 在做NLP任务使用大模型预训练时，需要在做下游子任务时使用子任务的数据集，并且进行微调，但是有三个问题： 1、需要大量有标号的数据集。 2、样本没有出现在数据分布里面，大模型的泛化性不见得比小模型更好。微调效果好不能说明预训练模型泛化性好，因为可能是过拟合预训练的训练数据，这些训练数据与微调使用的数据刚好有一定的重合性。 3、人类不需要一个很大的数据集做任务， 解决以上问题的方法： meta-learning（自己定义的，真的训练了一个很大的模型，泛化性还不错）+ in-context learning（在子任务时即使给了一些训练样本，但是不更新权重） 在做不同任务时，因为模型输入的数据时多元化的，每个段落或者每个文章来源不同可能会教模型不同的东西，那么如果是在大量多样性的文章中做训练时，模型有在做元学习的过程，学习了大量的任务。 模型评估方法： 1、few-shot learning 每个子任务提供大概10~100个训练样本， 2、one-shot learning few-shot learning的特殊情况，每个任务只提供一个训练样本 3、zero-shot learning 一个样本都不告诉，直接执行任务 以下是三个设定下模型的区别，对于不同子任务做了个平均： GPT-3模型部分： 一般下游任务微调过程中，训练好预训练模型后，在每一个任务上面提供一些训练样本（有标号的样本），根据标号计算损失，然后对其权重进行更新，这里与微调不一样的是不需要很大量的数据，而且可以用较小的学习率，因为微调的初始值是之前预训练好的模型，所以此时的初始值与最终的解已经很相近了，所以只需要大概调一下就好。但是GPT-3追求的是不做梯度更新，因为这么大的模型如果换了一个任务就要重新训练的话成本很高，GPT-3做的是，当下游任务进行训练时，输入的不管是few-shot、one-shot还是zero-shot其实做的是预测任务而不是训练任务，给的样本例子是希望模型通过transformer架构可以阅读全文的语义信息帮助其进行预测，但是梯度不进行更新，few-shot样本量也不宜过多，因为模型可能看不了很长的文本信息，其中的Translate English to French是指示信息，告诉模型要进行翻译任务，这个指示信息在预训练时已经学习到了，后面的=&gt;是指示符，示意模型可以进行翻译了。 以上源自：超大型人工智能：从GPT-&gt;GPT2-&gt;GPT3的发展历程+大规模预训练神经网络模型原理详解 - 知乎 (zhihu.com) 这个确实比较逆天我觉得，尤其是上图左边和右边训练方式一对比就更觉得这玩意过于神奇。 在GPT-3问世后直至今日人们仍在研究其新玩法，在GPT-3的基础上还有越来越多的新模型在不断涌现。尤其ChatGPT于今年11月30日推出，更是让人们看到了NLP应用的美好未来。希望人类探究清楚自然语言的奥秘那天能够早日到来！ 后记这篇文章写了有三天吧，把整个NLP的技术大体上给串了一遍，收获还是不小的。当然仍然存在不少缺漏，比如GloVe，fastText以及CNN在NLP中的应用等，有时间时应当还是要把这些东西补上以冀完善文章。另外，写BERT和GPT时大段Ctrl CV了知乎帖子，但都是经过认真选择的，觉得他们讲得更好，于是摘了过来。这样说实话不利于自己的理解，以后还是应当认真学习。 ​ ​ 辛丑年圣诞日记于岳麓山","categories":[],"tags":[]},{"title":"Python札记","slug":"Python札记","date":"2023-06-04T11:44:35.375Z","updated":"2024-03-07T07:05:41.185Z","comments":true,"path":"Python札记.html","link":"","permalink":"https://asparticguan.github.io/blog/Python%E6%9C%AD%E8%AE%B0.html","excerpt":"","text":"尝试写一点关于python的笔记0.Hello World!​ print(&quot;Hello World!&quot;) ​ 这个确实应该是所有语言里面最简单的了。 1.Strings输出比如现在有一个string类型，以下两种情况皆是合法的： message=&quot;Hello World!&quot; message=&#39;Hello World!&#39; Python 在输出既定的带换行的字符串时也很有优势，如下： message = &quot;&quot;&quot;Hello World&quot;&quot;&quot; 注意要用三个&quot;&quot;&quot;，且不要想着把 Hello和World对齐，否则输出中间一堆空格。 单字符串处理计算字符串长度：len(message) 访问某个字符：message[i] Slicing Strings：message[i:j] 这样子其实访问的子串的编号区间为 [i,j)这点务必记清。 另外，下面两种写法 message[:i] message[j:] 等价于 message[0:i] message[j:len(message)] 将所有字符变为小写：message.lower() 将所有字符变为大写：message.upper() 计算字符串中某子串出现的次数：message.count(&#39;xxx&#39;) 计算字符串中某子串首次出现的位置：message.find(&#39;xxx&#39;) 涉及多字符串的处理替换字符串的某部分：（例） message = &#39;Hello World&#39; message = message.replace(&quot;World&quot;, &quot;Universe&quot;) print(message) 结果为 Hello Universe 注意：直接如下操作无法达到效果： message = &#39;Hello World&#39; message.replace(&quot;World&quot;, &quot;Universe&quot;) print(message) 原因自然与函数的返回有关 拼接字符串： 法一： string1 = &quot;Wuhan&quot; string2 = &quot;University&quot; jiubawu = string1 + &quot; &quot; + string2 这种适合两三个的拼接，长了就要看下面了。 法二： string1 = &quot;Huazhong&quot; string2 = &quot;Science and Technology&quot; daizhuan = &#39;&#123;&#125; University of &#123;&#125;&#39;.format(string1,string2) 法三（fstring）： string1 = &quot;Huazhong&quot; string2 = &quot;Science and Technology&quot; daizhuan = f&#39;&#123;string1&#125; University of &#123;string2&#125;&#39; 功能查询如果想要查询对一个变量可以进行的操作，用下面这条语句： print(dir(xxx)) xxx可以是数据类型（如str）或变量名（如string1）； 查询如何操作，用下面这条语句： print(help(xxx)) xxx只能是数据类型。 2.Integers and Floats（很多东西没提，代表和C语言无异） 定义与类型定义一个变量的方式：num = 1 Python是一种很方便的语言，故不像C语言分出了 long double等一堆七七八八实际上在人看来差不多的东西。Python里只有两种：int与float。 如果想要知道一个变量是什么类型，用 type(num) 基本计算Python是一种很方便的语言，故它的很多东西都是向人类习惯靠齐的。它不会像C语言一样认为这个的结果是1： int main() &#123; int a = 3; int b = 2; cout &lt;&lt; a/b &lt;&lt; endl; &#125; 但如果想在Python保留带类型的结果，用 a//b，这样子算出来也是1，但如果只有一个 /那就是1.5。 另外， a^b为 a**b 算绝对值用 abs() 求和已知数最接近的整数用 round(num) 如果是最接近的n位小数，用 round(num,n)（注意：当num为小数 &amp;&amp; n == 0时输出为类似 1.0的形式） 另外，Python中没有自增运算符 i++。 强转类型举例 num1 = &#39;100&#39; num2 = &#39;200&#39; print(num1 + num2) num1 = int(num1) num2 = int(num2) print(num1 + num2) 输出为： 100200 300 注意强转类型打括号的地方和C正好是反的。 3.Lists, Tuples, and Setslist（列表）类型定义与访问元素定义：universities = [&#39;huake&#39;,&#39;wuda&#39;,&#39;qinghua&#39;,&#39;beida&#39;] 计算数组长度：len(list) 访问全部元素：print(universities)，输出为 [&#39;huake&#39;, &#39;wuda&#39;, &#39;qinghua&#39;, &#39;beida&#39;] 访问某一个：list下标也是从0开始的。如上例，直接print(universities[0])即可输出 huake 而 universities[-2]指倒数第二个元素，即 qinghua（其实string也可以这么用）。 得到中间的某串元素，如第0至2个，用 universities[0:3]，和string一样，注意左闭右开。 另外，下面两种写法 message[:i] message[j:] 也可直接套到list上面来。 增加元素如上例，增加元素至list末尾用 universities.append(&#39;xxx&#39;) 插入到第n个位置用 universities.insert(n,&#39;xxx&#39;) 将一个list接在另一个list结尾 universities1 = [&#39;huake&#39;,&#39;wuda&#39;,&#39;qinghua&#39;,&#39;beida&#39;] universities2 = [&#39;fudan&#39;,&#39;shangjiao&#39;] universities1.extend(universities2) 如果直接用 append，输出是 [&#39;huake&#39;, &#39;wuda&#39;, &#39;qinghua&#39;, &#39;beida&#39;, [&#39;fudan&#39;, &#39;shangjiao&#39;]] 集合当元素了，这离散正讲着集合论就有活生生的例子出来了。 YouTube中并没有讲如何将一个list插入另一个list。是用切片（slice）做的： universities1 = [&#39;huake&#39;,&#39;wuda&#39;,&#39;qinghua&#39;,&#39;beida&#39;] universities2 = [&#39;fudan&#39;,&#39;shangjiao&#39;] universities1[1:1] = universities2 删除元素universities.remove(&#39;wuda&#39;)或universities.remove(universities[1]) 删除最后一个：universities.pop()，显然服务于栈和队列。 为了更好地服务，universities.pop()返回被删除的元素。 其他操作反转list：universities.reverse() 排序：universities.sort() 反向排序：universities.sort(reverse=True)（True首字母大写） 获得排序后的list并保留原list：sort_universities = sorted(universities) list最小值：min(universities)最大值：max(universities) 求和（元素类型需是int）：sum(nums) 查询某个元素在list中的位置：universities.index(&#39;huake&#39;) 查询list中是否有某个元素&#39;xxx&#39; in universities，返回 True或 False 循环输出每一个元素（for的首次出现） universities = [&#39;huake&#39;,&#39;wuda&#39;,&#39;qinghua&#39;,&#39;beida&#39;] for university in universities: #注意冒号 print(university) #注意缩进 事实上university可被替换成随便什么别的，如 i j k或别的什么（不过C语言中也可以，谁说for一定要用i j k） 循环输出，还加编号： universities = [&#39;huake&#39;,&#39;wuda&#39;,&#39;qinghua&#39;,&#39;beida&#39;] for index,university in enumerate(universities): print(index,university) index也可以改成别的名字，加编号靠的是enumerate()，enumerate的中文意思是”列举“。 这样子的结果为： 0 huake 1 wuda 2 qinghua 3 beida 如果不想从0开始，可以这样改： for index,university in enumerate(universities,start = 1): 然后就是： 1 huake 2 wuda 3 qinghua 4 beida list变string：unistr = &quot;, &quot;.join(universities) &quot;, &quot;可以改成别的任何字符串，这样就会在两个元素中间加那个字符串 string变list：现在你有了一个字符串，可能它长这样：str = &quot;A-B-C-D&quot;。现在我们要把ABCD存到 list里去，用：list = unistr.split(&quot;-&quot;) 关于list修改值的讨论考虑如下代码： univ1 = [&#39;huake&#39;,&#39;wuda&#39;,&#39;qinghua&#39;,&#39;beida&#39;] univ2 = univ1 univ1[1]=&#39;huakefushu&#39; print(univ1,univ2) 它的输出为：[&#39;huake&#39;, &#39;huakefushu&#39;, &#39;qinghua&#39;, &#39;beida&#39;] [&#39;huake&#39;, &#39;huakefushu&#39;, &#39;qinghua&#39;, &#39;beida&#39;] “但是，我只改变了univ1的值啊！” 这是因为list是可变对象，变量名univ1和univ2是绑定的同一内存地址，对任一个变量对应的值得改变，都会反映到另一个变量上。这点务必要注意。 tuple（元组）类型tuple类型与list类型很像，但最大的不同是tuple不可修改。定义一个tuple变量的方式如下： universities = (&#39;huake&#39;,&#39;wuda&#39;,&#39;qinghua&#39;,&#39;beida&#39;) []改 ()即可。 set（集合）类型定义set类型universities = &#123;&#39;huake&#39;,&#39;wuda&#39;,&#39;qinghua&#39;,&#39;beida&#39;&#125; set的特点：1.无序性 2.互异性 对，set就是集合。 set常用于判断元素是否属于集合——list与tuple也能做，但set更快。set也可以用于丢掉没用的元素——毕竟这是集合的性质。 set的另外一个功能是用于求两个集合的交并差集，如下： univ1 = &#123;&#39;huake&#39;,&#39;wuda&#39;,&#39;qinghua&#39;,&#39;beida&#39;&#125; univ2 = &#123;&#39;huake&#39;,&#39;hagong&#39;,&#39;xijiao&#39;,&#39;dongnan&#39;&#125; print(univ1.intersection(univ2)) #求交集 print(univ1.union(univ2)) #求并集 print(univ1.difference(univ2)) #求差集 输出结果为 &#123;&#39;huake&#39;&#125; &#123;&#39;wuda&#39;, &#39;beida&#39;, &#39;dongnan&#39;, &#39;xijiao&#39;, &#39;huake&#39;, &#39;qinghua&#39;, &#39;hagong&#39;&#125; &#123;&#39;wuda&#39;, &#39;beida&#39;, &#39;qinghua&#39;&#125; 创建空list，tuple和set如下： emptylist1 = [] emptylist2 = list() emptytuple1 = () emptytuple2 = tuple() emptyset1 = &#123;&#125; #这个不行，这样子创建的是dictionary型 emptyset2 = set() #只有这个可以 4.DictionariesPython里的dictionary类型在其他语言中可能会被称为Hash Map，采用键值存储（key–value database），可用于快速查找（哈希表的时间复杂度为 O(1)) 定义与访问定义dictionary类型：student = &#123;&#39;name&#39;:&#39;Mike&#39;,&#39;age&#39;:25, &#39;courses&#39;:[&#39;math&#39;,&#39;art&#39;]&#125; 查询dictionary中某个键的值：student[&#39;courses&#39;] 但这种访问方式不太安全。当想要查询的键不存在时，会直接报错。一个更稳妥的方法是用：student.get(&#39;courses&#39;)，此时当键不存在时返回 None。 甚至还可以定义当键不存在时应该返回什么：student.get(&#39;courses&#39;,&#39;404&#39;) 这样子，如果代码是：student.get(&#39;course&#39;,&#39;404&#39;)，就会输出 404。 增删改增加和改动是一样的，接上例： student[&#39;phone&#39;] = &#39;13588888888&#39; #增 student[&#39;age&#39;] = 18 #改 还有更简单的方法： student.update(&#123;&#39;age&#39;:18,&#39;phone&#39;:&#39;13588888888&#39;&#125;) update()要求传入一个新dictionary 第一种删除的方法是用 关键字del：del student[&#39;courses&#39;] 第二种方法是用 pop()：student.pop(&#39;courses&#39;)，且和list中讲的一样，pop()可以返回被删除的元素。 其他查找键的个数：len(student) 只访问键：student.keys() 只访问值：student.values() 访问键和值：student.items() 直接用 print(student)不也差不多吗？别急，下面就有应用： 循环输出： student = &#123;&#39;name&#39;:&#39;Mike&#39;,&#39;age&#39;:25, &#39;courses&#39;:[&#39;math&#39;,&#39;art&#39;]&#125; for i in student: print(i) 但这样子的输出只有键： name age courses 但如果这样： student = &#123;&#39;name&#39;:&#39;Mike&#39;,&#39;age&#39;:25, &#39;courses&#39;:[&#39;math&#39;,&#39;art&#39;]&#125; for i in student.items(): print(i) 输出会变成： (&#39;name&#39;, &#39;Mike&#39;) (&#39;age&#39;, 25) (&#39;courses&#39;, [&#39;math&#39;, &#39;art&#39;]) 更进一步： student = &#123;&#39;name&#39;:&#39;Mike&#39;,&#39;age&#39;:25, &#39;courses&#39;:[&#39;math&#39;,&#39;art&#39;]&#125; for i,j in student.items(): print(i,j) 那么输出为： name Mike age 25 courses [&#39;math&#39;, &#39;art&#39;] 5.Conditionals and Booleans - If, Else, and Elif基本语句if [判断]: [执行语句] elif [判断]: [执行语句] else: [执行语句] 逻辑运算符and or not 这个就没什么好讲的了。 is（附Python中有关地址的一些事）is用于判断两个变量的地址是否相等——==只能看两个值是否一样，但只有内存相等，那我们才能说 a is b 如下例： a = [1,2,3] b = [1,2,3] print(id(a),&quot; &quot;,id(b)) print(a is b) 结果为： 2519572185472 2519572513920 False 对了忘了说了，查地址用 id()。 然而在实验时，很惊讶地发现这样一件事： a = (1,2,3) b = (1,2,3) print(id(a),&quot; &quot;,id(b)) print(a is b) 结果为： 2585870549184 2585870549184 True 这是为什么？在网上找了一下，找到一个说得比较全的，保险起见复制下来好了： 我们之前提到过，Python中有一些和我们认知不同的事情，比如： a = 1 b = 1 print(a is b) a = (1, 2, 3) b = (1, 2, 3) print(a is b) # True # True 还有更奇怪的： a = (1,2,3) b = tuple(a) print(a is b) # True 所有的这些，其实都是一种Python解释器的优化机制，它会使用类似Java中字符串常量池的做法，把对不可变对象的引用指向相同的一个对象，避免重复构建不必要的对象，这样做会避免不必要的空间开销。这种做法在Python中称为“驻留”。 需要说明的是，驻留是属于Python解释器的实现细节，普通开发者无需细究，也不能依赖于这一特性，因为实际运行中，相同的不可变元素的引用是否一定会指向同一个变量，那取决于Python解释器。 我们也不必担心可能出现的bug，比如： a = (1,2,3,[1,2]) b = (1,2,3,[1,2]) print(a is b) # False 当元组包含可变容器的时候，解释器很聪明地没有使用同一引用。 然而其实这个也是对的： a = [1,2,3] b = a print(a is b) # True 但是，再往下又会发现不对： a = [1,2,3] b = a print(a is b) print(id(a)) print(id(b)) b = (1,2,4) print(a is b) print(id(b)) # True # 2290771788160 # 2290771788160 # False # 2290772033728 暂时不深入看了，实验到这里也能自己看出一点规律了罢（雾）。 默认为 FalseFalse ,None,0,&#39; &#39;,(),[],&#123;&#125; Loops and Iterations - For/While Loopsfor 基本语句for i in xxx : 这个语句里面就默认了每次循环完后会进行一次 i++。不过由于Python语言特性，for循环中的变量建议见名知意。 break和 continue用法不变。 有限次循环例：for i in range(1:11)，这个代表循环从1到10。 while基本语句while xxx: 其余与C语言无异。 由于Python中的for循环没有体现出条件判断的功能，故若涉及条件判断还请多用 while。 练习打印矩形for i in range(6): for j in range(4): print(&#39;*&#39;, end=&quot;&quot;) print() 打印三角形for i in range(6): for j in range(i+1): print(&#39;*&#39;, end=&quot;&quot;) print() 打印菱形for i in range(1,6): for j in range(1,6-i+1): print(&#39; &#39;,end=&quot;&quot;) for j in range(6-i+1,6+i): print(&#39;*&#39;,end=&quot;&quot;) print() i = 4 while i&gt;=1: for j in range(1,6-i+1): print(&#39; &#39;,end=&quot;&quot;) for j in range(6-i+1,6+i): print(&#39;*&#39;,end=&quot;&quot;) print() i-=1 Functions基本操作定义一个空函数： def func(): pass pass保证当这个函数还只是空函数时编译器不会报错。 基本操作与C中无异。 视频中提到了函数默认参数，这玩意C中也是有的，怕不记得提一嘴。 *args和 **kwargs*args 和**kwargs主要用于函数定义，用于将不定数量的参数传递给一个函数。 例如：def func(*args,**kwargs)： *args 表示任何多个无名参数，它是一个tuple；**kwargs 表示关键字参数，它是一个dict。并且同时使用*args和**kwargs时，必须*args参数列要在**kwargs前，像foo(a=1, b=&#39;2&#39;, c=3, a&#39;, 1, None, )这样调用的话，会提示语法错误“SyntaxError: non-keyword arg after keyword arg”。 其实不一定要叫*args和 **kwargs，你随便叫什么别的也没问题，但这个算是约定俗成了。因为 *args全称arguments，**kwargs全称keyword arguments，即“参数”和“关键字参数”。 使用例： def func(*args,**kwargs): print(args) print(kwargs) func(1,2,3,4,d=100,e=&#39;a hundred&#39;) # (1, 2, 3, 4) # &#123;&#39;d&#39;: 100, &#39;e&#39;: &#39;a hundred&#39;&#125; 列表推导自从大一暑假花了两天时间（应该是）用Matthes的书把Python过了一遍后才发现Python根本不需要看视频，看书就完全可以了。然后后面就一直没打开过这个文件了。 这次是一点进阶的东西：列表推导。因为个人认为这玩意很帅。 列表推导式语法1.简单版语法：[表达式 for 变量 in 可迭代对象] #生成一个1-10的列表， #x代表向列表中添加的元素，而x来自于后面for循环 [x for x in range(1,11)] 2.带条件无else： [表达式 for 变量 in 可迭代对象 if 条件语句] #生成一个0-9的偶数列表 #x代表向列表中添加的元素，x来自于后面for循环，但是x要满足后面if条件才往列表中添加 [x for x in range(10) if x%2==0] 3.带条件且有else：[表达式 if 条件语句 else 条件语句 for 变量 in 可迭代对象] #生成一个0-10的列表，其中偶数加2 ，奇数+1 #x 如果满足条件语句就执行if前面的x+2，如果不满足则执行else后面的x+1 [x+2 if x%2==0 else x+1 for x in range(11)] 如果要用else的话for就要挪到后面去。 多个for循环语法： 表达式 第一个for 第二个for 假如要生成一个列表，每个元素由1-4的奇数，和1-4的偶数对应：[(1, 2), (1, 4), (3, 2), (3, 4)] #不使用列表推导式，一般方法： l1=[] for i in range(1,5): if i%2!=0: for j in range(1,5): if j%2==0: l1.append((i,j)) print(l1) #如果使用列表推导式： #（i,j）是要添加到列表的数据变量 # i的取值 第一个for i in range（1,5） 条件是i%2！=0 l3=[(i,j) for i in range(1,5) if i%2!=0 for j in range(1,5) if j%2==0] print(l3) 推导式扩展不止列表有推导式，还有字典推导式，集合推导式（语法相同只是外边[]变成{}） #字典推导式 dict1=&#123;&#39;name&#39;:&#39;jimi&#39;,&#39;age&#39;:300&#125; #k的值是age时产生一个新的字典 d1=&#123;k:v for k,v in dict1.items() if k is &#39;age&#39; &#125; print(d1) #集合推导式 l1=[1,1,2,3,4,4,4] s1=&#123;i for i in l1&#125; print(s1) 以上内容全是从CSDN上剽的，这玩意要想熟练掌握还得靠自己平时多加练习。","categories":[],"tags":[]},{"title":"Datalog札记","slug":"Datalog札记","date":"2022-03-14T14:04:16.000Z","updated":"2024-05-29T07:24:34.767Z","comments":true,"path":"Datalog札记.html","link":"","permalink":"https://asparticguan.github.io/blog/Datalog%E6%9C%AD%E8%AE%B0.html","excerpt":"","text":"Datalog札记前言中文互联网上——至少是在谷歌、知乎、百度上——找不到一篇系统地讲述Datalog的博客。有者，也基本上是对北大熊英飞老师或南大李樾老师、谭添老师的简单总结。本文基于《Incrementalizing Static Analyses in Datalog》一书，力求更加深入的讲述有关Datalog的知识。 Datalog文法都学了Datalog了不可能上下文无关文法看不懂吧。下面是Datalog的基本文法形式。事实上用CFG直接描述比用文字讲述快得多。 &lt;program&gt; ::= &lt;rule&gt; &lt;program&gt; | \"\" &lt;rule&gt; ::= &lt;atom&gt; \":-\" &lt;atom-list&gt; \".\" &lt;atom&gt; ::= &lt;relation&gt; \"(\" &lt;term-list&gt; \")\" &lt;atom-list&gt; ::= &lt;atom&gt; | &lt;atom&gt; \",\" &lt;atom-list&gt; | \"\" &lt;term&gt; ::= &lt;constant&gt; | &lt;variable&gt; &lt;term-list&gt; ::= &lt;term&gt; | &lt;term&gt; \",\" &lt;term-list&gt; | \"\" 举例如下： CFlow(src, trg) :– CSimple(src, trg). CFlow(src, trg) :– CWhile(src, trg). CSimple(src, trg) :– PrecedingStatement(src, trg), SimpleStatement(src). CWhile(src, trg) :– PrecedingStatement(src, trg), WhileStatement(src). CWhile(src, trg) :– WhileStatement(src), FirstStatement(src, trg). CWhile(src, trg) :– WhileStatement(trg), LastStatement(trg, src). 其中CWhile，PrecedingStatement称为谓词符号（predicate symbol），对h :- a1, a2, ..., an，h和ai皆称元素（atom）。元素中括号前面的叫关系名（relation name）。括号里面的叫term，term可以是常量或变量。只包含常量的term叫ground term或fact。一整行称为一个规则（rule），以句号（.）结尾。 EDB和IDBDatalog和数据库之间有着很深的关系。我们利用Datalog进行程序分析，靠的就是这两个数据库。在 Datalog 语言中，称先验定义的谓词为 外延数据库（extensional database， EDB） ，其中的关系是不可修改的，可视为输入关系；称由规则建立的谓词为 内涵数据库（intensional database， IDB） ，其中的关系是由规则推导出来的，可视为输出关系。 Datalog就是通过给定的规则和 EDB ，推导出IDB，直到没有新的元组被推导出来为止。 Datalog的模型论语义从一个EDB实例开始，并使用Datalog程序，Datalog求解器的任务是计算的最小模型。可以证明一定存在。 模型论语义（model-theoretic semantics）关注的是Datalog程序的声明性性质，它解释了IDB实例是什么，但不涉及如何计算这些关系。 但是到底如何计算呢，这需要用到另一种语义：不动点语义（fixpoint-theoretic semantics）。 Datalog的不动点语义在程序分析里面不动点是老生常谈的了。在Datalog中，我们定义直接结果算子（Immediate Consequence Operator） 。这个算子将一个数据库实例（我们可以称之为）映射到另一个数据库实例（），其中包含了的所有元组，以及通过应用中的一条规则基于中的事实能够直接推导出来的所有额外元组。换句话说，是的直接结果，意味着它是通过单步应用Datalog规则从衍生出来的最大扩展。 单调性是的一个重要属性。它保证了如果数据库实例包含在中（即），那么应用于得到的结果也会被应用于得到的结果所包含（即）。这意味着增加更多的初始事实只会导致能推导出更多而不会减少能推导出的元组。最终IDB的结果就是推导到不动点的结果。 否定在Datalog里面否定（negation）是一件很难办的事情，因为稍有不慎就可能导出矛盾。例如： A(x) :- B(x). B(x) :- not A(x). 这就什么都推不出了。为了解决这个问题，需要利用分层（stratified）规则。即谓词上的任何环状依赖不能包含否定规则。举例如下： data(D, V) :- gen(D, V). data(D, V) :- edge(V', V), data(D, V'), not kill(D, V). data(d, entry). 这是定义可达性分析的Datalog代码。其谓词依赖图如下图所示： 图中data构成环状依赖。所以不能有一条规则中有形如data :- not data这样的形式。 分层关系也可以通过图中看得出来，gen, kiil, edge的层级比data要低，严格来说，定义层级，如果在一条规则中为计算关系 用到了关系 的肯定，且 ，则 ；如果用到了关系 的否定，则 。 在使用分层进行计算时，由于否定规则将谓词分成若干层，则每层需要计算到不动点，多层之间顺序计算。 动态维护IDB这个部分的标题是我自己取的，因为好像也没看到别的中文资料讲这一部分内容。 我们现在知道了Datalog会应用不动点语义不断往IDB里面加推理结果直到没有新结果可以加入了。那么，如果此时需要改动EDB，怎么去动态维护IDB呢？ 显然答案不是再跑一遍（w）。1993年，Ashish Gupta等人在论文《Maintaining Views Incrementally》中给出了一种解法——针对非递归程序的Counting算法和针对递归程序的DRed算法。 Counting算法首先，Counting算法的Counting指的是其为EDB和IDB中每个元组附带了一个Counting值。这个值的意义是，一个元组能通过多少条不同的规则推导出来，Counting值就是多少。 在Counting算法中需要添加一系列delta规则。delta规则公式如下： \\Delta_i(r): \\Delta(h):- a_1^v, \\ldots, a_{i-1}^v, \\Delta\\left(a_i\\right), a_{i+1}, \\ldots, a_n其中 代表的是更新之后的规则，是更新的情况。则。其中，给定元组集合和，定义如下： 如果元组仅在和中的一个出现，且的Counting值为，那么在中出现且Counting值为。 如果元组同时在和中出现，且Counting值分别为和，那么若则在中出现且Counting值为，反之不在中。 光这么讲应该没人能够听懂，我也不知道《Incrementalizing Static Analyses in Datalog》这本书里面就这么讲了几句是不是觉得读者就能搞懂，反正我没看原论文前是没懂。下面这个例子来自《Maintaining Views Incrementally》原论文。 假设规则如下： hop(X,Y) :- link(X,Z), link(Z,Y). tri_hop(X,Y) :- hop(X,Z), link(Z,Y). 这个的物理意义是在图上，link是图上由一条有向边相连的两点，hop是跳一个点能到的两点，tri_hop是跳两个点能到的两点。 根据上面的规则，得到如下delta规则： (1) △hop(X,Y) :- △link(X,Z), link(Z,Y). (2) △hop(X,Y) :- link^v(X,Z), △link(Z,Y). (3) △tri_hop(X,Y) :- △hop(X,Z), link(Z,Y). (4) △tri_hop(X,Y) :- hop^v(X,Z), △link(Z,Y). 现在我们假设有如下EDB： \\text { link }=\\{a b, a d, d c, b c, c h, f g\\}根据之可推出如下IDB： \\begin{aligned} & \\text { hop }=\\{a c * 2, d h, b h\\} \\\\ & \\text { trihop }=\\{a h * 2\\} \\end{aligned}其中跟在​号后面的就是Counting值，不写就默认为1。 现在将原来的EDB修改如下： \\begin{aligned} & \\Delta(\\text { link })=\\{a b *-1, d f, a f\\} \\\\ & \\operatorname{link}^v=\\{a d, a f, b c, d c, c h, d f, f g\\}\\end{aligned}根据delta规则(1)，得到 根据delta规则(2)，得到 综上， 同样的，根据delta规则(3)，得到 根据delta规则(4)，得到 综上， 这样我们就利用Counting算法动态维护了IDB。 DRed算法Counting算法只适合用来维护没有递归的规则，当遇到规则中带递归时，需要用DRed算法。DRed算法分为三个阶段，每个阶段都要跑到fixpoint再进下一个阶段。 Delete这一阶段计算了IDB中由于EDB元组的删除而需要被删除的元组的一个高估。注意是高估，也就是只要可能被删就一定会被删，不管是不是会有别的规则又把这个元组推导出来。本阶段delta规则如下： \\Delta^{d e l}(h):-a_1, \\ldots, a_{i-1}, \\Delta^{d e l}\\left(a_i\\right), a_{i+1}, \\ldots, a_n其中，若对应的谓词为，若在EDB中，则对应的是要delete的fact；若在IDB中，则对应的是目前根据delta规则计算出来的应该删掉的元组。当全部稳定后，将中对应的全部移除就得到了。 Re-derive这一阶段负责平反，因为上一阶段搞了扩大化，删了很多明明可以用别的规则推导出来的，所以这一阶段要平反。这一阶段的delta规则如下： \\Delta^{r e d}(h):-\\Delta^{d e l}(h), a_1^v, \\ldots, a_n^v在此阶段，根据的变化也会不断更新。 可能一下看这个公式不知道是怎么平反回去的，看后面例子就知道了。 Insert上面两个阶段只针对EDB中删除的元组，还没考虑新插入的元组。这一阶段的delta规则如下： \\Delta^{i n s}(h):-a_1^v, \\ldots, a_{i-1}^v, \\Delta^{i n s}\\left(a_i\\right), a_{i+1}^v, \\ldots, a_n^v .和delete阶段类似，若对应的谓词为，若在EDB中，则对应的是要insert的fact，是更新之后的元组；若在IDB中，则对应的是目前根据delta规则计算出来的应该插入的元组，是re-derive阶段的和的union。 举例看到上面这一堆云里雾里的想必已经迫不及待地要看例子了吧！这个例子是根据上面Counting的例子自己造的，本来应该要举一个递归的例子但我怕错就还是沿用上面的例子。如有错误敬请谅解！ 假设起初的EDB和推理好的IDB如下： \\text{link} = \\{ab,cb,ac,ad,db\\} \\\\ \\text{hop} = \\{ab\\}现在这个图发生了调整，要删去边，并加一条边。示意图如下： delete阶段：对delete阶段，delta规则写为： △del(hop(X,Y)) :- △del(link(X,Z)), link(Z,Y). △del(hop(X,Y)) :- link(X,Z), △del(link(Z,Y)). 现在要删去边，故，推知。把边从中删了。delete阶段结束。此时，。 其实在这里边就是被错杀的，因为还在，删了 仍然可以用这两条边来推。下面就看re-derive阶段怎么救回来。 re-derive阶段：对re-derive阶段，delta规则写为： △red(hop(X,Y)) :- △del(hop(X,Y)), link_v(X,Z), link_v(Z,Y). 其中，且由于没有在规则头出现过，所以这就是的最终结果。结合上面推导得到的，我们得到，也就是顺利把边加回来了。故。 insert阶段：对insert阶段，delta规则写为： △ins(hop(X,Y)) :- △ins(link(X,Z)), link_v(Z,Y). △ins(hop(X,Y)) :- link_v(X,Z), △ins(link(Z,Y)). 由于新加入了边，故，而。 故，。 结合上面讲的Counting算法和DRed算法也可以结合，具体而言就是在re-derive阶段，像前面被错杀的边这种当时如果算Counting值还是正的，所以可以被直接放回去，就不用再这么推了。 但是注意Counting只能用在re-derive阶段。在delete阶段用Counting，放过了本要错杀的，看上去是好的，却是会导致结果出问题的。 结语为了保证文章结构的完整性在这里放个结语，其实也没什么好讲的。参考文献也基本上没有，就是文中提到的两篇论文以及北大南大的两门课提到的一点点基础知识。是为记。","categories":[],"tags":[]},{"title":"Precise Sparse Abstract Execution via Cross-Domain Interaction","slug":"Precise Sparse Abstract Execution via Cross-Domain Interaction","date":"2022-03-14T14:04:16.000Z","updated":"2024-04-17T09:46:17.127Z","comments":true,"path":"Precise Sparse Abstract Execution via Cross-Domain Interaction.html","link":"","permalink":"https://asparticguan.github.io/blog/Precise%20Sparse%20Abstract%20Execution%20via%20Cross-Domain%20Interaction.html","excerpt":"","text":"Precise Sparse Abstract Execution via Cross-Domain InteractionAbstract无论怎么说看论文还是应该先看摘要。可以看到重点在于“Sparse”——稀疏的Abstract Execution。其基本思想为首先进行快速指针分析，对value flow进行过度近似，并仅沿着预先计算的value flow稀疏地传播data flow fact，而不是在所有control flow point上传播。本文介绍的CSA是一种新的跨域稀疏抽象执行方法，它结合了多个抽象域（例如内存地址和区间域）的值之间的相关性。与传统的没有跨域交互的稀疏分析不同，CSA通过建立一个域到另一个域的值的含义来进行相关性追踪。 自然看Abstract是看不懂的。不过还有救，继续看Introduction部分的解释。 Introduction主流的稀疏分析用的都是用的SSA，从而产生了一个sparse representation，即数据流事实只需沿着一个over-approximated的value flow graph而不是control flow graph传播到所需的程序点，从而减少了计算和维护数据流事实的时间和空间。 现有的稀疏技术主要依赖单一的内存地址域上的保守指针分析来支持主要分析阶段，即一种 bootstrap-based sparse analysis。 在静态程序分析中，”bootstrap-based sparse analysis”（基于引导的稀疏分析）是一种优化技术，旨在通过初步的分析（引导阶段）来确定程序中值的流动路径，然后在主分析阶段仅沿着这些预先计算的路径传播数据流信息，以提高分析的效率和可扩展性。这种方法通过减少需要考虑的路径数量，来降低分析的复杂度和执行时间。 在引导阶段，通常会执行一些轻量级的分析，如快速的指针分析，来建立变量间可能的引用关系和数据依赖关系。这个阶段的结果不需要非常精确，但应该覆盖所有可能的数据流路径，以确保随后的主分析阶段不会漏掉潜在的数据流动。 然后，在基于这些预处理信息的基础上，主分析阶段专注于更详细、更深入的分析，比如变量的具体取值范围、潜在的运行时错误等，但仅限于在引导阶段确定的路径上进行。这样做的目的是在保持分析深度和精度的同时，通过减少需要分析的路径数量来提高分析的速度和效率。 总的来说，”bootstrap-based sparse analysis”通过在引导阶段预计算信息来指导主分析，减少了分析的工作量，使得对大型或复杂程序的分析变得更加可行。 在预分析阶段的离线指针分析可能会导致沿着保守数据依赖关系的冗余数据流传播，这些依赖关系在运行时并不存在，从而影响稀疏性。此外，主分析阶段的指针分析（例如SFS和Sparrow中的稀疏流敏感指针分析），由于不了解主分析阶段的数据流分析（如间隔分析）的需求，没有利用来自其他域的信息，留下了大量改进精度的空间。但是，预计算出一个精确的依赖关系（例如考虑数组内部细节的数组敏感分析）是不切实际的，可能会超过主分析的时间，从而阻碍了bootstrap-based sparse analysis的目的。 针对不用其他域的信息可能产生的问题，作者举了如下示例： 在这段代码中，第7行tab不会越界，但是错误的静态分析将会把loc1和loc2视作alias，从而报告程序在访问tab[5]，所以越界。 不使用跨域信息和使用跨域信息的对比如下： 在同时进行指针别名分析和数据流分析阶段，且在合并的抽象域上操作时。主要有两个挑战： 跨域分析的成本和复杂性：在多个域（如指针域和数值域）上进行分析可以提高精度，但这会带来更高的成本。合并指针和数值域的格（lattice）可能会变得非常庞大，计算一个固定点（分析的稳定状态）变得非常困难，除非有一个高效的在线细化过程。此外，在跨域交互过程中，还必须保证稀疏分析的准确性不被损害。 内存对象过多的问题：当处理字段和数组敏感分析时，如处理ids中的每个元素所示，特别是在大型程序中，内存对象的数量可能会大幅增加。这就需要（虽然困难但必要）最小化在有大量内存对象存在时，进行指向性（points-to）和区间分析所涉及的计算和内存开销。 为了解决现有稀疏技术的不精确问题，在合并的抽象域上进行指针别名和数据流分析时，需要面对和解决跨域分析的高成本和复杂性，以及处理大量内存对象带来的挑战。 针对这两个问题，作者提出了新框架CSA。CSA是一种在多个抽象域上进行的精确且高效的稀疏抽象执行方法。为了应对挑战1，文章提出了一种跨域细化技术，有效地捕获了指针分析与间隔分析之间的相关性。不同于简单地将每个分析域的结果结合起来，CSA通过创新性地使用reduced cardinal power来实现对两种分析的在线双向细化。通过快速过度近似的指针分析初步建立def-use（定义-使用）依赖关系，随着主分析的进行，指针别名信息会逐渐在线细化，减少了由离线分析产生的错误数据流依赖的传播。同时，精确的指针别名信息也提高了主分析阶段间隔分析的准确性。为了应对挑战2，文章提出了等价关联跟踪技术，这种方法高效地处理过多的内存对象。该技术通过识别和合并具有等价含义结果的内存地址集合，旨在最小化处理内存对象时的计算开销和内存成本。 框架基本结构为：①输入：首先让LLVM IR经过SVF的分析变成ICFG和SVFG（Sparse Value Flow Graph，使用Andersen指针分析完成）。②稀疏分析：CSA跨的是内存地址域和数值区间域，使用reduced cardinal power来连接两域，同时内存地址域中的冗余信息也会被消除。③下游任务：CSA支持缓冲区溢出、空值引用等的检查。 BackgroundCombined Abstract Domains文章中的定义1（Definition 1）提出了一个具体化的概念（Concretization for Cartesian Product），该定义是针对于组合抽象域中的笛卡尔积（Cartesian Product）进行具体化的描述。定义具体内容如下： 对于抽象域 中的一个抽象值 ，假设 是这些抽象值对应的具体值集合。那么 的具体化值是这些具体值集合的交集，即 。 这个定义说明了，在组合抽象域中，每个抽象值是由不同抽象域中的值组成的元组。每个抽象域有一个具体化函数 ，用于将抽象值映射到具体值集合。一个元组的具体化值是由这些具体值集合的交集给出的，这反映了在组合抽象域中如何通过结合不同域的信息来获得更加精确的具体化结果。 这个定义提供了一种方式，通过考虑多个不同抽象域中的信息并将它们结合起来，来提高分析的精度和可靠性。 Language论文中分析的语言是LLVM类语言。程序中的变量被分为两类：顶级变量和地址取值变量。顶级变量（包括栈虚拟寄存器和全局变量）在LLVM的语言中只能被定义一次，而地址取值对象只能通过解引用顶级指针在LoadStmt/StoreStmt中读取/修改。 Abstract State and Abstract Execution在论文中，Abstract Trace定义为一个函数 ，这里 代表程序点（即代码中的位置）， 代表变量集合，而 代表抽象域，用于表示变量的抽象状态。具体来说，Abstract Trace是一组通过程序点限定的抽象状态集合，每个状态提供了在给定程序点上每个变量的抽象值。 表示在特定程序点 处，变量 的抽象值。这里 可以是程序语句 执行前后的任一点，即 可以代表程序中任一位置的状态。 这种定义使得Abstract Trace能够捕获程序执行过程中变量状态的变化，而这些变量状态以抽象的形式表示。这是静态分析中的一个关键概念，因为它允许分析器预测程序在各种不同执行路径和条件下可能的状态，而无需实际运行程序。 使用 表示地址取值变量 在 处定义并在 处使用的 value flow。 Motivating Example 上图是使用两种组合抽象域方法对文章开头的示例进行的重新分析。左边是baseline之一，称为CSA-CP，即直接把区间域和内存域用笛卡尔积拼在一起。可以看到，直接拼在一起不互动也就跟分开分析没区别，分析结果还是错的。右边这个就不一样了，叫做Analysis over combined domains using correlation tracking，即利用了相关性跟踪的组合域分析。在这种方法中，只维护 top-level variable 的abstract trace。为了跟踪 address-taken variable 的值，分析需要捕捉在分析指针相关语句时 MemAddress 和 Intervals 之间的抽象域之间的关联。这种关联是通过建立一个映射来捕获的，该映射将每个内存对象映射到其暗示的内存地址或区间值。即上右图中绿色高亮中的信息。 一言以蔽之，top-level variable是从没被&amp;取过地址的变量，address-taken variable是曾被&amp;取过地址的变量。 意思差不多是这么个意思，接下来是具体算法。 ApproachCorrelation Tracking Across Domains首先介绍了几个定义。我们将称为一个Implication（蕴含），其中是MemAddress，是IntervalMemAddress，也就是的类型是MemAddressIntervalMemAddress。用表示程序点处的implications，表示在处的implication结果，即 是地址 上的抽象域值。 简单来说就是，负责 top-level variable 的抽象域，负责 address-taken variable 的抽象域。 下图展示的就是分析规则。 其中GepStmt是Get Element Pointer Statement的意思。这条规则负责使用区间域的内容更新内存地址域。也就是Motivating Example一节中很关键的 能被更新的原因。 Equivalent Correlation Tracking在第4.1节中的相关跟踪为每个单对象都建立了implication。这种single-object correlation tracking可能会引入冗余的连接操作，即保存的其实是具有等效含义结果的内存地址，造成了浪费。此外，单对象方法可能会消耗更多内存来冗余存储抽象值。为了避免冗余计算和存储，我们引入了一种等价推理跟踪方法，它合并具有相同推理结果的内存对象，以便我们可以分析和仅一次地存储这些等价推理对象而不损失精度或准确性。 如上图所示是这种方法的一个示例。简单说就是合并相同的。下图则是在这种方法下新的规则。","categories":[],"tags":[]},{"title":"ProbLog A Probabilistic Prolog and Its Application in Link Discovery","slug":"ProbLog- A Probabilistic Prolog and Its Application in Link Discovery","date":"2022-03-14T14:04:16.000Z","updated":"2024-06-27T14:33:30.388Z","comments":true,"path":"ProbLog- A Probabilistic Prolog and Its Application in Link Discovery.html","link":"","permalink":"https://asparticguan.github.io/blog/ProbLog-%20A%20Probabilistic%20Prolog%20and%20Its%20Application%20in%20Link%20Discovery.html","excerpt":"","text":"ProbLog: A Probabilistic Prolog and Its Application in Link Discovery本文是首先提出将Prolog和概率结合在一起的，但是这篇文章也就仅限于结合了此二者。从他举的例子上可以看出来他主要关注的是生物数据库，而非代码分析。 首先先看一下Problog的一般形式，这是这篇论文里最简单的部分了： 随后还要做一个很重要的定义就是在一个Problog程序中，对于一个查询，成功（即有解）的概率是多少。什么是一个查询？就比如在上面的例子中，我们就可以查likes(john,tom)的概率，即john和tom有多大概率相互喜欢呢。 我们一步一步思考如何求。假设程序，其中，那么如果有子集，使得当中元素全部为真时可以成立，则我们可以得到： P(q \\mid T) = \\sum_M P(q,M \\mid T)而对于， 其中，为二值概率，如果根据可以成立，则值为1，反之值为0。而 P(L \\mid T)=\\prod_{c_i \\in L} p_i \\prod_{c_i \\in L_T \\backslash L}\\left(1-p_i\\right)其实都是很简单的高中概率知识。 这就是原理部分。但实践中仍然还有很多问题。上面的实际上是理论公式，在实际中搜直接爆炸了，搞不出的。 实际中，首先用SLD消解找的解法。 SLD消解（SLD Resolution）是一种在逻辑编程和自动定理证明中使用的推理方法。SLD是”Selective Linear Definite clause resolution”的缩写，这种方法主要用于处理逻辑程序，例如Prolog中的程序。 SLD消解的基本步骤包括： 选择一个目标子句：从当前的目标集中选择一个子目标。 匹配和替换：找到一个程序中的规则（子句），其头部能够与选定的子目标匹配。将这个规则的体部替换到目标集中，形成一个新的目标集。 重复过程：对新的目标集重复上述步骤，直到目标集中没有子目标（成功）或无法继续匹配（失败）。 SLD消解是一种线性推理方法，每次处理一个子目标，并通过背追（backtracking）机制在搜索空间中进行探索，直至找到一个解决方案或确定没有解决方案。 比如说在上面那个likes(john,tom)的例子中，SLD树如下图所示： 在这个例子中，□有两个，代表成功推出，别的打横线的都是推到这一步没法继续推了。 在实际中，概率的计算公式是： P(q \\mid T)=P\\left(\\bigvee_{b \\in p r(q)} \\bigwedge_{b_i \\in c l(b)} b_i\\right)其中是的解法，是解法中用到的规则。根据上面的公式，我们就可以得到likes(john,tom)的成功概率： \\begin{aligned} & P(\\text { likes }(\\text { john, tom }) \\mid T)=P\\left(\\left(l_1 \\wedge l_2 \\wedge f_1 \\wedge f_2 \\wedge f_4\\right) \\vee\\left(l_1 \\wedge l_2 \\wedge f_1 \\wedge f_3\\right)\\right) \\end{aligned}然而你光有这个式子是算不出最终概率的。光这个式子就有三种结果： l1 l2 f1 f2 f3 f4 1 T T T F T F 2 T T T T T T 3 T T T F T T 4 T T T T T F 5 T T T T F T 事实上这是#SAT问题，也就是计算析取范式SAT结果总数，这是一个NP困难问题，所以硬枚举是枚不出的。 [!NOTE] 这里还牵涉到一个问题就是这种算法和理论上的算法算出来的结果会是一样的吗？经过观察可以发现是肯定的。上表中情况1和2合起来对应的是只有子句2成立的情况，情况2和3合起来对应的是只有子句1成立的情况，也就是理论中的算法。 那么怎么把最终的概率算出来呢？这需要用到BDD（binary decision diagrams，二元决策图）。BDD是一种用于表示布尔函数的图形化数据结构。其由二元决策树归约得到（决策树总知道吧！不知道去看李航《统计学习方法》第五章，具体归约过程可看https://zhuanlan.zhihu.com/p/397164596）。 有了BDD的话就可以算概率了。具体算法如下，是一个很简单的递归过程。 当然，当程序继续变大这样的递归算法还是会很慢。所以作者也研究了近似算法。 近似算法基于以下几点观察： 在DNF中，如果有概率为1的变量，可以直接删去。 假设在搜索SLD树的过程中（搜SLD用的是从左到右的深搜），已经搜出一个DNF ，而在后续搜索过程中有一个，满足，即被逻辑蕴含，那么这些证明可以被忽略。由于我们处理的是单调DNF公式，这个条件可以通过验证 不被（）中任何一个合取公式蕴涵来检查。 不完整的SLD树可以用来估计待计算概率的上下界。从SLD中提取两个DNF。第一个DNF 是树中已经得到的proof。第二个DNF 不仅包含树中已经得到的proof，还有被截断的proof。然后我们有：​。 于是就产生了一种通过上下逼近来估计概率的方法： 但是这个算法写法上有不少瑕疵。首先，Iterate中的第6-11行都应该退一个tab（shift+tab一下）。另外，第12行call的Iterate函数少传了两个DNF参数，实际上就是原样传入。故这行应当写成：。 修改了这些小问题后，稍微解释一下Iterate函数的意义。大多数会落到else里面。在else里面，用一条规则推一下，相当于在SLD树中向下推了一层，也就是离depthbound近了一步，所以要减一。 再来看前面的if和elseif。if代表推完了，那么（下界）和（上界）都要加上这个完整的proof。elseif代表没推完但depthdound到了，那么（上界）要加上这个不完整的proof。 但是有一点要注意的是，这个算法不一定能终止。例如，对查询? - p，Problog程序为1.0 p :- p，bound为，这个情况就无法终止。","categories":[],"tags":[]},{"title":"Rust札记","slug":"Rust札记","date":"2022-03-14T14:04:16.000Z","updated":"2024-03-26T08:22:47.889Z","comments":true,"path":"Rust札记.html","link":"","permalink":"https://asparticguan.github.io/blog/Rust%E6%9C%AD%E8%AE%B0.html","excerpt":"","text":"原神，启动！——Rust札记变量绑定与解构我们只说重点。 直接let x = 5这样声明后x就不能改了。如果还想改请写成let mut x = 5。mut即mutable。 如果声明了不使用会报警告。要么在变量名前加下划线写成let _x = 5，要么在开头加注释#[allow(unused_variables)]。 任意类型的基本变量输出都可以用println!(\"{:?}\",x)这样的形式。 Rust 允许声明相同的变量名，在后面声明的变量会遮蔽掉前面声明的，且允许类型不同。这是mut做不到的。 Rust中的const：const MAX_POINTS: u32 = 100_000;，注意常量名建议全大写，且必须显式指明类型。 数值类型首先应该厘清两个在用Visual Studio时就该厘清的概念：Debug和Release。Debug 配置主要用于开发和调试，提供更多的开发支持和错误检测。Release 配置则用于最终的产品发布，注重性能和可执行文件大小。 在当使用 --release 参数进行 release 模式构建时，Rust不检测溢出。相反，当检测到整型溢出时，Rust 会按照补码循环溢出（two’s complement wrapping）的规则处理。 fw语言，跟C一样算不清数。 for i in 1..5：1,2,3,4；for i in 1..=5：1,2,3,4,5。这个叫做序列，数字和字符都可以用。 对于数学上未定义的结果，例如对负数取平方根 -42.1.sqrt() ，会产生一个特殊的结果：Rust 的浮点数类型使用 NaN (not a number)来处理这些情况。所有跟 NaN 交互的操作，都会返回一个 NaN，而且 NaN 不能用来比较。 用as来做类型转换，如let e: u32 = 100; let _f = e as i64;转成i64。 语句和表达式在我们Rust中，语句和表达式是两种东西。语句完成操作但不返回值，表达式求值并返回值。 fn main() { let y = { let x = 3; x + 1 }; println!(\"The value of y is: {}\", y); } 以上述代码为例，表达式不能包含分号。这一点非常重要，一旦你在表达式后加上分号，它就会变成一条语句，再也不会返回一个值。上述代码的结果显然是4。 函数 函数名和变量名使用蛇形命名法(snake case)，例如 fn add_two() -&gt; {} 函数的位置可以随便放，Rust 不关心我们在哪里定义了函数，只要有定义即可 每个函数参数都需要标注类型 函数声明中需标注每一个变量的类型。 一般返回值直接写成表达式。在表达式前面加return也可以。加了return后在最后加引号也可以。即x + 5、 return x + 5、 return x + 5; 都可以，唯独x + 5;不行。 当用 ! 作函数返回类型的时候，表示该函数永不返回( diverge function )，特别的，这种语法往往用做会导致程序崩溃的函数： fn dead_end() -&gt; ! { panic!(\"你已经到了穷途末路，崩溃吧！\"); } fn forever() -&gt; ! { loop { //... }; } // 永不退出循环，故永不返回 所有权所有的程序都必须和计算机内存打交道，如何从内存中申请空间来存放程序的运行内容，如何在不需要的时候释放这些空间，成了重中之重，也是所有编程语言设计的难点之一。在计算机语言不断演变过程中，出现了三种流派： 垃圾回收机制(GC)，在程序运行时不断寻找不再使用的内存，典型代表：Java、Go 手动管理内存的分配和释放, 在程序中，通过函数调用的方式来申请和释放内存，典型代表：C++ 通过所有权来管理内存，编译器在编译时会根据一系列规则进行检查，典型代表：Rust 栈中的所有数据都必须占用已知且固定大小的内存空间，假设数据大小是未知的，那么在取出数据时，你将无法取到你想要的数据。 与栈不同，对于大小未知或者可能变化的数据，我们需要将它存储在堆上。 当向堆上放入数据时，需要请求一定大小的内存空间。操作系统在堆的某处找到一块足够大的空位，把它标记为已使用，并返回一个表示该位置地址的指针, 该过程被称为在堆上分配内存，有时简称为 “分配”(allocating)。 接着，该指针会被推入栈中，因为指针的大小是已知且固定的，在后续使用过程中，你将通过栈中的指针，来获取数据在堆上的实际内存位置，进而访问该数据。 由上可知，堆是一种缺乏组织的数据结构。想象一下去餐馆就座吃饭: 进入餐馆，告知服务员有几个人，然后服务员找到一个够大的空桌子（堆上分配的内存空间）并领你们过去。如果有人来迟了，他们也可以通过桌号（栈上的指针）来找到你们坐在哪。 栈比堆要快。 重要的所有权规则 [!IMPORTANT] Rust 中每一个值都被一个变量所拥有，该变量被称为值的所有者 一个值同时只能被一个变量所拥有，或者说一个值只能拥有一个所有者 当所有者(变量)离开作用域范围时，这个值将被丢弃(drop) 字符串类型：Rust中有两种字符串类型，一种是字符串字面值&amp;str，用法为：let s =\"hello\"，即硬编码。还有一种为动态字符串String，用法为：let s = String::from(\"hello\"); 转移所有权let x = 5; let y = x; 在这段代码中没有所有权的转移，而是用值拷贝的方式完成的，是自动拷贝。 let s1 = String::from(\"hello\"); let s2 = s1; 但是这段代码不一样。区别在于String是复杂类型，由存储在栈中的堆指针、字符串长度、字符串容量共同组成。就不能单纯浅拷贝了，要用let s2 = s1.clone();进行深拷贝（Clone）。如果就像上面那样进行浅拷贝，结果是触发所有权规则第二条，“一个值同时只能被一个变量所拥有，或者说一个值只能拥有一个所有者”，因此s1失效了。 &amp;str可以直接浅拷贝。还有可以直接浅拷贝（Copy）的有： 所有整数类型，比如 u32 布尔类型，bool，它的值是 true 和 false 所有浮点数类型，比如 f64 字符类型，char 元组，当且仅当其包含的类型也都是 Copy 的时候。比如，(i32, i32) 是 Copy 的，但 (i32, String) 就不是 不可变引用 &amp;T ，例如转移所有权中的最后一个例子，但是注意: 可变引用 &amp;mut T 是不可以 Copy的 对函数来说，将值传递给函数的过程中可能有所有权的转移。比如下面这个例子： fn main() { let s = String::from(\"hello\"); // s 进入作用域 takes_ownership(s); // s 的值移动到函数里 ... // println!(\"在move进函数后继续使用s: {}\",s); // ... 所以到这里不再有效 let x = 5; // x 进入作用域 makes_copy(x); // x 应该移动函数里， // 但 i32 是 Copy 的，所以在后面可继续使用 x } // 这里, x 先移出了作用域，然后是 s。但因为 s 的值已被移走， // 所以不会有特殊操作 fn takes_ownership(some_string: String) { // some_string 进入作用域 println!(\"{}\", some_string); } // 这里，some_string 移出作用域并调用 `drop` 方法。占用的内存被释放 fn makes_copy(some_integer: i32) { // some_integer 进入作用域 println!(\"{}\", some_integer); } // 这里，some_integer 移出作用域。不会有特殊操作 所以说，相当于说可以浅拷贝的，函数传参时就直接浅拷贝了。只能深拷贝的，函数传参也不深拷贝，直接拿原来的用，发生了所有权的转移。 函数的返回值也有所有权。如下例： fn main() { let s1 = gives_ownership(); // gives_ownership 将返回值移给 s1 } fn gives_ownership() -&gt; String { // gives_ownership 将返回值移动给 // 调用它的函数 let some_string = String::from(\"hello\"); // some_string 进入作用域. some_string // 返回 some_string 并移出给调用的函数 } 引用和借用在前面takes_ownership一例中，s的值移动到函数中后，其所有权会转移给函数参数，s就始终无效了。这样会使整个程序变得复杂。引用可以解决这个问题，下面来看Rust的引用。 fn main() { let x = 5; let y = &amp;x; assert_eq!(5, x); assert_eq!(5, *y); } 此时y的类型是&amp;{integer}。&amp;-引用，*-解引用。 引用，就是允许使用值，但是不获取所有权。 这里和C++做个对比，两者比较容易混淆。 int a = 10; int *pt = &amp;a; // 指针 int &amp;ref = a // 引用 // 指针传参 void function(int* param) { *param = 20; // 修改了指针所指向的原始数据 } int main() { int x = 10; function(&amp;x); // x 现在是 20 } // 引用传参 void function(int&amp; param) { param = 20; // 直接修改了原始数据 } int main() { int x = 10; function(x); // x 现在是 20 } Rust中的引用传参是： fn main() { let s1 = String::from(\"hello\"); let len = calculate_length(&amp;s1); println!(\"The length of '{}' is {}.\", s1, len); } fn calculate_length(s: &amp;String) -&gt; usize { s.len() } 形式上其实更像C++的指针传参。不过Rust这个引用传参的写法讲实话比C++容易理解一些。 正如变量默认不可变一样，引用指向的值默认也是不可变的。需要用可变引用解决此问题。 可变引用写法如下： fn main() { let mut s = String::from(\"hello\"); change(&amp;mut s); } fn change(some_string: &amp;mut String) { some_string.push_str(\", world\"); } 即首先声明变量是可变变量，并在调用函数时也说明引用是可变引用。 [!IMPORTANT] 限制一：同一作用域，特定数据只能有一个可变引用。引用作用域的结束位置是其最后一次使用的位置。 限制二：可变引用与不可变引用不能同时存在。 悬垂引用：悬垂引用是指一个指针（或引用）指向了一个已经被释放的内存区域。例如函数中返回指向某值的指针，函数返回后值释放掉了，但指针还在，这就难办了。 不可变对象不能有可变引用，但可变对象可以有不可变引用。 复合类型字符串切片：在学Python的时候就学过，切片就是切数组。在Rust中也差不多，严格来说就是允许引用集合中部分连续的元素序列。即如下图所示，world切片就是一个新的指针。 let s = String::from(\"hello world\"); let hello = &amp;s[0..5]; // 切片必须是引用 let world = &amp;s[6..11]; 从而注意一个点：切片的类型是引用。特别地，字符串类型String的切片类型为&amp;str，与字符串字面量类型相同。事实上，可以说字符串字面量是切片。String 类型转为 &amp;str 类型取引用即可。 Rust 中的字符是 Unicode 类型，因此每个字符占据 4 个字节内存空间，但是在字符串中不一样，字符串是 UTF-8 编码，也就是字符串中的字符所占的字节数是变化的，这样有助于大幅降低字符串所占用的内存空间。UTF-8的编码规则如下： Unicode码点范围 U+0000 到 U+007F（0-127）的字符采用单字节编码，采用0xxxxxxx的形式，其中x表示码点。 Unicode码点范围 U+0080 到 U+07FF（128-2047）的字符采用两个字节编码，采用110xxxxx 10xxxxxx的形式。 Unicode码点范围 U+0800 到 U+FFFF（2048-65535）的字符采用三个字节编码，采用1110xxxx 10xxxxxx 10xxxxxx的形式。 Unicode码点范围 U+10000 到 U+10FFFF（65536-1114111）的字符采用四个字节编码，采用11110xxx 10xxxxxx 10xxxxxx 10xxxxxx的形式。 String不能索引，即如下操作非法。 let s1 = String::from(\"hello\"); let h = s1[0]; 字符串不能索引的主要原因是采用UTF-8编码。如果按字节索引，对于一个汉字（三字节编码）取到的就只是中间的一个字节，很莫名其妙的。同时在对字符串切片时也要注意这点，不要切到字节中间去了。 元组let tup: (i32, f64, u8) = (500, 6.4, 1); let a = tup.0; // 索引 元组的长度是固定的，元组中元素的顺序也是固定的。 结构体其实有一个问题，在学C++的时候有说struct和class差不多。但为什么Rust作为一门二十一世纪一十年代才出现的语言，仍要保留struct呢。 struct User { active: bool, username: String, email: String, sign_in_count: u64, } // 定义 let user1 = User { email: String::from(\"someone@example.com\"), username: String::from(\"someusername123\"), active: true, sign_in_count: 1, }; // 创建实例 let user2 = User { active: user1.active, username: user1.username, email: String::from(\"another@example.com\"), sign_in_count: user1.sign_in_count, }; // 根据已有的实例创建新的实例 let user2 = User { email: String::from(\"another@example.com\"), ..user1 }; // 或者更快的写法，只详写要更新的部分 // 在此间存在所有权转移。user1 的部分字段所有权被转移到 user2 中：username 字段发生了所有权转移，作为结果，user1 无法再被使用。注意也只有username字段发生了所有权转移，active和sign_in_count这种能Copy的直接都Copy了。并且user1中除了username以外的字段都可以正常使用。 // 结构体必须要有名称，但是结构体的字段可以没有名称，这种结构体长得很像元组，因此被称为元组结构体 struct Color(i32, i32, i32); struct Point(i32, i32, i32); let black = Color(0, 0, 0); let origin = Point(0, 0, 0); 若想直接打印结构体，需要在代码开头加上#[derive(Debug)]，或者使用dbg!打印。它会拿走表达式的所有权，然后打印出相应的文件名、行号等 debug 信息，当然还有我们需要的表达式的求值结果。除此之外，它最终还会把表达式值的所有权返回。 枚举enum PokerSuit { Clubs, Spades, Diamonds, Hearts, } let heart = PokerSuit::Hearts; let diamond = PokerSuit::Diamonds; enum PokerCard { Clubs(u8), Spades(u8), Diamonds(u8), Hearts(u8), } fn main() { let c1 = PokerCard::Spades(5); let c2 = PokerCard::Diamonds(13); } // 枚举值可以带一个值 Option枚举用于处理空值：Option枚举是一种特殊的枚举，定义如下： enum Option&lt;T&gt; { Some(T), None, } Option 枚举包含两个成员，一个成员表示含有值：Some(T), 另一个表示没有值：None。这样做也是为了安全考虑，即为了避免null引发的错误（前几天才碰到过，函数返回值为null然后直接接受并访存导致segmentation fault）。 用法如下： let some_number = Some(5); let some_string = Some(\"a string\"); let absent_number: Option&lt;i32&gt; = None; // 将一个变量声明成None需要显式声明其类型 Option由于已被包含进标准库中，所以不写成Option::Some也是没问题的。如果使用 None 而不是 Some，需要告诉 Rust Option&lt;T&gt; 是什么类型的，因为编译器只通过 None 值无法推断出 Some 成员保存的值的类型。 Option&lt;T&gt; 相比null的好处：Option&lt;T&gt;和T之间不能直接作运算，所以就避免了直接使用可能为None的变量的情况。 数组Rust中的数组分两种 let a: [i32; 5] = [1, 2, 3, 4, 5]; let a: [i32; 5] = [3; 5]; // 一个值重复N次 当数组元素为非基本类型是，不能用[a;b]的语法，因为根据所有权的知识，[a;b]中的元素都是copy出来的，但是非基本类型不能copy，所以就不能用了。 let array = [String::from(\"rust is good!\"); 8]; // 这么写不对 let array: [String; 8] = std::array::from_fn(|_i| String::from(\"rust is good!\")); // 正确写法是这样，长得有点奇怪 let names = [String::from(\"Sunfei\"), \"Sunface\".to_string()]; // `get` 返回 `Option&lt;T&gt;` 类型，因此它的使用非常安全 let name0 = names.get(0).unwrap(); // 但是下标索引就存在越界的风险了 let _name1 = &amp;names[2]; 流程控制fn main() { let names = [String::from(\"liming\"),String::from(\"hanmeimei\")]; // names中的元素不能Copy，所以一定要加引用，否则所有权转移了`println!(\"{:?}\", names);`就报错了 for name in &amp;names { // do something with name... } println!(\"{:?}\", names); let numbers = [1, 2, 3]; // numbers中的元素实现了 Copy，因此无需转移所有权 for n in numbers { // do something with name... } println!(\"{:?}\", numbers); } 当有多层循环时，可以使用 continue 或 break 来控制外层的循环。要实现这一点，外部的循环必须拥有一个标签 'label, 然后在 break 或 continue 时指定该标签 fn main() { let mut count = 0; 'outer: loop { 'inner1: loop { if count &gt;= 20 { // 这只会跳出 inner1 循环 break 'inner1; // 这里使用 `break` 也是一样的 } count += 2; } count += 5; 'inner2: loop { if count &gt;= 30 { break 'outer; // 这里就可以直接跳出outer，不用一层一层再去break了，有点像goto了，但比C语言严格一些 } continue 'outer; } } assert!(count == 30) } 模式匹配match 与 if letmatch target { 模式1 =&gt; 表达式1, 模式2 =&gt; { 语句1; 语句2; 表达式2 }, _ =&gt; 表达式3 } 最后结果是个表达式，所以可以用作赋值、返回值等。match 的每一个分支都必须是一个表达式，且所有分支的表达式最终返回值的类型必须相同。 match 的匹配必须穷尽所有情况。 let v = Some(3u8); match v { Some(3) =&gt; println!(\"three\"), _ =&gt; (), } // 这个写得太繁琐了 if let Some(3) = v { println!(\"three\"); } // 如果只是想针对性地处理一种情况，可用if let。但其他所有情况还是都用match enum MyEnum { Foo, Bar } fn main() { let v = vec![MyEnum::Foo,MyEnum::Bar,MyEnum::Foo]; } v.iter().filter(|x| matches!(x, MyEnum::Foo)); // 用matches!进行筛选，只保留类型是 MyEnum::Foo 的元素 match可以用来解构Option。举例如下： fn plus_one(x: Option&lt;i32&gt;) -&gt; Option&lt;i32&gt; { match x { None =&gt; None, Some(i) =&gt; Some(i + 1), } } while let：如果模式匹配就一直循环。例如下面的例子： // Vec是动态数组 let mut stack = Vec::new(); // 向数组尾部插入元素 stack.push(1); stack.push(2); stack.push(3); // stack.pop从数组尾部弹出元素 while let Some(top) = stack.pop() { println!(\"{}\", top); } 可驳模式匹配和不可驳模式匹配：可驳模式匹配允许match中遗漏情况，如if let，类似 let , for和match 都必须要求完全覆盖匹配，才能通过编译，即不可驳模式匹配。 例如，不能写成let Some(x) = Some(10);，因为这样漏了None，如果想这么写，应该写成if let Some(x) = Some(10) {} @绑定enum Message { Hello { id: i32 }, } let msg = Message::Hello { id: 5 }; match msg { Message::Hello { id: id_variable @ 3..=7 } =&gt; { println!(\"Found an id in range: {}\", id_variable) }, Message::Hello { id: 10..=12 } =&gt; { println!(\"Found an id in another range\") }, Message::Hello { id } =&gt; { println!(\"Found some other id: {}\", id) }, } 上面例子中，测试了 Message::Hello 的 id 字段是否位于 3..=7 范围内，同时将其值绑定到 id_variable 变量中以便此分支中相关的代码可以使用它。用到的就是这个@。 使用 @ 还可以在绑定新变量的同时，对目标进行解构： #[derive(Debug)] struct Point { x: i32, y: i32, } fn main() { // 绑定新变量 `p`，同时对 `Point` 进行解构 let p @ Point {x: px, y: py } = Point {x: 10, y: 23}; println!(\"x: {}, y: {}\", px, py); println!(\"{:?}\", p); let pnt = Point {x: 10, y: 5}; if let p @ Point {x: 10, y} = pnt { println!(\"x is 10 and y is {} in {:?}\", y, p); } else { println!(\"x was not 10 :(\"); } } fn main() { match 1 { num @ (1 | 2) =&gt; { // 写成`num @ 1 | 2`就报错了，因为没绑定全 println!(\"{}\", num); } _ =&gt; {} } } 最后再注意一个点： fn main() { let mut v = String::from(\"hello,\"); let r = &amp;mut v; match r { ①&amp;mut value =&gt; value.push_str(\" world!\") // 这行是错的 ②value =&gt; value.push_str(\" world!\") // 这行是对的 ③&amp;mut ref mut value =&gt; value.push_str(\" world!\") // 这样也可以 } // 若用②，这里会报错，因为match已经拿走r的值的所有权 // 若用③，这里不会报错 println!(\"{}\", r); } 用①时，value的类型是String，是不可变的，所以报错。对应地，如果前面是let mut v = 1;，是不会报错的。 用②时，虽然是对的，但r的值的所有权转移给了match。出match后r就用不了了。 用③时，首先需要解释一下&amp;mut ref mut value是什么。如果我们把&amp;mut ref mut value看作&amp;mut X，那么按照上面的思路：X能从r中匹配到v。如果表达为let模式匹配：let ref mut value : &amp;mut String = v。ref的作用是使value作为指针指向字符串v，加上mut使其可变。因此类型为&amp;mut String。这里的value其实是通过模式匹配得到的对原始数据v的可变引用r的再借用。这好像就有些超纲了（w）。 方法总体印象：Rust类中的成员和方法是分离的。 struct Circle { x: f64, y: f64, radius: f64, } impl Circle { // new是Circle的关联函数，因为它的第一个参数不是self，且new并不是关键字 // 这种方法往往用于初始化当前结构体的实例 fn new(x: f64, y: f64, radius: f64) -&gt; Circle { Circle { x: x, y: y, radius: radius, } } // fn new(x: f64, y: f64, radius: f64) -&gt; Self { // Self { // x: x, // y: y, // radius: radius, // } // } // 或者改成用Self关键字 // Circle的方法，&amp;self表示借用当前的Circle结构体 fn area(&amp;self) -&gt; f64 { std::f64::consts::PI * (self.radius * self.radius) } } 对这段代码的注解： new方法叫做关联函数，因为不带&amp;self，只能用A::B()的形式调用，例如String::from()，还有这里能用Circle::new()来构建新的Circle实例。 self 是 self: Self 的语法糖。&amp;self 是 self: &amp;Self 的语法糖。&amp;mut self 是 self: &amp;mut Self 的语法糖。 #![allow(unused)] enum Message { Quit, Move { x: i32, y: i32 }, Write(String), ChangeColor(i32, i32, i32), } impl Message { fn call(&amp;self) { match self { Message::Quit =&gt; println!(\"Quit\"), Message::Move { x, y } =&gt; println!(\"Move x: {}, y: {}\", x, y), Message::Write(text) =&gt; println!(\"Write: {}\", text), Message::ChangeColor(r, g, b) =&gt; println!(\"ChangeColor r: {}, g: {}, b: {}\", r, g, b), } } } fn main() { let m = Message::Write(String::from(\"hello\")); m.call(); } 如上所示，还可以对枚举定义方法。 泛型与特征泛型根据C++中的经验，泛型应该写成这样：fn add&lt;T&gt;(a: T, b: T) -&gt; T{}。但这样子有问题。因为并不是所有类型都能进行相加操作，故需要对T加以限制，写成fn add&lt;T: std::ops::Add&lt;Output = T&gt;&gt;(a:T, b:T) -&gt; T {}。 结构体泛型：struct Point&lt;T,U&gt; {x: T, y: U,}。这是保证x 和 y 既能类型相同，又能类型不同。 枚举泛型：Option本身是枚举泛型：enum Option&lt;T&gt; {Some(T), None,}。还举一例：enum Result&lt;T, E&gt; {Ok(T), Err(E),} 方法泛型： struct Point&lt;T&gt; { x: T, y: T, } impl&lt;T&gt; Point&lt;T&gt; { fn x(&amp;self) -&gt; &amp;T { &amp;self.x } } fn main() { let p = Point { x: 5, y: 10 }; println!(\"p.x = {}\", p.x()); } 使用泛型参数前，依然需要提前声明：impl&lt;T&gt;，只有提前声明了，我们才能在Point&lt;T&gt;中使用它，这样 Rust 就知道 Point 的尖括号中的类型是泛型而不是具体类型。需要注意的是，这里的 Point&lt;T&gt; 不再是泛型声明，而是一个完整的结构体类型，因为我们定义的结构体就是 Point&lt;T&gt; 而不再是 Point。 再举一例： struct Point&lt;T, U&gt; { x: T, y: U, } impl&lt;T, U&gt; Point&lt;T, U&gt; { fn mixup&lt;V, W&gt;(self, p2: Point&lt;V, W&gt;) -&gt; Point&lt;T, W&gt;{ // 注意这行泛型的字母 Point{x: self.x, y: p2.y} } } fn main() { let p1 = Point { x: 5, y: 10 }; let p2 = Point { x: \"Hello\", y: '中'}; let p3 = p1.mixup(p2); assert_eq!(p3.x, 5); assert_eq!(p3.y, '中'); } const泛型： fn display_array&lt;T: std::fmt::Debug, const N: usize&gt;(arr: [T; N]) { println!(\"{:?}\", arr); } fn main() { let arr: [i32; 3] = [1, 2, 3]; display_array(arr); let arr: [i32; 2] = [1, 2]; display_array(arr); } const泛型主要解决传数组的问题。在Rust中，不同长度的数组类型不同，想要让同一个函数接受不同长度的数组就要用泛型。此时就要用const泛型来规定长度也是泛型。 特征pub trait Summary { fn summarize(&amp;self) -&gt; String; } pub struct Post { pub title: String, // 标题 pub author: String, // 作者 pub content: String, // 内容 } impl Summary for Post { fn summarize(&amp;self) -&gt; String { format!(\"文章{}, 作者是{}\", self.title, self.author) } } pub struct Weibo { pub username: String, pub content: String } impl Summary for Weibo { fn summarize(&amp;self) -&gt; String { format!(\"{}发表了微博{}\", self.username, self.content) } } fn main() { let post = Post{title: \"Rust语言简介\".to_string(),author: \"Sunface\".to_string(), content: \"Rust棒极了!\".to_string()}; let weibo = Weibo{username: \"sunface\".to_string(),content: \"好像微博没Tweet好用\".to_string()}; println!(\"{}\",post.summarize()); println!(\"{}\",weibo.summarize()); } 在Java里面这个好像叫“接口”，但我也没怎么学过Java也不太记得了。 就如上面的代码所示，首先定义一个特征Summary，然后分别在Post和Weibo两个结构体中实现之，然后就可以调用了。 pub fn notify(item: &amp;impl Summary) { println!(\"Breaking news! {}\", item.summarize()); } 上面这段代码传入的参数的类型是impl Summary，意思是 实现了Summary特征 的 item 参数。使用方法如notify(&amp; weibo);。 上面这种写法是pub fn notify&lt;T: Summary&gt;(item: &amp;T)的语法糖。事实上，前面泛型中举的例子就是这样写的：fn add&lt;T: std::ops::Add&lt;Output = T&gt;&gt;(a:T, b:T) -&gt; T {}这种完整写法的好处是可以在传多个参数时控制参数类型相同，如：pub fn notify&lt;T: Summary&gt;(item1: &amp;T, item2: &amp;T)。 此外，还可以控制多重约束，例如：pub fn notify(item: &amp;(impl Summary + Display)) {}或pub fn notify&lt;T: Summary + Display&gt;(item: &amp;T) {}。 可以通过 impl Trait 来说明一个函数返回了一个类型，该类型实现了某个特征： fn returns_summarizable() -&gt; impl Summary { Weibo { username: String::from(\"sunface\"), content: String::from( \"m1 max太厉害了，电脑再也不会卡\", ) } } 但是这种返回值方式有一个很大的限制：只能有一个具体的类型。如果想要实现返回不同的类型，需要使用下一章节中的特征对象。 use std::fmt::Display; struct Pair&lt;T&gt; { x: T, y: T, } impl&lt;T&gt; Pair&lt;T&gt; { fn new(x: T, y: T) -&gt; Self { Self { x, y, } } } impl&lt;T: Display + PartialOrd&gt; Pair&lt;T&gt; { fn cmp_display(&amp;self) { if self.x &gt;= self.y { println!(\"The largest member is x = {}\", self.x); } else { println!(\"The largest member is y = {}\", self.y); } } } 特征约束，可以在指定类型 + 指定特征的条件下去实现方法。例如上面的cmp_display 方法，并不是所有的 Pair&lt;T&gt; 结构体对象都可以拥有，只有 T 同时实现了 Display + PartialOrd 的 Pair&lt;T&gt; 才可以拥有此方法。 也可以有条件地实现特征。假设我们有一个Printable特征，我们只想为实现了std::fmt::Display特征的类型实现这个Printable特征。这可以通过如下方式完成： trait Printable { fn print(&amp;self); } impl&lt;T: std::fmt::Display&gt; Printable for T { fn print(&amp;self) { println!(\"{}\", self); } } 在这个示例中，Printable特征会被有条件地实现于所有实现了std::fmt::Display特征的类型上。这意味着任何满足Display约束的类型都自动成为Printable，可以调用.print()方法。 泛型和特征还有一堆内容没看，但这部分实在太抽象了，先跳了…… Vector和Hashmap老朋友了，我建议用的时候查表。 生命周期编译器使用三条消除规则来确定哪些场景不需要显式地去标注生命周期。其中第一条规则应用在输入生命周期上，第二、三条应用在输出生命周期上。若编译器发现三条规则都不适用时，就会报错，提示你需要手动标注生命周期。 每一个引用参数都会获得独自的生命周期 例如一个引用参数的函数就有一个生命周期标注: fn foo&lt;'a&gt;(x: &amp;'a i32)，两个引用参数的有两个生命周期标注:fn foo&lt;'a, 'b&gt;(x: &amp;'a i32, y: &amp;'b i32), 依此类推。 若只有一个输入生命周期(函数参数中只有一个引用类型)，那么该生命周期会被赋给所有的输出生命周期，也就是所有返回值的生命周期都等于该输入生命周期 例如函数 fn foo(x: &amp;i32) -&gt; &amp;i32，x 参数的生命周期会被自动赋给返回值 &amp;i32，因此该函数等同于 fn foo&lt;'a&gt;(x: &amp;'a i32) -&gt; &amp;'a i32 若存在多个输入生命周期，且其中一个是 &amp;self 或 &amp;mut self，则 &amp;self 的生命周期被赋给所有的输出生命周期 拥有 &amp;self 形式的参数，说明该函数是一个 方法，该规则让方法的使用便利度大幅提升。","categories":[],"tags":[]},{"title":"User-Guided Program Reasoning using Bayesian Inference","slug":"User-Guided Program Reasoning using Bayesian Inference","date":"2022-03-14T14:04:16.000Z","updated":"2024-05-17T13:43:00.402Z","comments":true,"path":"User-Guided Program Reasoning using Bayesian Inference.html","link":"","permalink":"https://asparticguan.github.io/blog/User-Guided%20Program%20Reasoning%20using%20Bayesian%20Inference.html","excerpt":"","text":"User-Guided Program Reasoning using Bayesian Inference这篇文章和《ProbLog: A Probabilistic Prolog and its Application in Link Discovery》有些相似小。在Problog一文中，加上了概率值的是Datalog中的一条条规则，通过BDD来解决概率计算。而在本文中，是将Datalog的对应的派生图视作贝叶斯网络来解决的，同样也需要将规则加上概率值。 https://zh.wikipedia.org/wiki/%E8%B2%9D%E6%B0%8F%E7%B6%B2%E8%B7%AF# 这篇文章最主要的算法就是这个： 详细解释一下： 第一步是输入待分析的程序，输出程序中的所有关系。 第二步是将输入进Datalog程序中，得到IDB 。通过加上原有的EDB 可以得到派生图。是经Datalog运算得到的警报信息。 第三步要消除派生图中的环，因为派生图要转成贝叶斯网络，而贝叶斯网络是有向无环图。 第四步做了一些优化。 第五步构建贝叶斯网络。构造过程需要用到派生图和各个规则的触发概率。 第六步初始化用户反馈集为空集。 第七步开始运用用户反馈，通过用户反馈根据贝叶斯网络计算条件概率并排序，每次选取排序最高的向用户寻求反馈，再据此迭代。 上面这套流程中有个东西没有讲是怎么来的，就是这个各个规则的触发概率​。对于完全标记数据的语料库，规则概率只是规则在给定真实假设的情况下产生错误结论的次数除以总次数得到的值。对于标记较少的数据集，一个面临潜在（或未观察到）变量的挑战，可以用下面描述的期望最大化（EM）算法来解决。 假设我们有一个数据集，这个数据集标注了各个需要警报的地方的真实结果（即需要警报的地方实际上要不要警报）。现在我们要拿这个去推算规则的概率。根据EM算法的过程，首先随机初始化各规则的概率，然后根据派生图导出的贝叶斯网络得到，即报警的概率。然后利用下面的公式进行更新： p_r^{(t+1)}=\\frac{\\sum_{g_r} \\operatorname{Pr}_{p^{(t)}}\\left(c_{g_r} \\wedge A_{g_r}\\right)}{\\sum_{g_r} \\operatorname{Pr}_{p^{(t)}}\\left(A_{g_r}\\right)}其中代表是与规则有关的地方。为真实结果，若为0的话整个也就为0。 经过一轮轮迭代最终达到稳定就是各个规则的触发概率了。 但是，这篇文章最神奇的地方我觉得也在这里，讲EM算法时来了这么一段： 这个的意思是他在实验中根本没用EM算法，而是直接把概率定成0.999了吗😥","categories":[],"tags":[]},{"title":"Flocks of Stochastic Parrots Differentially Private prompt Learning for Large Language Models","slug":"Flocks of Stochastic Parrots- Differentially Private  Prompt Learning for Large Language Models","date":"2019-11-15T05:46:12.000Z","updated":"2024-03-02T14:20:57.112Z","comments":true,"path":"Flocks of Stochastic Parrots- Differentially Private  Prompt Learning for Large Language Models.html","link":"","permalink":"https://asparticguan.github.io/blog/Flocks%20of%20Stochastic%20Parrots-%20Differentially%20Private%20%20Prompt%20Learning%20for%20Large%20Language%20Models.html","excerpt":"","text":"Flocks of Stochastic Parrots: Differentially Private prompt Learning for Large Language ModelsAbstract本论文主要讨论的是prompt的安全问题。作者发现可以对prompt进行成员推断攻击。虽然可以用放弃prompt并使用通过隐私梯度下降进行的微调来规避，但显然用微调成本太大了。作者首先证明了可以通过对下游数据的梯度下降获得private的soft prompt。但是对于离散的prompt不行。所以作者在呈现不同提示的LLM集群中进行了嘈杂的投票，也就是“一群随机的鹦鹉”。这个投票把集群的知识转化为了单一公共prompt。这样的prompt有着和原有prompt同样好的效果。 Introduction第一个贡献是证明prompt确实会泄露隐私。作者对prompt进行成员推断攻击，即可以确定一个给定的数据是否在prompt中。如果想要减缓隐私风险就只能放弃prompt使用fine-tune，这显然是不现实的。 所以为了保留prompt的好处（不需要很多数据，不需要修改模型）并保护prompt中的数据，作者提出了首个带privacy的prompt learning算法，算法使用了差分隐私。PromptDPSGD算法对LLM private input前的soft prompt embeddings执行private gradient descent。 但是用DPSGD（Differentially Private Stochastic Gradient Descent，差分隐私随机梯度下降）不一定总是有效，因为DPSGD需要计算关于prompt input的梯度，而现有的API并不能提供这些这些梯度（放张图在这里，说明确实是需要API的信息才能计算梯度）。 P-tuning：自动构建模版，释放语言模型潜能 - 科学空间|Scientific Spaces (kexue.fm) 所以作者选用了离散的prompt，并提出了PromptPATE，它创建了一个具有不同离散提示的llm集合，我们称之为一群随机鹦鹉。PromptPATE附加了一个知识迁移，所以，flock中的每个模型都会输出对一个短的公共数据序列的一个next token预测，然后再加入一个对所有模型token输出的噪声多数表决，就得到了一个添加了噪声的，使用了差分隐私的，并保证了从flock中学到知识的输出。 Background and Related WorkPrompt for LLMs Privacy Leakage in LLMs Defending Against Privacy Leakage in LLMs Private Information about Prompt Data Leaks from Prompted LLMs首先定义prompt的格式： 攻击者的目标就是判断一条在不在prompt中。攻击者有很多候选的text sequences和对应的标签。 最后的结果证实这样确实很容易就泄漏了。 Methods for Privacy Preserving PromptsPromptDPSGD: DPSGD for Private Soft Prompt Learning该算法通过DP guarantees产生软提示，可与LLM一起部署以解决相应的下游任务。PromptDPSGD的隐私分析遵循标准DPSGD的分析方法。然而需要注意的是，尽管在概念上与使用DPSGD对LLM参数进行微调相似，但是PromptDPSGD在一个关键的方面有所不同。在DP-SGD微调中，我们需要对所有或部分模型参数进行梯度计算，并更新这些参数以最小化损失。相比之下，PromptDPSGD使用与软提示嵌入相关的梯度，并仅对这些进行修改。我们在附录C中介绍的PromptDPSGD算法突显了这一区别。 尽管这个差异似乎微小，但它具有深远的影响。首先，需要更新的参数数量减少了几个数量级，这提高了训练效率。其次，最重要的是，这使得我们可以继续在原始LLM上操作。我们在本节末尾（4.3节）讨论了由此带来的优势，例如存储效率和同时处理多个不同任务的能力。这些优势使得PromptDPSGD在概念上优于私有微调。同时，正如我们在评估中展示的，尽管可训练参数数量较少，但对于较简单的任务，PromptDPSGD的性能与私有微调相匹配。然而，当前的API不支持软提示、前缀或私有微调，只能通过离散提示提供黑盒访问。对于这些设置，我们提出了PromptPATE。 PromptPATE: PATE for Privacy Preserving Discrete Prompts文中的方主要是对2017年提出的PATE算法的改进。首先介绍PATE（pâté）算法： 当涉及隐私保护的机器学习任务时，PATE方法采用教师-学生架构，以确保在训练过程中不泄露个别的训练样本信息。以下是PATE方法的工作方法，包括具体步骤： 数据集划分：首先，将原始训练数据集分为多个不相交的子集，称为”不可重叠教师数据集”（Non-overlapping Teacher Datasets）。这些子集可以是随机划分的或者基于某种特定的策略划分的，但是确保数据不会被重复使用。 教师模型训练：对于每个不可重叠教师数据集，分别训练一个独立的教师模型。这些教师模型可以是相同的或者不同的机器学习模型，它们在各自的数据集上进行训练。 教师模型预测（私有知识迁移）：使用已经训练好的教师模型，对未标记的测试数据进行预测。由于每个教师模型只看到其中的一部分数据，它们的预测结果可能会存在差异。设一个样本为，可能属于以下类别 ，代表第 个”教师“模型的预测结果，代表对第个类别的投票统计结果，LNMax机制输出结果为：。括号中的后一项为添加的噪音。如果投票存在一个多数则投票通过，否则没有太多共识的提案（query）会被否决以防止泄露教师模型的信息。 学生模型训练：在学生模型的训练中，不仅使用了标记的真实数据，还引入了带有不确定性的预测结果作为辅助信息。学生模型会尝试学习这些不确定性信息，以更好地泛化到未标记的测试数据。 评估学生模型：完成学生模型的训练后，可以使用该模型对未标记的测试数据进行预测，并得到最终的模型性能评估。 通过教师-学生架构和不确定性估计，PATE方法能够确保在隐私性和模型性能之间取得良好的平衡。教师模型的多样性和不确定性估计有助于提高模型的鲁棒性，并在没有暴露个别训练样本信息的情况下进行隐私保护的训练。 具体到PromptPATE中，此模型遵循了标准PATE的三步，即训练教师模型、私有知识迁移和训练学生模型。但是LLM的上下文学习和PATE中的有监督学习区别很大，所以需要重新设计PATE。 首先，现在不是拿数据去训练几个教师模型了，而是直接用私有数据去造交集为空的各种prompt。然后使用这些prompt给公共数据集打标，把在公共数据集上的打标结果作为student model的prompt。 理论上，通过让教师模型给公共数据集打标，就足以创建这样一个prompt。这种方法几乎没有隐私成本，但是这样得到的prompt可能效用不佳。因此，作者基于不同的打了标的公共数据集生成了多个prompt，==并通过prompt tuning（？怎么做到的）选择最佳prompt。==在选择过程中必须谨慎：由于prompt将被公开部署，如果再基于私有数据集上的效用评估进行选择将导致额外的隐私成本。为了解决这种紧张状态，使用部分新标记的公共数据作为验证数据来评估学生提示的效用。通过选择具有最高验证准确度的提示，部署最接近私有教师的学生提示。 Advantages of (Private) Prompting over (Private) Fine-TuningExperimental Evaluation从作者的结果来看，结果还是会损失一些精度的，相对于没考虑privacy的普通prompt，作者的这一通操作会使准确率有从3个点到8个点不等的降低。如果公共数据集和私有数据集不是同一个的话（为什么会是同一个？）降得就更多了，最多的降了11%。 Conclusions and Outlook总结我感觉这篇文章就非常符合万老师之前讲的“重做一遍，先套了再说”，其实这篇文章就是把17年提出，22年修改的PATE工作给照搬过来重做了一遍，甚至比原来的PATE算法更简单了。但最后的结果证明果然还是有缺陷的，还需要进一步改进。文章标题“Flocks of Stochastic Parrots”也只是指的把私有数据集分成几个prompt，感觉有点纯纯噱头。","categories":[],"tags":[]},{"title":"PUMA Secure Inference of Llama-7B in Five minutes","slug":"PUMA- SECURE INFERENCE OF LLAMA-7B IN FIVE MINUTES","date":"2019-11-15T05:46:12.000Z","updated":"2024-03-02T14:20:29.211Z","comments":true,"path":"PUMA- SECURE INFERENCE OF LLAMA-7B IN FIVE MINUTES.html","link":"","permalink":"https://asparticguan.github.io/blog/PUMA-%20SECURE%20INFERENCE%20OF%20LLAMA-7B%20IN%20FIVE%20MINUTES.html","excerpt":"","text":"PUMA: SECURE INFERENCE OF LLAMA-7B IN FIVE MINUTESAbstract以ChatGPT为代表，许多公司开始提供基于大型Transformer模型的服务。然而，使用这样的服务不可避免地会泄漏用户的提示信息给模型提供者。先前的研究已经通过安全多方计算（MPC）来研究了Transformer模型的安全推理，其中模型参数和客户端提示被保密。尽管如此，这些框架在模型性能、效率和部署方面仍存在一定限制。 为了解决这些限制，我们提出了PUMA框架，以实现快速且安全的Transformer模型推理。我们的框架设计了高质量近似函数来降低安全推理成本，并同时保持模型性能。此外，我们还设计了安全embedding和LayerNorm过程，在不损害Transformer架构功能的情况下忠实地实现所需功能。 与最先进的MPC框架MPCFORMER(ICLR 2023)相比，PUMA速度约快2倍，并且具有与未进行微调（之前工作无法达到）的明文模型类似的准确性。另外值得一提的是，PUMA可以在约5分钟内评估LLaMA-7B生成1个标记。 据我们所知，这是首次对一个具有如此规模参数大小的模型进行MPC评估。PUMA已在SecretFlow-SPU1的Github存储库中开源。 Introduction预训练 Transformer 模型在许多实际任务上都表现优良，也因此受到了很大关注，并且现在已经出现了不少基于这类技术的工具，它们常以「深度学习即服务（DLaaS）」范式提供服务。但是，这些服务常会有隐私问题，比如假如用户要使用 ChatGPT， 要么就需要用户向服务提供商提供自己的私人 prompt，要么就需要服务提供商将自己专有的训练得到的权重配置交给用户。 为了解决 Transformer 模型服务的隐私问题，一种解决方案是安全多方计算（Secure Multi-Party Computation），这可以在推理过程中保证数据和模型权重的安全。但是，多方计算（MPC）中简单基础的 Transformer 推理的时间成本和通信成本都很高，难以用于实际应用。为了取得更好的效率，已经有一些研究成果提出了多种加速 Transformer 模型安全推理的方法，但这些方法仍然存在以下一个或多个缺点： 替换很困难。近期一些工作提出，为了降低成本，可使用二次函数和 ReLU 函数等快速近似方法来替代高成本的 GeLU 和 softmax 等函数。但是，如果只是简单替换这些函数，可能会导致 Transformer 模型性能大幅下降（这可能就会需要额外再对模型进行训练，即微调）以及出现部署问题。 推理成本高。有研究提出使用更准确的多项式函数来近似高成本的非线性函数，但其近似方法并未考虑 GeLU 和 Softmax 的特殊性质。因此，使用近似之后，这种方法的成本依然很高。 不容易部署。最近也有些研究提出通过修改 Transformer 的模型架构来加速安全推理，例如分解嵌入过程并重新组织线性层。更糟糕的是，由于 Crypten 框架不支持安全 LayerNorm，因此如果仅使用 BatchNorm 模拟成本，就会导致安全推理得到不正确的结果。这些修改方式与现有的明文 Transformer 系统存在冲突。 综上所述，在 MPC Transformer 推理领域，模型性能和效率难以兼得，于是就有如下问题： 能否安全又高效地评估预训练大型 transformer 模型，同时无需进一步再训练也能达到与明文模型相近的准确度。 Related WorkTransformer模型在语言理解方面取得了显著的成功，在视觉理解方面也取得了成功。通常，Transformer模型采用两阶段的训练策略：i）首先在大型数据集上进行预训练，以获得一般理解能力，ii）然后在较小的下游数据集上进行微调[Sun等，2020年]，以学习任务特定的特征，从而提高模型性能。这种训练策略在各种场景中被证明是有效的，并且已经成为主流范式。在这项工作中，我们假设模型提供者在在线服务中使用预训练和微调的Transformer模型。 安全多方计算（MPC）使不信任的各方能够在保持其私有输入安全的情况下共同计算一个函数，使用MPC进行安全神经网络推断因其高度的隐私保护而受到了广泛关注。这些工作在各种不同的模型和架构中运行，包括 two-party setting，three-party setting，four-party setting等。在这些工作中，以诚实多数抵抗半诚实对手的基于三方的方法具有最高的具体效率，并且引起了广泛关注。然而，这些方法大多只考虑卷积/深度神经网络的安全推断，不能直接扩展以支持快速安全的Transformer模型推断。最近，一些研究工作已经提出了基于MPC的Transformer模型安全推断解决方案。然而，这些方法在模型性能，效率和部署方面仍然存在局限性。在这些工作中，MPCFORMER是唯一一个已经开源的解决方案，它基于CrypTen，一个使用非串通第三方为客户端和服务器产生相关随机性的三方框架。在这项工作中，我们主要将我们提出的框架PUMA与MPCFORMER在相同的三方威胁模型下进行比较。 Background 2-out-of-3 Replicated Secret Sharing -&gt;这玩意没看懂 Secure Design of PUMAOverview of PUMA在PUMA中，我们的目标是实现基于Transformer模型的安全计算。为了实现这一目标，系统定义了三个实体：模型所有者、客户端和计算方。模型所有者提供经过训练的Transformer模型，客户端负责向系统提供数据并接收推理结果，而计算方（即 ，即3PC中的三个计算方）执行安全计算协议。需要注意的是，模型所有者和客户端也可以成为计算方，在这里我们将它们分开描述以便更容易说明。 在安全推理过程中，有一个关键不变量被保持：计算方始终从客户端输入和层权重的2-out-of-3复制秘密分享开始，并以层输出的2-out-of-3复制秘密分享结束。由于分享不会向任何一方泄露任何信息，这确保了协议模块可以按任意深度顺序组合以获得适用于任何基于Transformer模型的安全计算方案。 PUMA 的主要关注点是在保持所需安全级别的同时减少计算方之间运行时间和通信成本。通过利用复制秘密共享和我们的三方协议（3PC），PUMA 在三方设置下实现了对基于Transformer模型进行快速且安全地推理。 Protocol for Secure Embedding==这篇论文是真的莫名其妙，感觉就是把一些特别简单的东西写得弯来绕去让人看不懂，最大的贡献可能还是改进了transfomer里面的几个算子。==","categories":[],"tags":[]},{"title":"Privacy Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models","slug":"Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models","date":"2019-11-15T05:46:12.000Z","updated":"2024-03-02T14:25:55.007Z","comments":true,"path":"Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models.html","link":"","permalink":"https://asparticguan.github.io/blog/Privacy-Preserving%20Recommender%20Systems%20with%20Synthetic%20Query%20Generation%20using%20Differentially%20Private%20Large%20Language%20Models.html","excerpt":"","text":"Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language ModelsAbstract我们提出了一种新颖的方法，用于开发隐私保护的大规模推荐系统，该系统利用差分隐私（DP）大型语言模型（LLM），克服了在训练这些复杂系统时遇到的某些挑战和限制。我们的方法特别适用于基于LLM的推荐系统这一新兴领域，但也可以轻松应用于处理自然语言输入表示的任何推荐系统。我们的方法涉及使用DP训练方法，在查询生成任务上对公开预训练的LLM进行微调。由此产生的模型可以生成代表原始查询的私有合成查询，并可自由共享给下游非私密推荐训练过程中使用，而不会增加额外的隐私成本。我们评估了我们的方法在安全地训练有效深度检索模型方面所能取得的能力，并观察到与直接进行DP训练检索模型相比，在不损害查询级隐私保证情况下显著改善了其检索质量。 Introduction在许多推荐应用中，输入查询包含用户隐私信息，而候选推荐（如文章、产品、电影、广告）是公开信息。鉴于此，本文的目标是开发一种用于训练查询级隐私保护推荐系统的方法。 隐私保护推荐系统的标准方法是直接使用差分隐私（DP）训练方法来训练模型。然而，直接DP训练推荐系统存在各种问题。首先，DP训练方法通常确保示例级的隐私，这比我们的隐私目标更严格，因为推荐数据集中的示例对应于一个查询和若干候选推荐。因此，标准的DP训练隐私保证可能导致比必要更大的性能降低，以保护查询隐私。 与直接DP训练推荐系统的各种问题相比，我们采取了更简单的方法，在训练系统之前确保查询隐私。在这项工作中，我们在使用DP大型语言模型（LLM）生成合成数据的框架上进行了扩展，以开发一种用于具有查询级隐私保护的下游推荐系统的私密文本数据共享方法。我们的方法涉及使用DP训练方法对公开预训练的生成型LLM进行微调（或第二阶段预训练），针对包含匹配的查询-文档对的私有推荐数据集执行文档条件的查询生成任务。对于DP微调，我们采用差分隐私随机梯度下降（DP-SGD），其通过对每个示例的梯度进行剪辑，并在反向传播期间向聚合梯度注入校准噪声来工作。所得到的DP微调LLM用于生成私有合成查询，这些查询与训练数据中的文档有关。通过利用DP的后处理属性，可以安全地共享这些私有合成查询，用于任何下游的非私有训练程序，以用于推荐系统，而不会增加原始查询的任何隐私成本。我们使用这种方法在实证中证明，在与直接DP训练方法相比不损害隐私保证的情况下，深度检索任务的表现得到了显著改进。 我们的贡献如下： 我们提出了一种新颖的方法，用于训练具有查询级隐私保护的推荐系统。我们的方法涉及使用DP微调的LLM生成合成查询数据，并使用这些私有合成数据来训练下游推荐系统，而无需对标准训练过程进行任何修改。 通常情况下，我们的方法提供了一种新颖的方式，用于获得具有非逐示例可分解损失（non-per-example decomposable loss，推荐数据集中的示例对应于一个查询和若干候选推荐，不是per-example）的模型的DP保证，这种损失在大规模多阶段推荐系统中普遍存在，如对比损失。 通过广泛的实验，我们从经验上证实了我们的方法生成的合成数据确实具有查询级隐私保护，并且我们证明，使用这些私有合成数据训练的下游深度检索模型与标准DP训练方法相比在性能方面表现出色。 Related Work在推荐系统中的隐私保护。以往关于推荐系统的差分隐私研究主要涉及矩阵分解类模型。这些方法具有挑战性，容易受到冷启动问题的影响，并且在更严格的差分隐私参数下性能降低更为明显。以联邦学习方式训练推荐系统是另一种保护隐私的方法。然而，这种学习方法涉及到数据所有权和处理方式，而非数学上的隐私保证。 使用差分隐私语言模型生成合成数据。对于生成具有个人隐私特征表格数据已经进行了广泛探索，但是生成带有个人隐私特征文本数据还相对较新颖。利用差分隐私微调语言模型（LLMs）来生成带有差分隐私特征文本是目前主流方法，并且最近的研究也探讨了使用从差分隐私微调LLMs得到的合成数据在下游任务中的效用。这些研究发现，在相同的随机性预算下，使用合成数据训练的下游模型在性能上明显优于直接使用差分隐私训练的模型。有趣的是，在与非差分隐私模型进行比较时，结果各异。例如，Yue等人和Putta等人报告了使用合成数据训练的文本分类模型效用显著下降的情况，即使是非差分隐私微调LLM数据，这表明合成文本具有低保真度。然而，Mattern等人和Mireshghallah等人则报告了在文本分类和语义解析任务中使用非差分隐私合成数据生成可以提高性能的实例。这些发现表明，在没有应用差分隐私技术时，通过生成合成数据可以改善下游任务的性能，并且我们的工作探索了另一种潜在优势所在：针对部分数据（如我们所研究对象中的查询）确保隐私。 据我们所知，并没有先前研究探索仅针对部分数据（如我们案例中的查询）生成合成数据以确保隐私。 BackgroundDeep Retrieval深度检索系统，也被称为密集检索系统，在现代推荐系统中已经成为有效的组件。这些系统通常由两个encoder组成，能够生成丰富、密集的查询和文档表示，从而实现近似最近邻搜索，以高效地检索与查询语义意义相符的相关文档。深度检索系统通常通过对比损失进行训练，使用两种类型的示例：正样本和负样本。正样本将相关的查询-文档对embedding在embedding space中拉近，而负样本则将不相关对的embedding推远。获取硬负样本可能具有挑战性，因为它们需要从大量候选文档中进行额外挖掘。因此，在深度检索中流行选择 contrastive loss是in-batch softmax loss，该函数利用batch内的其他文档作为软负例。 假设我们有一个深度检索模型，用于给用户推荐电影。给定一个查询（用户的喜好或兴趣），模型的目标是从候选电影中找到与查询相关的电影，同时将不相关的电影视为负样本。在这种情况下，获取困难负样本可能是一个挑战。想象一下，我们有数千部电影作为候选，但在现实世界中，获取用户明确表示不喜欢的电影可能会很困难，因为用户通常只会对喜欢的电影进行评分或点击。为了解决这个问题，我们可以采用in-batch softmax loss。假设我们有一个训练批次，其中包含一些查询和与之对应的正样本电影，我们可以将这些正样本电影作为硬正样本。然后，我们可以将同一批次中的其他电影视为软负样本，通过计算查询与这些电影之间的相似度来推动模型学习。 特别地，在给定一个训练批次的查询-文档对中，每个 都是查询 的正向文档，并且批次中所有其他文件被视为负样本。in-batch softmax loss 对于批次中的每个样本都是 \\mathcal{L}_i=-\\log \\frac{e^{\\operatorname{sim}\\left(q_i, d_i\\right)}}{\\sum_{j \\in \\mathcal{B}} e^{\\operatorname{sim}\\left(q_i, d_j\\right)}}其中对于任意的，是和 embedding vectors之间的余弦相似度。批次越大且多样化，对于表示学习来说越好。 Differentially Private Training尽管DP-SGD可以无缝地应用于针对每个训练数据示例定义损失（例如交叉熵损失），但它不太适用于使用多个训练实例计算损失值（例如in-batch softmax loss）的对比性损失。原因是每个示例梯度还取决于多个示例，因此灵敏性可能会随着计算中使用的示例数量而增加。因此，确保隐私需要通过将渐变剪切中灵敏性按照用于计算对比损耗所使用的示例数量进行缩放，这可能会大大增加噪声。找到更好的方法来对非逐个分解损失进行DP-Training模型仍然是一个活跃的研究课题，各种工作在特定条件下引入了专门的算法，如凸性、平滑性和Lipshitz连续性，以保持合理的灵敏度界限。我们强调，尽管直接将DP-training应用于对比损失是粗糙的，因为它引入了太多噪声，但获得更好的算法本身就是一项艰巨任务。 Approach我们描述了一种通用方法，可以在确保查询级隐私的同时获得DP合成数据，以训练下游推荐系统。我们特别关注以这种方式训练双encoder 检索系统。我们还将讨论直接在原始数据上进行DP训练双 encoder 的替代方法，以进行比较。 Training using Synthetic Data obtained from a DP LLMDP-Training LLM on Conditional Query Generation Task 我们使用的数据集包含查询-文档对。我们使用一个合适的公开预训练LLM，该模型没有用我们使用的数据集进行预训练。我们使用T5语言模型的encoder，它们能够根据输入文本生成回答，并针对特定的生成任务进行微调。我们使用DP-Adafactor2来微调LLM，并完成以下条件查询生成任务：给定训练数据中的一个查询-文档对，通过输入“generate_query： ”与target：” “，来用==teacher forcing==训练T5模型（从参数量来说T5还真能算大模型，毕竟最大的T5有110亿参数，而ChatGLM只有60亿参数）。 Synthetic Query Generation using DP LLM 对于每个文档𝑑，我们通过将输入“generate_query: 𝑑”提供给模型来生成匹配的合成查询。在采样时，我们采用nucleus sampling策略。然后构建一个由原始文档及其相应合成查询组成的合成训练数据集。 Top-P Sampling (Nucleus sampling) 是预先设置一个概率界限 p 值，然后将所有可能取到的单词，根据概率大小从高到低排列，依次选取单词。当单词的累积概率大于或等于 p 值时停止，然后从已经选取的单词中进行采样，生成下一个单词。 Training Dual Encoder with DP Synthetic Data 合成数据可以安全地共享给任何下游训练程序，在原始查询上不会产生额外的DP损失，这是由DP的后处理属性所保证的。特别地，我们可以使用标准SGD方法，在合成训练数据上使用in-batch softmax loss来训练双encoder模型。 ==这个方法不是被谷歌说不好吗？拿生成出来的文本再做训练。== DP-Training using Original Data为了进行比较基准，我们还将直接在原始数据上对深度检索系统进行DP微调。然而，正如第3.4节中讨论的那样，DP-SGD需要考虑与对比损失（如in-batch softmax loss）兼容的额外因素。DP-SGD要求每个 per-example gradient clipping 中的敏感性按 batch 大小缩放。这意味着通过增加 batch size 来减少每个示例噪声以提高DP训练模型效用的最常见方法将无法奏效。此外，DP-SGD将保证示例级隐私，在本案例中一个示例包含一个查询和批次中的每个文档。然而，我们希望实现查询级隐私，这应该比保护查询和文档都更容易。与DP训练实施相关的另一个问题是为了利用计算每个示例梯度向量化和并行化策略，在批处理中的每个查询-文档示例必须复制到批处理中的每个示例中，导致内存需求呈二次增加趋势。在固定内存资源下，这就需要显著降低批量大小，并且除了梯度剪辑和添加噪声之外还会对学习产生负面影响，因为in-batch softmax loss的质量高度依赖于批内示例的数量和多样性。 EvaluationExperimental Setup数据集 我们在信息检索任务中使用公开可用的数据集。对于微调和评估，我们考虑了MSMARCO数据集[2]，该数据集包含从Bing搜索日志中采样得到的532,000个查询-文档对，涵盖了广泛的领域和概念。此外，我们还考虑了BEIR基准套件[52]中的数据集，该套件包含各种领域的信息检索数据集，用于零-shot评估。 数据生成 我们使用不同大小（小型、基础、大型、超大）和隐私保证（𝜖 ∈ {3, 8, 16, ∞}）训练了各种T5模型，以生成给定输入文档的合成查询。我们将MSMARCO数据集视为查询-文档训练数据。然后，我们使用每个训练好的模型为原始训练数据中的文档生成合成查询。这些合成查询和原始文档对构成了一个新的合成数据集。请参阅附录C，了解如何确保LLMs没有在训练数据中预先进行过查询预训练。我们对每个模型进行了30轮次的训练，批量大小为1024，并将输入文档的最大令牌长度设置为384，目标查询设置为128. 我们使用DP-Adam优化器，学习率为0.001，并且剪辑范数设定为0.1. 根据[33]所述，我们将隐私参数𝛿 = 1/2𝑛设定其中 𝑛 是培训资料集规模. 对于采样方法, 我们采用核心抽样(nucleus sampling)并设定 𝑝 = 0.8 。鉴于时间和计算限制，在T5-Small模型上通过小规模超参数搜索获得了结果(详见附录B)。 下游检索任务 对于每个数据源（原始MSMARCO数据和不同𝜖和模型大小的合成数据集），我们在 in-batch softmax loss上训练了一个深度检索双编码器模型（如第3.1节所述）。对于合成数据集和没有DP保证的原始数据集，我们使用标准SGD训练方法，而对于具有DP保证的原始数据集，则采用DP-SGD方法。我们利用预训练的T5-Base编码器作为查询和文档编码器，并共享它们之间的参数。我们将每个双编码器模型训练了5个epochs，并且使用与上述相同的令牌长度和超参数。对于直接DP训练，我们使用了与上述相同的隐私参数，并且考虑到第4.2节中讨论到的内存限制，用于DP训练双编码器模型的批量大小必须显著减小至32。重要说明一点，检索模型中的编码器与用于生成合成数据的T5模型是不同的。我们选择T5来评估基于LLM（语言理解与生成）技术推荐系统，在实际应用中越来越普遍[16]。由于我们旨在严格评估合成数据集性能，因此并未尝试不同的深度检索模型。 Evaluation on Retrieval TasksMSMARCO评估 表1显示了在原始数据和合成数据上训练的深度检索模型在MSMARCO测试集上的评估结果。在每个表格中，我们提供了一个基准评估，该评估是在没有任何差分隐私（DP）的情况下对原始数据进行训练得到的双编码器模型。我们观察到，在使用带有DP的合成数据进行训练时，检索模型明显优于使用原始数据进行DP训练时的检索效果。正如第4.2节所讨论的那样，使用对比损失来训练DP模型存在一些挑战。这很可能解释了为什么在原始数据上进行DP训练效果不佳。此外，我们发现，在非DP合成数据上进行训练得到的检索模型胜过了在原始数据上进行训练得到的检索模型。这表明合成数据生成确实增强了原始数据，并且在某种程度上改善了泛化能力，无论是通过融入额外公共知识还是通过清洗数据来实现。事实上，利用大规模语言模型生成合成数据以用于深度检索是近年来引起极大兴趣的研究领域[4, 13, 25]。我们还观察到，模型大小的增加会提高性能。这与之前类似的结果一致，表明在过参数化模型上进行差分隐私随机梯度下降（DP-SGD）可以比以前预期的效果要好得多[14, 33]。总体而言，我们展示了利用LLMs生成合成数据是一个可行的方法，在需要保护隐私的情况下训练检索模型。 zero-shot 评估 我们还评估了在合成数据上训练的检索模型的 zero-shot 泛化能力。我们将其与在原始数据上训练且没有差分隐私（DP）和 𝜖 = 16 的检索模型进行比较。结果见表2。再次，我们的结果显示，在某些情况下，使用 DP 合成数据相对于 DP 训练原始数据具有显著优势，并且几乎可以与非 DP 结果相媲美甚至超越其性能。这表明，在合理程度的隐私保护水平下，合成数据生成所带来的好处可能会超过 DP 训练中的效用降低，至少在零-shot 泛化任务中是如此。需要进一步研究以理解这一观察结果。 Similarity between Synthetic and Original Datasets相似度评分 由于合成数据是从原始数据一对一生成的，我们可以计算BLEU分数来评估相似性[41]。我们还计算了MAUVE分数，该分数被证明更能够比较文本分布的相似性[39]。请参见表3中的得分。我们观察到非DP微调模型生成的合成数据在这些指标下与预期相似，并且随着有限𝜖值的增加，与更高𝜖和模型大小增加而增加的相似性也会显著下降。通过将相似度评分与检索评价结果进行比较，我们观察到尽管更大的模型在合成数据相似性方面取得了显著改进，但随着模型大小增加，下游检索性能提升幅度相对较小。 Empirical Privacy可证明的差分隐私在𝜖增大时会显著衰减，但之前的研究表明即使在这些较大的值下也能提供强大的保护，防止最先进的隐私攻击[7, 9, 40]。为了验证我们的训练技术仍然遵循这一趋势，我们在此评估DP训练语言模型时所产生的实际隐私泄露情况，使用了[9]中引入的金丝雀暴露度指标。该技术经常用于评估实际隐私[23, 46, 62]。为了进行这个测试，我们构建包含个人信息（称为金丝雀）的示例，并将其中一部分引入原始训练数据中，然后测量模型输出注入式金丝雀查询结果的可能性。通常来说，生成金丝雀是一个依赖于领域特定决策，在我们检索应用程序中设计了三种类型的查询-文档对作为金丝雀：（随机查询、随机10位数字字符串）、（随机查询、相应文档+ 随机10位数字字符串）、（随机查询、随机文档+ 随机10位数字字符串）。每个金丝雀的秘密部分是随机的10位数字字符串。我们重复这些类型的金丝雀多次，并将它们包含在训练数据中。然后，我们使用不同DP保证对毒化训练数据集进行语言模型训练。一旦模型训练完成，我们生成合成数据集，并检查每个金丝雀查询的秘密部分是否出现在其输出中，并将注入式金丝雀与100个具有不同随机10位数字秘密字符串的候选序列进行比较，测量其在这些秘密之间的排名。我们重复整个实验过程，生成金丝雀、训练模型和测量泄露情况多次，并对指标取平均值，在表4中报告结果。如预期所见，在没有差分隐私的情况下进行训练会导致显著泄露。重复10次的金丝雀始终是最可能被提取出来的秘密，并经常能够被提取出来，而重复100次以上则总能够被提取出来。然而，即使𝜖为16时采用了我们的方法也可以防止模型泄露秘密信息，并显著增加排名[8]。最近关于将攻击成功率转换为𝜖参数下限[51]的技术使我们能够将这些排名解释为𝜖的下限，大约为0.015。这种巨大的差距与之前关于DP-SGD在大型语言模型上实际隐私性发现[9, 40]是一致的。 Conclusion我们提出了一种新颖的方法，通过使用差分隐私语言模型（DP LLMs）生成合成查询来以保护隐私的方式训练下游检索系统。更一般地说，我们的方法为具有非逐例可分解损失函数的模型获得了差分隐私保证的新途径。从实证上看，我们已经展示了在标准检索指标上可以实现具备差分隐私保证的高性能，并验证了我们方法的隐私保护效果。同时，我们还发现较大规模的模型可以在相同隐私水平下改善性能。因此，我们得出结论：基于合成文本生成技术是开发大规模、保护用户隐私推荐系统所迈出重要一步。 总结Learned a lot. 这篇文章思路很简单，感觉有点水，但这个思路很直接，可以在这篇文章基础上继续拓展到别的NLP领域。","categories":[],"tags":[]},{"title":"Who Wrote this Code? Watermarking for Code Generation","slug":"Who Wrote this Code  Watermarking for Code Generation","date":"2019-11-15T05:46:12.000Z","updated":"2024-03-02T14:08:21.055Z","comments":true,"path":"Who Wrote this Code  Watermarking for Code Generation.html","link":"","permalink":"https://asparticguan.github.io/blog/Who%20Wrote%20this%20Code%20%20Watermarking%20for%20Code%20Generation.html","excerpt":"","text":"Who Wrote this Code? Watermarking for Code Generation韩国人速度真快😭😭😭 Abstract大型代码语言模型最近展示了在生成可执行代码方面的卓越性能。然而，这种快速进步伴随着许多法律和道德问题，如代码许可问题、代码抄袭和恶意软件生成，使得为机器生成的代码添加水印成为一个非常及时的问题。尽管存在迫切需求，但我们发现现有的用于LLM（Large Language Models）的水印和机器生成文本检测方法在处理代码生成任务时无法正常工作。因此，在这项工作中，我们提出了一种新的水印方法SWEET，在为机器生成的代码添加水印时显著改进了先前的方法。我们提出的方法选择性地对熵高于定义阈值的令牌应用水印。对于代码生成基准测试实验表明，与先前最先进LLM水印方法产生的代码相比，我们带有水印标记的代码具有更好质量。此外，我们提出的水印方法在机器生成码检测任务上也优于DetectGPT。 Introduction最近，Kirchenbauer等人（2023年）提出了一种水印技术，我们称之为“香草水印”，它在从语言模型生成的标记中嵌入水印。对于每个步骤生成的标记，该方法随机将整个词汇表分成两组（例如，红色组用于避免的标记和绿色组用于偏好的标记）。绿色组的标记通过固定数量增加逻辑值。这样，在每个时间步长上，模型往往更倾向于生成来自绿色组的标记。在检测阶段，该方法计算绿色组标记的数量，并检查该数字是否具有统计学意义以得出结论：模型输出是在不知道绿-红规则下生成的。虽然前述方法已经在文本检测领域展示了强大性能，但它们在代码检测任务中尚未被探索应用。 我们对代码检测问题进行了大量实验，并观察到这些方法存在严重的退化现象。特别是，我们发现 Kirchenbauer 等人（2023）的两种失败模式是由于生成代码中极低熵性质所导致的。也就是说，先前生成的代码标记对模型来说非常熟悉，以至于它几乎可以确定地生成下一个标记。如果这些唯一的“黄金”标记属于红色组，则会出现以下情况：(i) 如果从绿色组中抽样其他标记（参见图1(a)），则模型输出的质量可能会严重受损；或者 (ii) 如果黄金标记的逻辑值仍然高于绿色组中其他标记（参见图1(b)），则检测机器生成代码将变得困难。 为了解决这些失败模式，我们提出了一种基于熵阈值选择性水印技术（SWEET）用于 Code LLMs （和 LLMs）。与将绿-红规则应用于代码输出中每个单个标记不同，我们只将该规则应用于比给定阈值更高熵度的标记上。这种选择性水印方法不会将红色组规则应用到黄金标记上，从而避免它们落入红色组，因此上述失败模式将不会发生。 基于我们对 LLaMA-13B（Touvron 等人，2023）在代码补全基准测试 HumanEval（Chen 等人，2021）和 MBPP（Austin 等人，2021）上的实验，我们可以总结出以下贡献： • 我们是第一个识别和制定LLM生成的代码水印问题的团队。• 我们提出了一种简单而有效的方法，称为选择性水印，与之前的普通水印方法（Kirchenbauer等人，2023年）相比，显著提高了代码执行通过率。• 与现有基线相比，我们的方法还显示出更高的机器生成代码检测准确性。 Related WorkMethod我们解决了之前的普通数字水印方法Kirchenbauer等人（2023年）在嵌入和检测水印时出现的问题，特别是在代码生成的情况下。我们提出了一种新的数字水印方法SWEET，可以有选择地对熵高的令牌进行数字水印。 Preliminaries关于Kirchenbauer等人（2023年）的论文建议看这个速成：大模型水印技术：判断文本是不是LLM生成的 - 知乎 (zhihu.com) Motivation虽然香草水印技术已经显示出检测LLM生成的文本的可能性，但我们认为该算法可能容易受到包含低熵令牌的文本（特别是代码）攻击，从两个方面进行了讨论：1）检测带有水印文本的性能；2）生成文本的质量。 低熵token避免被水印标记 当我们考虑图1（b）时，排名第一的令牌True（已列入红名单）持有最大概率。在这种情况下，水印标记无法正常工作，因为红名单中的True几乎是确定性生成的，而不是绿色列表中的False。因此，在大多数情况下，代码比纯文本少得多地被加上了水印标记，使其更难以检测。 水印会导致性能下降 另一方面，当我们考虑图1（a）描述的成功发出绿色标记False的情况时，我们可以看到代码将立即无法正常执行。尽管这种情况是一个相当极端的例子，但这仍然是我们在对代码进行水印处理后看到显着代码性能下降的主要原因。 Selective Watermarking with Entropy Threshold (SWEET)如上所述的Motivation小节中所描述的，盲目地应用普通数字水印技术于code LLMs将会导致imperceptibility（不可感知性）或者代码检测能力方面出现问题。虽然减少普通数字水印技术中的值也可以提高不可感知性，但这也会降低检测能力，因此需要权衡取舍。SWEET通过检测更多可互换标记并将其替换为水印来改善这种权衡。 上图为SWEET算法，其中算式如下： 其实和Kirchenbauer等人（2023年）没有多大区别，只不过加了个阈值，只对熵值高于 的token加水印（）。也就是说如果下一个token的结果极为确定（例如上面举的True和False），就不在此上加水印。 算哈希的方法，Kirchenbauer等人（2023年）中使用的方法是token id×hash key，这里估计差不多。 然后是检测的算法： 其中算式分别为： 其中是一段已知的不是LLM生成的代码段。总的来说验证的过程就是拿重新输出一遍，然后看有没有超过阈值。 Experiments我们进行了一系列实验来评估我们的水印方法在应用于基础LLM时的不可察觉性和检测能力。我们使用LLaMA-13B（Touvron等人，2023）作为所有实验的基础LLM。对于代码生成测试平台，使用了两个数据集：HumanEval（Chen等人，2021）和MBPP（Austin等人，2021）。这两个任务都测试语言模型是否能够根据函数名和指令生成完整的函数代码。带有或不带有水印的LLM会生成一个代码，并且我们旨在将其与人工编写的代码区分开来。 Baselines我们将选择性水印方法与各种机器生成文本检测基线进行比较，这些基线分为基于检测和基于水印的方法。 基于检测的方法可以在不加水印的情况下检测到合成文本。 在四个用于比较机器生成代码检测性能的基线中，有三个是基于检测的，一个是基于水印的。 基于检测的基线识别LLM生成输出特征以确定候选文本源LM，而基于水印的则通过统计t测试来检测隐藏水印是否存在。 Metrics生成代码的功能 为了评估代码生成，我们使用 pass@k（Chen等人，2021）通过为每个实例生成 n（&gt; k）个样本来进行评估。通过将水印代码的 pass@k 分数与基础 LM 生成的非水印代码的分数进行比较，我们可以评估水印方法的不可察觉性。请注意，我们无需评估基于检测的基线方法的不可察觉性，因为它们不会对代码进行水印处理；它们使用原始代码。 机器生成代码检测的性能 对于检测能力，我们计算AUROC（即ROC下面积）、TPR（真正例率）和FPR（假正例率）来衡量正确检测机器生成代码的性能。我们报告TPR得分在低于1%的FPR水平上，以最小化将人工编写的代码误分类为机器生成代码，这被认为是不可取的（即I型错误）。即希望确保在判定机器生成的代码时，不会超过1%的人工编写的代码被错误地分类为机器生成的。 Hyperparameters我们主要遵循LLaMA中的超参数来生成源代码。在HumanEval中，我们生成个样本，但由于计算资源昂贵，在MBPP中只生成个样本。我们使用p=0.95和温度为0.8的top-p (Holtzman等人，2020)采样方法。当T5-3B用于Detect-GPT时，我们遵循DetectGPT的原始超参数，例如2个单词跨度和15％的掩码。然而，在利用SantaCoder时，我们通过仅屏蔽每次扰动中一行代码来模拟单行填充任务场景，这使得SantaCoder表现良好（Fried等人, 2023; Bavarian等人, 2022）。对于水印方法，我们将绿色列表大小参数固定为0.25，并且添加到绿色列表logits。 Results and AnalysisMain Results Further Analysis分析熵阈值的影响 图3展示了在我们的方法中校准熵阈值时，代码生成性能和检测能力如何变化。随着熵阈值的增加，watermarked token比例降低，因此代码生成性能趋于非水印基础模型。这表明我们的方法始终处于普通数字水印和非水印基础模型之间。另一方面，检测能力呈现出明显的曲线：随着熵阈值的增加，它达到局部最大值但最终下降。我们认为这一点是帕累托前沿点，代表了代码生成性能和检测能力之间的权衡。 分析良好的代码 为了看到当机器生成的代码和人工编写的代码表示相同功能时，它们有多难以区分，我们仅对可运行的代码对进行检测性能评估。在表2中，包括我们自己在内的基于水印技术的方法遭受了巨大的性能下降；就TPR而言，在香草水印和我们两种方法中，分别下降了71％、75％和73％。这些结果表明，在嵌入水印并且像人工编写一样运行良好的情况下生成代码比其他文本生成任务更具挑战性。另一方面，考虑示例集合规模较小，基于检测方法性能变化微不足道。 消融研究 我们进行了一项消融研究，以评估使用熵阈值的贡献。在图2中，我们可以清楚地看到随着熵阈值的增加，水印标记（绿色标记）的数量减少。这自然引出一个问题：实际上最重要的是水印部分比例的减少吗？为了回答这个问题，我们尝试了修改后的香草水印算法版本，并将水印比例从100％（香草）逐渐降低到0％（规范解决方案-无水印）。我们通过随机决定是否排除目标比例下每个令牌进行水印来做到这一点。在图4中，我们还绘制了SWEET在不同熵阈值下的pass@10结果（因为通过增加阈值，水印比例会下降）。 在这里，我们可以观察到随着减少加密比例，在60％左右有一个显着下降曲率和拐点处时vanilla watermarking pass@10呈现出凸形曲线。另一方面，我们的方法显示出(近乎)单调递增曲线。对于两种方法都存在明显峰值——当从20%上涨到0%时，这是一个显而易见的结果，因为对规范解决方案几乎没有进行任何修改。有趣的是，在SWEET中，当性能开始上升（约25%）时，熵阈值为0.9，并且在图3中显示它获得了最高AUROC。正如图所示，在此处检测AUROC开始下降。这也有一个简单明了的答案，因为要加密的令牌越少，则检测置信度变弱。因此，我们可以安全地得出结论：通过使用熵阈值-SWEET-我们能够找到pass@k和detection AUROC之间权衡的最佳点。 Case Study: Breakdown of DetectGPT虽然DetectGPT在纯文本生成任务（如XSum、SQuAD或Wikipassage）中表现出了卓越的检测性能（Mitchell等人，2023年），但它在代码生成任务中表现最差。我们假设Detect-GPT的检测能力取决于底层LLM的性能。它在LLM的性能接近人类水平的任务中表现出色，而在代码生成任务中其性能显著较低，没有任何LLM达到人类水平的性能。 Limitations我们主要指出这项工作的局限性，并提出缓解它们的方法。我们特别要注意，这些限制并不是我们工作的“弱点”，因为它们不仅仅局限于我们提出的方法，而且也是该领域现状的一种限制。 第一个问题涉及手动选择熵阈值的过程。改变此阈值会极大地改变代码和检测性能，因此它是一个非常重要的超参数，其取决于用于生成和基准数据集中使用骨干模型。然而，根据我们观察到的情况，在保持语料库平均熵附近时通常可以得到接近 Pareto 最优结果之间检测精度和 pass@k 的平衡点，表明存在优化可能。 此外，在实验结果讨论中提到了，在检测时还需要源代码 LLM ，因此该方法只适用于完全白盒设置下（就像 DetectGPT 一样）。对于希望应用我们工作但计算负担较大用户来说可能会有困难。 Conclusion在本文中，我们确定并强调了对代码LLM水印的需求，并首次进行了正式规范化。尽管大型语言模型编码能力迅速提高，但鼓励安全使用这些模型的必要措施尚未实施。我们的实验表明，现有的水印和检测技术无法在代码生成设置下正确运行。失败发生在两种模式下：1）代码不能被正确地加上水印（因此无法被检测），或者2）带有水印的代码无法正确执行（质量降低）。另一方面，我们提出的方法（SWEET）通过引入选择性熵阈值处理过滤与执行质量最不相关的标记，在某种程度上解决了这两种故障模式。确实，使用SWEET进行实验得到结果并没有完全恢复原始非水印性能；然而，我们认为这是朝着实现这个雄心勃勃目标迈出重要一步。 总结正如作者在这篇文章中所说的，使用SWEET进行实验得到结果并没有完全恢复原始非水印性能；然而，我们认为这是朝着实现这个雄心勃勃目标迈出重要一步，明天就去看看OpenAI的PPT是怎么解决这个问题的。","categories":[],"tags":[]}],"categories":[],"tags":[]}